{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mike Babb\n",
    "# babbm@uw.edu\n",
    "# Find anagrams\n",
    "## Part 2: Generate and store the anagrams v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard libraries - installed by default\n",
    "import collections\n",
    "import datetime\n",
    "import pickle\n",
    "import sqlite3\n",
    "import string\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# external libraries - not installed by default\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from part_00_file_db_utils import *\n",
    "from part_00_process_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base file path\n",
    "base_file_path = '/project/finding_anagrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input path\n",
    "in_file_path = 'data'\n",
    "in_file_path = os.path.join(base_file_path, in_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output db path and name\n",
    "db_path = 'db'\n",
    "db_path = os.path.join(base_file_path, db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(db_path):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name = 'words.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...query execution took: 2.169225 seconds...\n"
     ]
    }
   ],
   "source": [
    "# load the word_df, the words from Part 1\n",
    "sql = 'select * from words;'\n",
    "\n",
    "word_df = query_db(sql = sql, db_path = db_path, db_name = db_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234370, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the column of word ids as a numpy array\n",
    "word_id_list = word_df['word_id'].to_numpy(dtype = int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the letters sorted by the frequency of words that\n",
    "# start with a particular letter\n",
    "agg_word_df = word_df['first_letter'].groupby(word_df['first_letter']).agg(np.size).to_frame()\n",
    "\n",
    "# set column names\n",
    "agg_word_df.columns = ['word_count']\n",
    "\n",
    "# reset the index to rename columns\n",
    "agg_word_df = agg_word_df.reset_index()\n",
    "\n",
    "# sort the dataframe by frequency\n",
    "agg_word_df = agg_word_df.sort_values(by='word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the letters sorted by word frequency\n",
    "sorted_first_letters = agg_word_df['first_letter'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the letter dictionary from part 1\n",
    "in_file_name = 'letter_dict.pkl'\n",
    "letter_dict = load_pickle(in_file_path = in_file_path, in_file_name=in_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the word dictionary from part 1\n",
    "in_file_name = 'word_dict.pkl'\n",
    "word_dict = load_pickle(in_file_path = in_file_path, in_file_name=in_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the char matrix from part 1\n",
    "in_file_name = 'char_matrix.npy'\n",
    "ipn = os.path.join(in_file_path, in_file_name)\n",
    "char_matrix = np.load(file = ipn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the word group df: wg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop duplicates based on the word group. \n",
    "# by default, this will only keep the first record and it will drop all others\n",
    "wg_df = word_df.drop_duplicates(subset = ['word_group_id']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wg_df = wg_df.sort_values(by = 'word_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unique word groups\n",
    "wg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the word group ids\n",
    "word_group_id_list = wg_df['word_group_id'].to_numpy()\n",
    "# and the associated word_id\n",
    "word_id_list = wg_df['word_id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trim the char matrix by word id\n",
    "# and not the word_group id\n",
    "wchar_matrix = char_matrix[word_id_list, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i don't use these objects, but i can't delete them?\n",
    "# build a word_id to word_group_id dictionary\n",
    "word_id_wg_id_dict = dict()\n",
    "# and a word_group_id to word_id dictionary\n",
    "wg_id_word_id_dict = dict()\n",
    "\n",
    "for word_id, wg_id in zip(wg_df['word_id'], wg_df['word_group_id']):\n",
    "    word_id_wg_id_dict[word_id] = wg_id\n",
    "    wg_id_word_id_dict[wg_id] = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df['letter_selector'] = wg_df['letter_group_ranked'].str[:n_subset_letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the dictionary holding the sub-matrices\n",
    "#n_char_matrix_dict = {}\n",
    "\n",
    "# by word length\n",
    "word_length_list = sorted(wg_df['n_chars'].unique().tolist())\n",
    "\n",
    "# python dictionaries work by storing the hash values of objects\n",
    "# Anything that can be hashed can be a dictionary key. \n",
    "# Computing the hash value of an object ahead of time can reduce dictionary access time.\n",
    "# we'll compute the associated hash value of the tuple used to identify the sub-matrices.\n",
    "\n",
    "#wg_id_n_char_matrix_dict = {}\n",
    "#wg_df['wg_id_n_char_matrix_key'] = wg_df['letter_selector']\n",
    "#wg_df['wg_id_n_char_matrix_key_hash'] = wg_df['wg_id_n_char_matrix_key'].map(hash)\n",
    "#for curr_word_id, curr_key_hash in zip(wg_df['word_group_id'], wg_df['wg_id_n_char_matrix_key_hash']):\n",
    "#    wg_id_n_char_matrix_dict[curr_word_id] = curr_key_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there will be three parts to this function\n",
    "# The first part does a selection by a single character\n",
    "# The second part does the selection based on the newly created subselection\n",
    "\n",
    "# peforming selections on a dataframe is slow.\n",
    "# Especially so since we are comparing characters\n",
    "    \n",
    "# now, do it again, but this time use the dictionary\n",
    "# by word length and n least common letters\n",
    "wg_df['letter_selector'] = wg_df['letter_group_ranked'].str[:n_subset_letters]\n",
    "nc_ls_df = wg_df[['n_chars', 'letter_selector']].drop_duplicates()\n",
    "\n",
    "print(n_subset_letters)\n",
    "print('...creating', nc_ls_df.shape[0], 'sets of ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_char_matrix_dict, single_letter_matrix_dict, letter_selector_matrix_dict, nc_ls_matrix_dict, split_count_df, single_letter_df = split_matrix(\n",
    "    letter_dict = letter_dict,\n",
    "    word_group_id_list = word_group_id_list,\n",
    "    nc_ls_df = nc_ls_df,\n",
    "                 wg_df = wg_df,\n",
    "                 wchar_matrix = wchar_matrix,\n",
    "db_path = db_path, \n",
    "db_name = db_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df = pd.merge(left = wg_df, right = single_letter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join this back to the wg_df\n",
    "wg_df = pd.merge(left = wg_df, right = split_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# demonstrate the different matrix extraction options with the word 'achiever'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo_word = 'achiever'\n",
    "\n",
    "wg_id = word_df.loc[word_df['lcase'] == demo_word, 'word_group_id'].iloc[0]\n",
    "\n",
    "demo_wg_df = wg_df.loc[wg_df['word_group_id'] == wg_id, : ]\n",
    "\n",
    "# option 2\n",
    "n_char = demo_wg_df['n_chars'].iloc[0]\n",
    "\n",
    "# option 3\n",
    "first_letter = demo_wg_df['first_letter'].iloc[0]\n",
    "\n",
    "# option 4\n",
    "least_common_letter = demo_wg_df['letter_selector'].iloc[0][0]\n",
    "\n",
    "# option 5\n",
    "letter_selector = demo_wg_df['letter_selector'].iloc[0]\n",
    "\n",
    "# option 6\n",
    "nc_ls_tuple = demo_wg_df['nc_ls_tuple'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 full matrix: complete | get_values_full_matrix\n",
    "# 2 n char (word length): complete | get_values_n_char\n",
    "# 3 first letter: complete | get_value_first_letter\n",
    "# 3 focal letter: complete | get_values_letter_selector\n",
    "# 4 n char and least common letters: complete | get_values_n_char_letter_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where I left offHi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select on the full matrix: option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# demo the full matrix selection\n",
    "output = get_values_full_matrix(wg_id = wg_id, \n",
    "                    wchar_matrix = wchar_matrix,\n",
    "                   word_group_id_list = word_group_id_list)\n",
    "\n",
    "# this is an array of from words to the word 'achiever'\n",
    "format_demo_output(demo_word = demo_word,\n",
    "                   word_df = word_df,\n",
    "                   demo_output = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select on the matrices split by word-length: option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo the n char selection\n",
    "output = get_values_n_char(wg_id = wg_id,\n",
    "                      n_char = n_char,\n",
    "                      n_char_matrix_dict = n_char_matrix_dict)\n",
    "\n",
    "# this is an array of from words to the word 'achiever'\n",
    "format_demo_output(demo_word = demo_word,\n",
    "                   word_df = word_df,\n",
    "                   demo_output = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select on the matrices split by the first letter: option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo the first letter selection\n",
    "output = get_values_single_letter(wg_id = wg_id, single_letter = least_common_letter,\n",
    "                                                           single_letter_matrix_dict = single_letter_matrix_dict)         \n",
    "\n",
    "# this is an array of from words to the word 'achiever'\n",
    "format_demo_output(demo_word = demo_word,\n",
    "                   word_df = word_df,\n",
    "                   demo_output = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select on the matrices split by the single least common letter: option 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo the first letter selection\n",
    "output = get_values_single_letter(wg_id = wg_id, single_letter = least_common_letter,\n",
    "                                                           single_letter_matrix_dict = single_letter_matrix_dict)         \n",
    "\n",
    "# this is an array of from words to the word 'achiever'\n",
    "format_demo_output(demo_word = demo_word,\n",
    "                   word_df = word_df,\n",
    "                   demo_output = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select on the matrices split by the letter selector: option 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo with the letter selector\n",
    "output = get_values_letter_selector(wg_id = wg_id,\n",
    "                      letter_selector = letter_selector,\n",
    "                      letter_selector_matrix_dict = letter_selector_matrix_dict)\n",
    "\n",
    "# this is an array of from words to the word 'achiever'\n",
    "format_demo_output(demo_word = demo_word,\n",
    "                   word_df = word_df,\n",
    "                   demo_output = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select on the matrices split by word-length and the letter selector: option 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo with the n_char letter selector\n",
    "output = get_values_n_char_letter_selector(wg_id = wg_id,\n",
    "                           nc_ls_tuple = nc_ls_tuple,                           \n",
    "                           nc_ls_matrix_dict=nc_ls_matrix_dict)\n",
    "\n",
    "# this is an array of from words to the word 'achiever'\n",
    "format_demo_output(demo_word = demo_word,\n",
    "                   word_df = word_df,\n",
    "                   demo_output = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we've tested with one word, let's time many evaluations to get a sense of how quickly \n",
    "# the different matrix extraction options work\n",
    "# use the timeit() function to evaluate how long, on average, a single matrix operation\n",
    "# takes to complete\n",
    "\n",
    "n_trials = 10\n",
    "\n",
    "code_snippet_dict = {\n",
    "    'Selecting by full matrix':\n",
    "\"\"\"get_values_full_matrix(wg_id = wg_id, wchar_matrix = wchar_matrix, word_group_id_list = word_group_id_list)\"\"\",\n",
    "    'Selecting by word length':\n",
    "\"\"\"get_values_n_char(wg_id = wg_id, n_char = n_char, n_char_matrix_dict = n_char_matrix_dict)\"\"\",\n",
    "    'Selecting by first letter':\n",
    "\"\"\"get_values_single_letter(wg_id = wg_id, single_letter = first_letter, single_letter_matrix_dict = single_letter_matrix_dict)\"\"\",    \n",
    "    'Selecting by single least common letter':\n",
    "\"\"\"get_values_single_letter(wg_id = wg_id, single_letter = least_common_letter, single_letter_matrix_dict = single_letter_matrix_dict)\"\"\",\n",
    "    'Selecting by letter selector':\n",
    "\"\"\"get_values_letter_selector(wg_id = wg_id, letter_selector = letter_selector, letter_selector_matrix_dict = letter_selector_matrix_dict)\"\"\",\n",
    "    'Selecting by word length and letter selector':\n",
    "\"\"\"get_values_n_char_letter_selector(wg_id = wg_id, nc_ls_tuple = nc_ls_tuple, nc_ls_matrix_dict=nc_ls_matrix_dict)\"\"\"\n",
    "}\n",
    "\n",
    "for csd, cs in code_snippet_dict.items():\n",
    "    \n",
    "    total_time = timeit.timeit(cs, number=n_trials, globals=globals())\n",
    "\n",
    "    # total time    \n",
    "    total_time_formatted = '{:,}'.format(round(total_time, 4))    \n",
    "\n",
    "    # average time\n",
    "    avg_time = total_time / n_trials\n",
    "    avg_time_formatted = '{:,}'.format(round(avg_time, 6)) \n",
    "        \n",
    "    print(csd)\n",
    "    # average number of seconds per trial\n",
    "    print('Total time:', total_time_formatted, 'seconds. Average time:', avg_time_formatted, 'seconds.')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the combination of the word length and letter selector is the fastest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate total number of from/to word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many anagrams are there?\n",
    "# let's estimate the number of anagrams by assuming that the number of\n",
    "# parent/from words is a function of word length. \n",
    "# let's sample 10 words of each word length, compute the number of from/parent anagrams\n",
    "# for each word in the sample, compute the min, mean, and max, and apply those values\n",
    "# to the numbers of words by length and multiply accordingly\n",
    "# this will give us very generous upper bound of anagram pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_possible_anagrams = estimate_total_pairs(word_df = word_df, wg_df = wg_df,\n",
    "                         nc_ls_matrix_dict = nc_ls_matrix_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discover from/to word group id pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialize counters to count the number of to (child words) from a focal word.\n",
    "# we could do this in post-processing, but the data are already in memory and it's a simple\n",
    "# calculation to make.\n",
    "# we want to minimize the number of trips through our data.\n",
    "\n",
    "# the number of candidate words examined for each focal word\n",
    "\n",
    "# a list to hold the dataframes generated for each letter\n",
    "proc_time_df_list = []\n",
    "\n",
    "# subset the list of leters\n",
    "if letter_subset_list:\n",
    "    letters = letter_subset_list[:]\n",
    "else:\n",
    "    letters = sorted_first_letters\n",
    "\n",
    "anagram_pair_count = 0\n",
    "# use numpy to pre-allocate an array that will be updated while enumerating.\n",
    "# this eliminates list.append() calls\n",
    "\n",
    "output_list = np.full(shape=(n_possible_anagrams, 2), fill_value=-1, dtype=int)\n",
    "\n",
    "wg_count = 0\n",
    "\n",
    "for curr_letter in letters:\n",
    "    # enumerate by each letter\n",
    "    # this isn't absolutely necessary, we could just enumerate by word id,\n",
    "    # but for testing and development, letters are a handy way to chunk up the data.\n",
    "\n",
    "    # this dictionary will store the calculations for each letter\n",
    "    proc_time_dict = {}\n",
    "\n",
    "    # the list of words that start with the focal letter\n",
    "    curr_wg_df = wg_df.loc[wg_df[\"first_letter\"] == curr_letter, :]\n",
    "\n",
    "    # sort the dataframe by n_chars and letter_selector, if it exists.\n",
    "    # this will cut down on dictionary lookups for matrix_extraction_types 3 and 4.\n",
    "    curr_wg_df = curr_wg_df.sort_values(by=[\"n_chars\", \"letter_selector\"])\n",
    "    curr_word_group_id_list = curr_wg_df[\"word_group_id\"].to_numpy()\n",
    "    curr_nc_ls_tuple_list = curr_wg_df[\"nc_ls_tuple\"].to_numpy()\n",
    "\n",
    "    wg_count += len(curr_word_group_id_list)\n",
    "\n",
    "    n_curr_words = \"{:,}\".format(curr_wg_df.shape[0])\n",
    "    print(\n",
    "        \"...finding parent anagrams for\",\n",
    "        n_curr_words,\n",
    "        \"words that start with\",\n",
    "        curr_letter,\n",
    "    )\n",
    "\n",
    "    # enumerate by word id, working with integers is faster than words\n",
    "    for row in curr_wg_df.itertuples(index = False):\n",
    "        wg_id = row.word_group_id\n",
    "            \n",
    "        # start timing to record processing for each word\n",
    "        s_time = datetime.datetime.now()\n",
    "\n",
    "        # get the current word length, from the word id\n",
    "        # to_word, to_word_length, curr_first_letter, clg, clgr = word_dict[word_group_id]\n",
    "        to_word_length = word_dict[wg_id][1]\n",
    "\n",
    "        # get the tuple associated with the word id\n",
    "        # much faster to look up stored values for the hash value than it is to\n",
    "        # only look up if the hash value has changed\n",
    "\n",
    "        # get the possible candidate word_group_ids and char matrix\n",
    "        ####\n",
    "        ## TODO: CODE OPTIONS 1 THROUGH 4. OR 5?\n",
    "        ####\n",
    "        if matrix_extraction_option == 1:\n",
    "            # option 1: full matrix        \n",
    "            outcome_word_id_list = get_values_full_matrix(wg_id = wg_id, wchar_matrix = wchar_matrix, word_group_id_list = word_group_id_list)\n",
    "        elif matrix_extraction_option == 2:\n",
    "            # option 2: word length\n",
    "            outcome_word_id_list = get_values_n_char(wg_id = wg_id, n_char = row.n_chars, n_char_matrix_dict = n_char_matrix_dict)\n",
    "        elif matrix_extraction_option == 3:            \n",
    "            # option 3: first character\n",
    "            outcome_word_id_list = get_values_single_letter(wg_id = wg_id, single_letter = row.first_letter,\n",
    "                                                           single_letter_matrix_dict = single_letter_matrix_dict)            \n",
    "        elif matrix_extraction_option == 4:\n",
    "            # option 4: letter selector / focal letter\n",
    "            outcome_word_id_list = get_values_letter_selector(wg_id = wg_id, letter_selector = row.letter_selector, letter_selector_matrix_dict = letter_selector_matrix_dict)                            \n",
    "        else:     \n",
    "            # option 5: word length and letter selector\n",
    "            outcome_word_id_list = get_values_n_char_letter_selector(\n",
    "                wg_id=wg_id, nc_ls_tuple=row.nc_ls_tuple, nc_ls_matrix_dict=nc_ls_matrix_dict\n",
    "            )\n",
    "        \n",
    "        # if the outcome is greater than or equal to zero, then the current word is an\n",
    "        # anagram of the other word\n",
    "        # a value  >= 0 means that the current word contains the exact same number of focal letters\n",
    "        # mite --> time or miter --> time\n",
    "        # a value >= 1 means that current word contains at least the same number of focal letters\n",
    "        # terminator --> time\n",
    "        # a value of <=-1 means that the current word does not have the\n",
    "        # correct number of letters and is therefore not an anagram.\n",
    "        # trait <> time\n",
    "\n",
    "        # number of parent words found\n",
    "        n_from_words = outcome_word_id_list.shape[0]\n",
    "\n",
    "        if n_from_words > 1:\n",
    "            # we have matches\n",
    "            # the focal word\n",
    "\n",
    "            # enumerate the from/parent words\n",
    "            new_anagram_pair_count = anagram_pair_count + n_from_words\n",
    "            # the from words\n",
    "            # print(anagram_pair_count)\n",
    "            # print(new_anagram_pair_count)\n",
    "            # print(len(outcome_word_id_list))\n",
    "            # print(output_list.shape)\n",
    "            output_list[\n",
    "                anagram_pair_count:new_anagram_pair_count, 0\n",
    "            ] = outcome_word_id_list[:, 0]\n",
    "\n",
    "            # the to word\n",
    "            output_list[\n",
    "                anagram_pair_count:new_anagram_pair_count, 1\n",
    "            ] = outcome_word_id_list[:, 1]\n",
    "\n",
    "            # set the anagram pair count\n",
    "            anagram_pair_count = new_anagram_pair_count\n",
    "\n",
    "        del outcome_word_id_list\n",
    "\n",
    "        # record the time for the word\n",
    "        e_time = datetime.datetime.now()\n",
    "        p_time = e_time - s_time\n",
    "        p_time = p_time.total_seconds()\n",
    "\n",
    "        proc_time_dict[wg_id] = (p_time, n_from_words)\n",
    "\n",
    "    # create a dataframe from the proc_time_dict\n",
    "    proc_time_df = pd.DataFrame.from_dict(data=proc_time_dict, orient=\"index\")\n",
    "    proc_time_df = proc_time_df.reset_index()\n",
    "    proc_time_df.columns = [\n",
    "        \"word_group_id\",\n",
    "        \"n_seconds\",\n",
    "        \"n_from_word_groups\"        \n",
    "    ]\n",
    "\n",
    "    # display processing time for the current letter\n",
    "    total_proc_time = round(proc_time_df[\"n_seconds\"].sum(), 2)\n",
    "    print(\n",
    "        \"...finding parent anagrams for\",\n",
    "        curr_letter,\n",
    "        \"words took\",\n",
    "        total_proc_time,\n",
    "        \"seconds...\",\n",
    "    )\n",
    "\n",
    "    proc_time_df_list.append(proc_time_df)\n",
    "\n",
    "# let's do some stuff that counts the words\n",
    "proc_time_df = pd.concat(objs = proc_time_df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape and store output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# truncate the output array to only include indices with a from/to word pair\n",
    "output_indices = np.all(output_list >= 0, axis = 1)\n",
    "output_list = output_list[output_indices, ]\n",
    "del output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many anagram pairs were found?\n",
    "n_total_anagrams = len(output_list)\n",
    "n_total_anagrams_formatted = '{:,}'.format(n_total_anagrams)\n",
    "print('...total anagrams', n_total_anagrams_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## count the number of to words\n",
    "# https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "# number of to words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write anagram pairs to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write the anagram pairs to the database\n",
    "if write_data:\n",
    "    store_anagram_pairs(output_list = output_list, db_path = db_path, db_name = db_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store number of from/to word pairs and time related to processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store_anagaram_processing_time(output_list=output_list, proc_time_df=proc_time_df, word_df=word_df,\n",
    "                               wg_df = wg_df,\n",
    "                               matrix_extraction_option = matrix_extraction_option, db_path=db_path,\n",
    "                               db_name=db_name, total_time_start=total_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
