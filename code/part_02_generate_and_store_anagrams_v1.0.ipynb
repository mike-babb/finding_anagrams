{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mike Babb\n",
    "# babbm@uw.edu\n",
    "# Find anagrams\n",
    "## Part 2: Generate and store the anagrams v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries - installed by default\n",
    "import collections\n",
    "import datetime\n",
    "import pickle\n",
    "import sqlite3\n",
    "import string\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external libraries - not installed by default\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from part_00_process_functions import load_pickle, build_db_conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base file path\n",
    "base_file_path = '/project/finding_anagrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input path\n",
    "in_file_path = 'data'\n",
    "in_file_path = os.path.join(base_file_path, in_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output db path and name\n",
    "db_path = 'db'\n",
    "db_path = os.path.join(base_file_path, db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(db_path):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'words.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a sqlite3 database connection and cursor object\n",
    "db_path_name = os.path.join(db_path, db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process control flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy to perform matrix opertions and determine from/to and exact anagram relationships\n",
    "# option 1 - work with the full char_matrix\n",
    "# option 2 - create submatrices by word length\n",
    "# option 3 - create submatrices by word length and letter\n",
    "# option 4 - create submatrices by word length and least common two letters\n",
    "\n",
    "matrix_extraction_option = 1\n",
    "\n",
    "# max number of letters to slice to use for the generation of sub-matrices for\n",
    "# option 4. More letters means more sub-matrices\n",
    "n_common_letters = 3\n",
    "\n",
    "# set write_data to true to store the generated list of anagrams\n",
    "write_data = True\n",
    "\n",
    "# set to None to include all letters\n",
    "# test with a subset of letters by setting the letter_subset_list to ['q', 'x'] or \n",
    "# a different set of letters\n",
    "#letter_subset_list = ['q', 'x', 's']\n",
    "letter_subset_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a timer to record the entire operation\n",
    "total_time_start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word_df, the words from Part 1\n",
    "input_file_name = 'word_df.csv'\n",
    "# build the file path\n",
    "ipn = os.path.join(in_file_path, input_file_name)\n",
    "\n",
    "# specify the datatypes of the columns using a dictionary\n",
    "# because NA and NULL are reserved python words, but also words in our list of words,\n",
    "# we need to specify the data types of the columns\n",
    "dtype_dict = {'word': str,\n",
    "              'lcase': str,\n",
    "              'n_chars': int,\n",
    "              'first_letter': str,\n",
    "              'word_id': int,\n",
    "              'word_group_id': int,\n",
    "              'letter_group': str,\n",
    "              'letter_group_ranked': str}\n",
    "\n",
    "# read in the file and be careful of the NA and NULL values\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "word_df = pd.read_csv(filepath_or_buffer = ipn, sep = '\\t',header = 0,\n",
    "                          dtype=dtype_dict, na_values = '!!', keep_default_na=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the column of word ids as a numpy array\n",
    "word_id_list = word_df['word_id'].to_numpy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the letters sorted by the frequency of words that\n",
    "# start with a particular letter\n",
    "agg_word_df = word_df['first_letter'].groupby(word_df['first_letter']).agg(np.size).to_frame()\n",
    "\n",
    "# set column names\n",
    "agg_word_df.columns = ['word_count']\n",
    "\n",
    "# reset the index to rename columns\n",
    "agg_word_df = agg_word_df.reset_index()\n",
    "\n",
    "# sort the dataframe by frequency\n",
    "agg_word_df = agg_word_df.sort_values(by='word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_word_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the letters sorted by word frequency\n",
    "sorted_first_letters = agg_word_df['first_letter'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the letter dictionary from part 1\n",
    "in_file_name = 'letter_dict.pkl'\n",
    "letter_dict = load_pickle(in_file_path = in_file_path, in_file_name=in_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word dictionary from part 1\n",
    "in_file_name = 'word_dict.pkl'\n",
    "word_dict = load_pickle(in_file_path = in_file_path, in_file_name=in_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the char matrix from part 1\n",
    "in_file_name = 'char_matrix.npy'\n",
    "ipn = os.path.join(in_file_path, in_file_name)\n",
    "char_matrix = np.load(file = ipn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract sub-matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 does not take advantage of submatrices. Options 2, 3, and 4 do.\n",
    "\n",
    "# the dictionary holding the sub-matrices\n",
    "n_char_matrix_dict = {}\n",
    "\n",
    "# by word length\n",
    "word_length_list = sorted(word_df['n_chars'].unique().tolist())\n",
    "\n",
    "# python dictionaries work by storing the hash values of objects\n",
    "# Anything that can be hashed can be a dictionary key. \n",
    "# Computing the hash value of an object ahead of time can reduce dictionary access time.\n",
    "# we'll compute the associated hash value of the tuple used to identify the sub-matrices.\n",
    "\n",
    "word_id_n_char_matrix_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# BUILD OUT SUBMATRICES FOR OPTION 2\n",
    "####\n",
    "# Create submatrices based on the words with at least the same length as the focal word\n",
    "if matrix_extraction_option == 2:\n",
    "    loop_count = 0\n",
    "    s_time = datetime.datetime.now()\n",
    "    n_sub_matrices = len(word_length_list)\n",
    "    print('...creating', n_sub_matrices, 'sub matrices')\n",
    "        \n",
    "    for i_nchars, n_chars in enumerate(word_length_list):\n",
    "        # word id by character length\n",
    "        curr_n_char_word_id_list  = word_df.loc[word_df['n_chars']>=n_chars, 'word_id'].to_numpy()\n",
    "        \n",
    "        #curr_n_char_word_id_list = curr_df['word_id']\n",
    "        curr_char_matrix = char_matrix[curr_n_char_word_id_list, ]\n",
    "                \n",
    "        # use an empty string to form a consistent dictionary key \n",
    "        # across matrix_extraction_options\n",
    "        key_value = (n_chars, '')\n",
    "        key_value_hash = hash(key_value)\n",
    "        n_char_matrix_dict[key_value_hash] = (curr_n_char_word_id_list, curr_char_matrix)        \n",
    "    \n",
    "    # store the tuple in the word_df\n",
    "    word_df['word_id_n_char_matrix_key'] = word_df['n_chars'].map(lambda x: (x, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# BUILD OUT SUBMATRICES FOR OPTIONS 3 OR 4. \n",
    "####\n",
    "if matrix_extraction_option in (3,4):\n",
    "    loop_count = 0\n",
    "    s_time = datetime.datetime.now()\n",
    "    if matrix_extraction_option == 3:\n",
    "        # by word length and first letter\n",
    "        word_df['letter_selector'] = word_df['first_letter']\n",
    "    else:   \n",
    "        # by word length and n least common letters\n",
    "        word_df['letter_selector'] = word_df['letter_group_ranked'].str[:n_common_letters]\n",
    "    \n",
    "    # store the tuple in the word_df\n",
    "    # we have to use tuples because tuples are immutable - once created, they cannot be changed\n",
    "    #https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences    \n",
    "    word_df['word_id_n_char_matrix_key'] = tuple(zip(word_df['n_chars'], word_df['letter_selector']))\n",
    "    \n",
    "    # This is a combinatorial problem.\n",
    "    # Limit the number of selections we need to make    \n",
    "    letter_selector_df = word_df[['n_chars', 'letter_selector']].drop_duplicates()\n",
    "    n_sub_matrices = len(letter_selector_df)\n",
    "    print('...creating', n_sub_matrices, 'sub matrices')\n",
    "    # this means n_sub_matrices are queried.\n",
    "    # we can expedite this by only selecting certain word ids once, converting to a set,\n",
    "    # and then storing that set based on the selection criteria.\n",
    "    # many words are going to have the same least common characters, let's identify the\n",
    "    # corresponding rows accordingly\n",
    "    \n",
    "    letter_selector_list = letter_selector_df['letter_selector'].unique().tolist()\n",
    "    n_char_word_id_list_dict = {}\n",
    "    ls_word_id_list_dict = {}\n",
    "    \n",
    "    for n_chars, letter_selector in zip(letter_selector_df['n_chars'], letter_selector_df['letter_selector']):\n",
    "        \n",
    "        # word id set by character length\n",
    "        if n_chars in n_char_word_id_list_dict:\n",
    "            # get the set if it already exists\n",
    "            curr_n_char_word_id_set = n_char_word_id_list_dict[n_chars]\n",
    "        else:\n",
    "            # create the set if it does not exist\n",
    "            curr_n_char_word_id_set = word_df.loc[(word_df['n_chars']>=n_chars) , 'word_id'].tolist()\n",
    "            curr_n_char_word_id_set = set(curr_n_char_word_id_set)\n",
    "            n_char_word_id_list_dict[n_chars] = curr_n_char_word_id_set\n",
    "\n",
    "        # word id by letter selector\n",
    "        if letter_selector in ls_word_id_list_dict:\n",
    "            # get the set if it already exists\n",
    "            curr_letter_select_word_id_set = ls_word_id_list_dict[letter_selector]\n",
    "        else:\n",
    "            # the set needs to be computed\n",
    "            # build the oolumn selector using list comprehension\n",
    "            column_selector = [letter_dict[curr_letter] for curr_letter in letter_selector]\n",
    "            \n",
    "            # create a true-false matrix where only certain columns, corresponding to\n",
    "            # letter indices, have a value of 1 or more\n",
    "            outcome = char_matrix[:, column_selector] > 0    \n",
    "        \n",
    "            # which rows in the above matrix evaluate to all True\n",
    "            outcome_indices = np.all(a = outcome, axis = 1)\n",
    "        \n",
    "            # these indices match with the word_id_list, extract the subset        \n",
    "            curr_letter_select_word_id_set = word_id_list[outcome_indices]\n",
    "            curr_letter_select_word_id_set = set(curr_letter_select_word_id_set)\n",
    "            ls_word_id_list_dict[letter_selector] = curr_letter_select_word_id_set\n",
    "            \n",
    "        # the set intersection of the curr_n_char_word_id_set and the\n",
    "        # curr_letter_select_word_id_set are indices that feature a word of at\n",
    "        # least a certain length and the characters of interest\n",
    "        outcome_word_id_set = curr_n_char_word_id_set.intersection(curr_letter_select_word_id_set)\n",
    "        # convert the set to an array\n",
    "        outcome_word_id_list = np.array(list(outcome_word_id_set))\n",
    "        \n",
    "        # subset the char_matrix to get the sub matrix\n",
    "        curr_char_matrix = char_matrix[outcome_word_id_list, ]\n",
    "\n",
    "        # now, store that in the sub matrix dictionary\n",
    "        key_value = (n_chars, letter_selector)\n",
    "        key_value_hash = hash(key_value)\n",
    "        n_char_matrix_dict[key_value_hash] = (outcome_word_id_list, curr_char_matrix)\n",
    "        \n",
    "        # simple progress display\n",
    "        loop_count += 1\n",
    "        if loop_count % 100 == 0:\n",
    "            print(loop_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if matrix_extraction_option != 1:    \n",
    "    # populate the key_hash_field and then the word_id_n_char_matrix_dict\n",
    "    # the word_id and the hashed (word_length, letters of interest) tuple are stored\n",
    "    # in a dictionary to expedite comparison. This is a quick way to go from one id to another.    \n",
    "    word_df['word_id_n_char_matrix_key_hash'] = word_df['word_id_n_char_matrix_key'].map(hash)\n",
    "    for curr_word_id, curr_key_hash in zip(word_df['word_id'], word_df['word_id_n_char_matrix_key_hash']):\n",
    "        word_id_n_char_matrix_dict[curr_word_id] = curr_key_hash\n",
    "            \n",
    "    e_time = datetime.datetime.now()\n",
    "    p_time = e_time - s_time\n",
    "    # how long did this pre-processing take?\n",
    "    p_time = round(p_time.total_seconds(), 2)\n",
    "    print('...sub-matrix extraction took', p_time, 'seconds...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_id_n_char_matrix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's examine what we've created, for processing options 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_focal_word = 'orange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if matrix_extraction_option in (3,4):\n",
    "    temp_focal_word_id = word_df.loc[word_df['lcase']==temp_focal_word, 'word_id'].iloc[0]\n",
    "    # the ID of the focal word\n",
    "    print(temp_focal_word_id)\n",
    "    # the hash corresponding to the tuple of the candidate word ids and the sub-matrix\n",
    "    temp_focal_word_hash_id = word_id_n_char_matrix_dict[temp_focal_word_id]\n",
    "    print(temp_focal_word_hash_id)\n",
    "    # the candidate word ids and the sub matrix\n",
    "    temp_word_id_list, temp_sub_matrix = n_char_matrix_dict[temp_focal_word_hash_id]\n",
    "    print(temp_word_id_list.shape)\n",
    "    print(temp_sub_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a function to query the matrix, examine the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(word_id, word_dict, word_id_list, matrix_extraction_option,                \n",
    "               n_char_matrix_dict, word_id_n_char_matrix_dict, char_matrix):\n",
    "    \"\"\" FIND ANAGRAMS FOR A SPECIFIC USING word_id AND MATRIX COMPARISONS    \n",
    "    \"\"\" \n",
    "    \n",
    "    # A USEFUL WAY TO PROTOTYPE, TIME, AND DETERMINE THE\n",
    "    # CORRECTNESS OF PROGRAM OPERATION AND OUTPUT\n",
    "\n",
    "    # get information data based on word id\n",
    "    cw, cw_length, cfl, clg, clgr = word_dict[word_id]   \n",
    "    \n",
    "    if matrix_extraction_option == 1:\n",
    "        outcome = char_matrix - char_matrix[word_id, ]\n",
    "        \n",
    "    if matrix_extraction_option in (2, 3, 4):        \n",
    "        key_hash = word_id_n_char_matrix_dict[word_id]\n",
    "        \n",
    "        cw_id_list, curr_char_matrix = n_char_matrix_dict[key_hash]\n",
    "        # subtract the curr_test_vector from every row in the matrix\n",
    "        # this produces a new matrix.        \n",
    "        new_word_id = cw_id_list==word_id        \n",
    "        outcome = curr_char_matrix - curr_char_matrix[new_word_id, ]\n",
    "        \n",
    "    # compute the score by finding where rows, across all columns, are GTE 0\n",
    "    outcome_indices = np.all(outcome >= 0, axis = 1)\n",
    "    outcome = None        \n",
    "\n",
    "    # extract anagrams based on index values\n",
    "    if matrix_extraction_option == 1:\n",
    "        outcome_word_id_list = word_id_list[outcome_indices]\n",
    "    else:\n",
    "        outcome_word_id_list = cw_id_list[outcome_indices]    \n",
    "    \n",
    "    output_list = np.zeros(shape = (len(outcome_word_id_list), 2),  dtype=np.int32)\n",
    "    \n",
    "    # update the output list with the word_id_list - these are from/parent words    \n",
    "    output_list[:, 0] = outcome_word_id_list\n",
    "    \n",
    "    # update with the word_id - this is the to/child word\n",
    "    output_list[:, 1] = word_id\n",
    "        \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with the word 'quiet', id 160875\n",
    "curr_word_id = 160875\n",
    "curr_word, curr_word_length, curr_first_letter, curr_letter_group, curr_letter_group_ranked = word_dict[curr_word_id] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_id_n_char_matrix_dict[160875]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_values(word_id = curr_word_id, word_dict = word_dict, word_id_list = word_id_list, \n",
    "                    matrix_extraction_option = matrix_extraction_option,\n",
    "                    n_char_matrix_dict = n_char_matrix_dict,\n",
    "                    word_id_n_char_matrix_dict = word_id_n_char_matrix_dict,\n",
    "                    char_matrix = char_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many parent/from words were found for the word 'quiet'?\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an array of from words to the word 'quiet'\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and those words are...\n",
    "word_list = word_df.loc[word_df['word_id'].isin(output[:, 0]), 'lcase'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we've tested with one word, let's time many evaluations to get a sense of how quickly \n",
    "# the current matrix_extraction_option executes\n",
    "# use the timeit() function to evaluate how long, on average, a single matrix operation\n",
    "# takes to complete\n",
    "code_snippet = \"\"\"get_values(word_id = curr_word_id, word_dict = word_dict, word_id_list = word_id_list, \n",
    "                    matrix_extraction_option = matrix_extraction_option,\n",
    "                    n_char_matrix_dict = n_char_matrix_dict,\n",
    "                    word_id_n_char_matrix_dict = word_id_n_char_matrix_dict,\n",
    "                    char_matrix = char_matrix)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "total_time = timeit.timeit(code_snippet,\n",
    "              number=n_trials, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average number of seconds per trial\n",
    "total_time / n_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate total number of from/to word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many anagrams are there?\n",
    "# let's estimate the number of anagrams by assuming that the number of\n",
    "# parent/from words is a function of word length. \n",
    "# let's sample 10 words of each word length, compute the number of from/parent anagrams\n",
    "# for each word in the sample, compute the min, mean, and max, and apply those values\n",
    "# to the numbers of words by length and multiply accordingly\n",
    "# this will give us very generous upper bound of anagram pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the number of characters per word\n",
    "n_char_list = sorted(word_df['n_chars'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate and sample\n",
    "output_list = []\n",
    "for i_n_char, n_char in enumerate(n_char_list):\n",
    "    curr_id_list = word_df.loc[word_df['n_chars']==n_char, 'word_id'].to_numpy()\n",
    "    # sample with replacement\n",
    "    sample_id_list = np.random.choice(a = curr_id_list, size = 10, replace = True)\n",
    "    for sid in sample_id_list:\n",
    "        output = get_values(word_id = sid, word_dict = word_dict, word_id_list = word_id_list, \n",
    "                    matrix_extraction_option = matrix_extraction_option,\n",
    "                    n_char_matrix_dict = n_char_matrix_dict,\n",
    "                    word_id_n_char_matrix_dict = word_id_n_char_matrix_dict,\n",
    "                    char_matrix = char_matrix)\n",
    "        curr_from_words = len(output)\n",
    "        curr_output = [n_char, curr_from_words]\n",
    "        output_list.append(curr_output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe\n",
    "pos_df = pd.DataFrame(data = output_list, columns = ['n_chars', 'n_from_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum, max, and mean number of from words\n",
    "agg_pos_df = pos_df.groupby('n_chars').agg([np.min, np.max, np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_pos_df.columns = ['min_n_from_words', 'max_n_from_words', 'mean_n_from_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's aggregate by number of letters per word, and then join\n",
    "n_word_length_df = word_df['n_chars'].groupby(word_df['n_chars']).agg(np.size).to_frame()\n",
    "n_word_length_df.columns = ['n_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos_df = pd.merge(left = n_word_length_df, right = agg_pos_df, left_index = True,\n",
    "                   right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos_df['n_tot_max_anagrams'] = n_pos_df['n_words'] * n_pos_df['max_n_from_words']\n",
    "n_pos_df['n_tot_mean_anagrams'] = n_pos_df['n_words'] * n_pos_df['mean_n_from_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the upper bound of anagrams as the midway point\n",
    "# between the mean and the max of the estimated number of anagrams\n",
    "n_possible_anagrams = (n_pos_df['n_tot_mean_anagrams'].sum() + n_pos_df['n_tot_max_anagrams'].sum()) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round and convert to integer\n",
    "n_possible_anagrams = int(np.round(n_possible_anagrams, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this number will be used to create an array that will hold the from/to pairs\n",
    "n_possible_anagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discover from/to word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize counters to count the number of to (child words) from a focal word.\n",
    "# we could do this in post-processing, but the data are already in memory and it's a simple \n",
    "# calculation to make.\n",
    "# we want to minimize the number of trips through our data.\n",
    "\n",
    "# the number of candidate words examined for each focal word\n",
    "\n",
    "# a list to hold the dataframes generated for each letter\n",
    "proc_time_df_list = []\n",
    "\n",
    "# subset the list of leters\n",
    "if letter_subset_list:\n",
    "    letters = letter_subset_list[:]\n",
    "else:\n",
    "    letters = sorted_first_letters\n",
    "\n",
    "anagram_pair_count = 0 \n",
    "# use numpy to pre-allocate an array that will be updated while enumerating. \n",
    "# this eliminates list.append() calls\n",
    "# note: I am guessing that there are 150M anagram pairs. \n",
    "\n",
    "output_list = np.full(shape = (n_possible_anagrams, 3), fill_value = -1,  dtype=np.int32)\n",
    "\n",
    "for i_cl, curr_letter in enumerate(letters):\n",
    "    # enumerate by each letter\n",
    "    # this isn't absolutely necessary, we could just enumerate by word id, \n",
    "    # but for testing and development, letters are a handy way to chunk up the data. \n",
    "\n",
    "    # this dictionary will store the calculations for each letter\n",
    "    proc_time_dict = {}    \n",
    "    \n",
    "    # the list of words that start with the focal letter     \n",
    "    curr_word_df = word_df.loc[word_df['first_letter'] == curr_letter, :]\n",
    "    \n",
    "    # sort the dataframe by n_chars and letter_selector, if it exists.\n",
    "    # this will cut down on dictionary lookups for matrix_extraction_types 3 and 4.    \n",
    "    if 'letter_selector' in word_df.columns.tolist():\n",
    "        curr_word_df = curr_word_df.sort_values(by = ['n_chars', 'letter_selector'])\n",
    "    else:\n",
    "        curr_word_df = curr_word_df.sort_values(by = ['n_chars'])\n",
    "        \n",
    "    curr_word_id_list = curr_word_df['word_id'].tolist()\n",
    "    \n",
    "    n_curr_words = '{:,}'.format(len(curr_word_df))    \n",
    "    print('...finding parent anagrams for', n_curr_words, 'words that start with', curr_letter)               \n",
    "    \n",
    "    # enumerate by word id, working with integers is faster than words    \n",
    "    for i_wi, word_id in enumerate(curr_word_id_list):            \n",
    "        # start timing to record processing for each word            \n",
    "        s_time = datetime.datetime.now()\n",
    "        \n",
    "        # get the current word length, from the word id\n",
    "        #to_word, to_word_length, curr_first_letter, clg, clgr = word_dict[word_id]   \n",
    "        to_word_length = word_dict[word_id][1]\n",
    "        \n",
    "        if matrix_extraction_option == 1:\n",
    "            outcome = char_matrix - char_matrix[word_id, ]\n",
    "            n_possible_words = char_matrix.shape[0]            \n",
    "\n",
    "        if matrix_extraction_option in (2, 3, 4):        \n",
    "            \n",
    "            # get the tuple associated with the word id\n",
    "            # much faster to look up stored values for the hash value than it is to \n",
    "            # only look up if the hash value has changed            \n",
    "            key_hash = word_id_n_char_matrix_dict[word_id]                \n",
    "            # get the possible candidate word_ids and char matrix\n",
    "            curr_word_id_index_list, curr_char_matrix = n_char_matrix_dict[key_hash]                                \n",
    "        \n",
    "            # how many candidates?\n",
    "            n_possible_words = len(curr_word_id_index_list)\n",
    "        \n",
    "            # subtract the curr_test_vector from every row in the matrix\n",
    "            # this produces a new matrix.        \n",
    "            new_word_id = curr_word_id_index_list == word_id            \n",
    "            outcome = curr_char_matrix - curr_char_matrix[new_word_id, ]\n",
    "                        \n",
    "        # compute the score by finding where rows, across all columns, are GTE 0\n",
    "        outcome_indices = np.all(outcome >= 0, axis = 1)\n",
    "        outcome = None        \n",
    "        \n",
    "        # extract anagrams based on same index values\n",
    "        if matrix_extraction_option == 1:\n",
    "            outcome_word_id_list = word_id_list[outcome_indices].tolist()\n",
    "        else:                \n",
    "            outcome_word_id_list = curr_word_id_index_list[outcome_indices].tolist()\n",
    "            \n",
    "        outcome_indices = None               \n",
    "        \n",
    "        # if the outcome is greater than or equal to zero, then the current word is an\n",
    "        # anagram of the other word    \n",
    "        # a value  >= 0 means that the current word contains the exact same number of focal letters\n",
    "        # mite --> time or miter --> time\n",
    "        # a value >= 1 means that current word contains at least the same number of focal letters\n",
    "        # terminator --> time\n",
    "        # a value of <=-1 means that the current word does not have the \n",
    "        # correct number of letters and is therefore not an anagram.\n",
    "        # trait <> time        \n",
    "\n",
    "        # number of parent words found\n",
    "        n_from_words = len(outcome_word_id_list)\n",
    "\n",
    "        if n_from_words > 1:\n",
    "            \n",
    "            # we have matches\n",
    "            # the focal word   \n",
    "                                    \n",
    "            # enumerate the from/parent words\n",
    "            # from word length\n",
    "            from_word_length_list = [word_dict[from_word_id][1] for from_word_id in outcome_word_id_list]\n",
    "            same_word_length_list = [1 if fwl == to_word_length else 0 for fwl in from_word_length_list]\n",
    "            \n",
    "            new_anagram_pair_count = anagram_pair_count + len(from_word_length_list)\n",
    "            # the from words\n",
    "            output_list[anagram_pair_count:new_anagram_pair_count, 0] = outcome_word_id_list        \n",
    "            # the to word\n",
    "            output_list[anagram_pair_count:new_anagram_pair_count, 1] = word_id                                            \n",
    "            # same length\n",
    "            output_list[anagram_pair_count:new_anagram_pair_count, 2] = same_word_length_list                                                                       \n",
    "            # set the anagram pair count\n",
    "            anagram_pair_count = new_anagram_pair_count\n",
    "                    \n",
    "                \n",
    "        del outcome_word_id_list\n",
    "            \n",
    "        # record the time for the word\n",
    "        e_time = datetime.datetime.now()\n",
    "        p_time = e_time - s_time    \n",
    "        p_time = p_time.total_seconds()\n",
    "\n",
    "        proc_time_dict[word_id] = (p_time, n_from_words, n_possible_words)       \n",
    "    \n",
    "    # create a dataframe from the proc_time_dict\n",
    "    proc_time_df = pd.DataFrame.from_dict(data=proc_time_dict, orient='index')\n",
    "    proc_time_df = proc_time_df.reset_index()\n",
    "    proc_time_df.columns = ['word_id', 'n_seconds', 'n_from_words', 'n_candidates']                \n",
    "    \n",
    "    # display processing time for the current letter\n",
    "    total_proc_time = round(proc_time_df['n_seconds'].sum(), 2)\n",
    "    print('...finding parent anagrams for', curr_letter, 'words took', total_proc_time, 'seconds...')\n",
    "    \n",
    "    proc_time_df_list.append(proc_time_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape and store output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the output array to only include indices with a from/to word pair\n",
    "output_indices = np.all(output_list >= 0, axis = 1)\n",
    "output_list = output_list[output_indices, ]\n",
    "del output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many anagram pairs were found?\n",
    "n_total_anagrams = len(output_list)\n",
    "n_total_anagrams_formatted = '{:,}'.format(n_total_anagrams)\n",
    "print('...total anagrams', n_total_anagrams_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count the number of to words, same length to words, and different length to words for each word using counters\n",
    "# https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "# number of to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the count of to words\n",
    "to_word_counter = collections.Counter(output_list[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the count of same length to words\n",
    "curr_indices = output_list[:, 2] == 1\n",
    "curr_output_list = output_list[curr_indices, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slt_word_counter = collections.Counter(curr_output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the count of different length to words\n",
    "# substract the counters from each other\n",
    "dlt_word_counter = to_word_counter - slt_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create database connection objects\n",
    "db_conn = build_db_conn(db_path = db_path, db_name = db_name)\n",
    "db_cursor = db_conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write anagram pairs to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the anagram pairs to the database\n",
    "if write_data:\n",
    "\n",
    "    # let's write to the SQLite database in chunks of 1M records\n",
    "    break_point_list = list(range(0, len(output_list), 1000000))\n",
    "    # add the last bit of records\n",
    "    if break_point_list[-1] < len(output_list):\n",
    "        break_point_list.append(len(output_list))\n",
    "    \n",
    "    # drop the anagrams table if it previously exists\n",
    "    sql = 'drop table if exists anagrams;'\n",
    "    \n",
    "    print('...dropping previous table...')\n",
    "    # send the sql statement to the database and commit the changes\n",
    "    db_cursor.execute(sql)\n",
    "    db_conn.commit()\n",
    "\n",
    "    # create the anagrams table\n",
    "    sql = 'create table anagrams ( from_word_id integer, to_word_id integer, same_word_length integer);'\n",
    "\n",
    "    # execute the statement and commit changes    \n",
    "    db_cursor.execute(sql)\n",
    "    db_conn.commit()\n",
    "        \n",
    "    # objects to record write time\n",
    "    db_write_time_list = []\n",
    "    db_write_time_start = datetime.datetime.now()\n",
    "    \n",
    "    # create a sql statement that we'll use to insert values.\n",
    "    print('...beginning to add anagram word pairs...')\n",
    "    base_sql = 'insert into anagrams values (?,?,?)'    \n",
    "    \n",
    "    insert_count = 0    \n",
    "    curr_db_write_time_start = datetime.datetime.now()\n",
    "    for i_bp, bp in enumerate(break_point_list[:-1]):\n",
    "        # slice the output list of word id pairs, convert to a python list\n",
    "        # the numpy.int data type is not compatable with sqlite.\n",
    "        # the cursor.executemany() is a quick way to write a lot of data.\n",
    "        next_bp = break_point_list[i_bp + 1]\n",
    "        \n",
    "        # converting the entire output_list to a python list adds too much overheard.\n",
    "        test_output_list = output_list[bp:next_bp, ].tolist()\n",
    "        \n",
    "        # use the executemany() function to write records\n",
    "        #https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.executemany\n",
    "        db_cursor.executemany(base_sql, test_output_list)\n",
    "        \n",
    "        # commit changes every 10M records        \n",
    "        if next_bp % 10000000 == 0:\n",
    "            print('...commiting changes:', '{:,}'.format(next_bp), 'records')\n",
    "            db_conn.commit()                       \n",
    "            curr_db_write_time_end = datetime.datetime.now()\n",
    "            curr_db_write_time_proc = curr_db_write_time_end - curr_db_write_time_start            \n",
    "            curr_db_write_time_proc = curr_db_write_time_proc.total_seconds()\n",
    "            curr_db_write_time_start = datetime.datetime.now()\n",
    "            db_write_time_list.append(curr_db_write_time_proc)                                      \n",
    "\n",
    "            # compute average write time, display after 1M writes\n",
    "            mean_write_time = np.mean(db_write_time_list)\n",
    "\n",
    "            # compute ETA            \n",
    "            n_seconds = (n_total_anagrams / 10000000) * mean_write_time\n",
    "            add_seconds = datetime.timedelta(seconds = n_seconds)\n",
    "            eta_write_complete = db_write_time_start + add_seconds            \n",
    "            eta_write_complete = eta_write_complete.strftime(format = \"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "            mean_write_time = round(mean_write_time, 3)\n",
    "            print('...average write time per 10M records:', mean_write_time, 'seconds...')\n",
    "            print('...estimated write complete time:', eta_write_complete)\n",
    "        \n",
    "    # commit the last round of changes\n",
    "    print('...commiting changes:', '{:,}'.format(len(test_output_list)), 'records')\n",
    "    db_conn.commit()\n",
    "    \n",
    "    # compute total write times\n",
    "    db_write_time_end = datetime.datetime.now()\n",
    "    db_write_time_proc = db_write_time_end - db_write_time_start\n",
    "    db_write_time_proc = db_write_time_proc.total_seconds() / 60\n",
    "    db_write_time_proc = round(db_write_time_proc, 2)\n",
    "    print('...writing to db took', db_write_time_proc, 'minutes')\n",
    "    \n",
    "    del test_output_list\n",
    "    \n",
    "# remove the list of from/to word pairs\n",
    "del output_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store number of from/to word pairs and time related to processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the processing times\n",
    "proc_time_df = pd.concat(proc_time_df_list)\n",
    "\n",
    "# drop columns related to data processing\n",
    "drop_col_names = ['letter_selector', 'word_id_n_char_matrix_key',\n",
    "                  'word_id_n_char_matrix_key_hash']\n",
    "curr_col_names = word_df.columns.tolist()\n",
    "for dcn in drop_col_names:    \n",
    "    if dcn in curr_col_names:\n",
    "        word_df = word_df.drop(dcn, axis = 1)\n",
    "\n",
    "# merge the word_df and the proc_time_df dataframes to get the processing time per word\n",
    "word_df = pd.merge(left=word_df, right = proc_time_df)\n",
    "\n",
    "# now, use the map function to get the number of from/to words and the number of\n",
    "# candidate words for each word\n",
    "word_df['n_to_words'] = word_df['word_id'].map(to_word_counter)\n",
    "word_df['n_slt_words'] = word_df['word_id'].map(slt_word_counter)\n",
    "word_df['n_dlt_words'] = word_df['word_id'].map(dlt_word_counter)\n",
    "#word_df['n_candidates'] = word_df['word_id'].map(n_possible_words_counter)\n",
    "\n",
    "# record the matrix extraction option\n",
    "word_df['matrix_extraction_option'] = matrix_extraction_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange columns\n",
    "col_names = ['word','lcase','n_chars','first_letter','word_id',\n",
    "             'word_group_id','letter_group','letter_group_ranked','n_seconds',\n",
    "             'n_from_words','n_to_words','n_slt_words','n_dlt_words','n_candidates',\n",
    "             'matrix_extraction_option']\n",
    "word_df = word_df[col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output table name\n",
    "table_name = 'words_me_' + str(matrix_extraction_option).zfill(2)\n",
    "# write the processing option table\n",
    "word_df.to_sql(name=table_name, con=db_conn, if_exists='replace', index = False)    \n",
    "# write the words table\n",
    "word_df.to_sql(name='words', con=db_conn, if_exists='replace', index = False)    \n",
    "    \n",
    "# close the connection\n",
    "db_cursor.close()\n",
    "db_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anagram_discovery_time = word_df['n_seconds'].sum()\n",
    "anagram_discovery_time = anagram_discovery_time / 60\n",
    "anagram_discovery_time = round(anagram_discovery_time, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...anagram discovery time:', anagram_discovery_time, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the total time\n",
    "total_time_end = datetime.datetime.now()\n",
    "total_time_proc = total_time_end - total_time_start\n",
    "total_time_proc = total_time_proc.total_seconds()\n",
    "total_time_proc = total_time_proc / 60\n",
    "total_time_proc = round(total_time_proc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('...total processing time:', total_time_proc, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
