{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mike Babb\n",
    "# babbm@uw.edu\n",
    "# Find Anagrams\n",
    "## Part 1: Structure the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries - installed by default\n",
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external libraries - not installed by default\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom, user-defined functions\n",
    "from part_00_file_db_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set input and output paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import list of words, shape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_and_format_words(in_fpn:str):\n",
    "    \n",
    "    # use pandas to load the data\n",
    "    # htps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    print('...Reading in list of words...')\n",
    "    word_df = pd.read_csv(filepath_or_buffer = in_fpn, sep = ',', header = None, names = ['word'])\n",
    "    \n",
    "    # how many words are we working with?\n",
    "    n_words = len(word_df)\n",
    "    print('...found', '{:,}'.format(n_words), 'words to find anagrams for...')\n",
    "    \n",
    "    # convert the only column to a string - just to be safe.\n",
    "    # 'nan' is a word in the dictionary. nan is an internal python value.\n",
    "    # same with 'null'\n",
    "    word_df['word'] = word_df['word'].astype(str)\n",
    "    \n",
    "    # create lower case values of the words\n",
    "    word_df['lcase'] = word_df['word'].str.lower()\n",
    "    \n",
    "    # remove hyphens\n",
    "    word_df['lcase'] = word_df['lcase'].str.replace('-', '')\n",
    "    \n",
    "    # and now drop duplicates, based on the lowercase version of each word\n",
    "    word_df = word_df.drop_duplicates('lcase')\n",
    "    \n",
    "    # Approximately 234K words. That's a lot of words. \n",
    "    \n",
    "    # find word length\n",
    "    word_df['n_chars'] = word_df['lcase'].str.len()\n",
    "    \n",
    "    # extract the first letter of each word\n",
    "    word_df['first_letter'] = word_df['lcase'].str[:1]\n",
    "    \n",
    "    # create an id\n",
    "    word_df['word_id'] = range(0, len(word_df))\n",
    "    \n",
    "    # add a hash id to capture the sorted letters in each word\n",
    "    # use map() with a lambda function to chain several operations together\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html\n",
    "    # as an example of what this is doing...\n",
    "    \n",
    "    # now, do this for all 234K words. \n",
    "    word_df['hash_id'] = word_df['lcase'].map(lambda x: hash(''.join(sorted(x))))\n",
    "\n",
    "    return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_group_id(word_df:pd.DataFrame) -> pd.DataFrame:        \n",
    "    \n",
    "    # create a dataframe of the unique, hashed values\n",
    "    word_id_hash_id_df = word_df['hash_id'].drop_duplicates().to_frame()\n",
    "    \n",
    "    # add a unique id\n",
    "    word_id_hash_id_df['word_group_id'] = range(0, len(word_id_hash_id_df))\n",
    "    \n",
    "    # create a dictionary using dictionary comprehension of the hash values using zip\n",
    "    # https://docs.python.org/3/library/functions.html#zip\n",
    "    hash_id_dict = {hash_id:word_group_id for word_group_id, hash_id in zip(word_id_hash_id_df['word_group_id'], word_id_hash_id_df['hash_id'])}\n",
    "    \n",
    "    # apply the word group id to the \n",
    "    word_df['word_group_id'] = word_df['hash_id'].map(hash_id_dict)\n",
    "    \n",
    "    # drop the hash id, no longer needed\n",
    "    word_df = word_df.drop(labels = 'hash_id', axis = 1)    \n",
    "\n",
    "    # use dictionary comprehension to store the letter\n",
    "    # we'll import the letters from string.ascii_lowercase \n",
    "    # index of the letter for fast look ups\n",
    "    letter_dict = {l:li for li, l in enumerate(string.ascii_lowercase)}\n",
    "    \n",
    "    # generate a list of letters from the string.ascii_lowercase\n",
    "    letters = string.ascii_lowercase\n",
    "    \n",
    "    # get the unique letters in each word and then sort those letters\n",
    "    word_df['letter_group'] = word_df['lcase'].map(lambda x: ''.join(sorted(set(x))))\n",
    "\n",
    "    return word_df, letter_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count letter frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_letter_frequency(word_df:pd.DataFrame):    \n",
    "    \n",
    "    # several versions of the anagram determination technique require subsetting by letters in each word. \n",
    "    # generate those data and use a ranking technique to help with anagram group identification\n",
    "    \n",
    "    # use a counter object to count the total occurences of each letter AND\n",
    "    # a counter to count the number of words that feature each letter\n",
    "    # counters are a special type of dictionary. \n",
    "    # https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "    # very fast\n",
    "    total_letter_counter = collections.Counter()\n",
    "    single_letter_counter = collections.Counter()\n",
    "    \n",
    "    # enumerate each word and then each letter\n",
    "    for curr_word in word_df['lcase'].to_numpy():\n",
    "        total_letter_counter.update(list(curr_word))\n",
    "    \n",
    "    for curr_letter_group in word_df['letter_group'].to_numpy():\n",
    "        single_letter_counter.update(list(curr_letter_group))\n",
    "\n",
    "    # make a dataframe from the counter object and then order from low to high\n",
    "    letter_count_df = pd.DataFrame.from_dict(data=total_letter_counter, orient = 'index', columns = ['total_letter_count']).reset_index(names=['letter'])\n",
    "\n",
    "    letter_count_df['single_letter_count'] = letter_count_df['letter'].map(single_letter_counter)\n",
    "    \n",
    "    # in this case, 'a' is featured in 144,511 words.\n",
    "    \n",
    "    # compute the total letter rank and the single_letter_count\n",
    "    letter_count_df['total_letter_rank'] = letter_count_df['total_letter_count'].rank(ascending=False).astype(int)\n",
    "    letter_count_df['single_letter_rank'] = letter_count_df['single_letter_count'].rank(ascending=False).astype(int)  \n",
    "        \n",
    "    # sort by letter count\n",
    "    letter_count_df = letter_count_df.sort_values(by = 'total_letter_count', ascending = False)\n",
    "    \n",
    "    letter_count_df['total_letter_percent'] = letter_count_df['total_letter_count'] / letter_count_df['total_letter_count'].sum()\n",
    "    # note the denomiantor - we are computing which words have a letter, most words have multiple letters. \n",
    "    # two thirds of words feature the letter 'e'. Wow. \n",
    "    letter_count_df['single_letter_percent'] = letter_count_df['single_letter_count'] / word_df.shape[0]\n",
    "\n",
    "    # join with the count of words that start with a focal letter. \n",
    "    \n",
    "    fl_count_df = word_df['first_letter'].groupby(word_df['first_letter']).agg(np.size).to_frame(name = 'first_letter_word_count').reset_index(names = ['letter'])\n",
    "    \n",
    "    fl_count_df['first_letter_word_percent'] = fl_count_df['first_letter_word_count'] / fl_count_df['first_letter_word_count'].sum()\n",
    "    \n",
    "    fl_count_df['first_letter_rank'] = fl_count_df['first_letter_word_count'].rank(ascending = False).astype(int)\n",
    "    \n",
    "    # joins\n",
    "    letter_count_df = pd.merge(left=letter_count_df, right = fl_count_df,\n",
    "                              left_on=['letter'], right_on = ['letter'])\n",
    "    \n",
    "    # sort the records\n",
    "    letter_count_df = letter_count_df.sort_values(by = 'letter')\n",
    "    \n",
    "    # reorder columns\n",
    "    col_names = ['letter',\n",
    "    'total_letter_count',\n",
    "    'single_letter_count',\n",
    "    'first_letter_word_count',\n",
    "    'total_letter_percent',\n",
    "    'single_letter_percent',\n",
    "    'first_letter_word_percent',\n",
    "    'total_letter_rank',\n",
    "    'single_letter_rank',\n",
    "    'first_letter_rank']\n",
    "    \n",
    "    letter_count_df = letter_count_df[col_names]\n",
    "    \n",
    "    # place the letter and its rank into a dictionary \n",
    "    # as well as the rank and the corresponding letter\n",
    "    # {'k':21, 21:'k'}\n",
    "    letter_count_rank_dict = {}\n",
    "    for cl, clr in zip(letter_count_df['letter'], letter_count_df['total_letter_rank']):\n",
    "        letter_count_rank_dict[cl] = clr\n",
    "        letter_count_rank_dict[clr] = cl\n",
    "    \n",
    "    # write a function to order the unique letters in each word by\n",
    "    # least common letter to most common letter\n",
    "    def get_least_common_letters(word):    \n",
    "        if len(word) == 1:\n",
    "            lcl = word\n",
    "        else:\n",
    "            # ranking of each letter\n",
    "            rank_list = [letter_count_rank_dict[curr_letter] for curr_letter in word]        \n",
    "            # sort the ranking\n",
    "            rank_list = sorted(rank_list, reverse = True)\n",
    "            # generate the letters sorted by rank\n",
    "            rank_list = [letter_count_rank_dict[curr_letter] for curr_letter in rank_list]\n",
    "            lcl = ''.join(rank_list)\n",
    "        return lcl        \n",
    "    \n",
    "    # extract letters by ranking\n",
    "    word_df['letter_group_ranked'] = word_df['letter_group'].map(get_least_common_letters)\n",
    "\n",
    "    return word_df, letter_count_df, letter_count_rank_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate the character matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_character_matrix(word_df:pd.DataFrame) -> np.ndarray:\n",
    "    # count the occurences of each letter in each word and store the results in a matrix\n",
    "    # populate the char_matrix and the word_id dictionary\n",
    "    # Aapply a function to each row in the dataframe\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\n",
    "    \n",
    "    # Upon intialization, the char_matrix is zero-filled.\n",
    "    # Each row in the char_matrix corresponds to a word.\n",
    "    # The char_matrix is 26 columns wide. Each column corresponds to a letter.\n",
    "    # ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "    # Each cell is a count of the number of times each letter occurs in each word.  \n",
    "    # the entry for emit (as do the entriees for time, mite, item) has the following value:\n",
    "    # [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    # we need to find all words that have matching rows with at least these values.\n",
    "    # for example, 'terminator'.\n",
    "    # ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "    # [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    # the zero-filled matrix will be populated once the \n",
    "    # fill_char_matrix() function is applied to the word_df\n",
    "    char_matrix = np.zeros(shape=(len(word_df), 26), dtype=int)\n",
    "    def fill_char_matrix(row):\n",
    "        # get a word from the current row\n",
    "        curr_word = row['lcase']    \n",
    "        ri = row['word_id'] # row index / word index    \n",
    "        # populate the char matrix\n",
    "        for i_letter, letter in enumerate(curr_word):\n",
    "            if letter in letter_dict:\n",
    "                # find the corresponding column index of that letter\n",
    "                li = letter_dict[letter]\n",
    "                # increment the count of letters in the current row and current column\n",
    "                char_matrix[ri, li] += 1\n",
    "        return None\n",
    "    \n",
    "    # catch the output from the function and delete\n",
    "    output = word_df.apply(fill_char_matrix, 1)\n",
    "    del output\n",
    "\n",
    "    return char_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and save the word_group dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_group_df(word_df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "\n",
    "    # drop duplicates based on the word group. \n",
    "    # by default, this will only keep the first record and it will drop all others\n",
    "    wg_df = word_df.drop_duplicates(subset = ['word_group_id']).copy()\n",
    "    \n",
    "    word_group_counter = collections.Counter(word_df['word_group_id'])\n",
    "    \n",
    "    wg_df['word_group_count'] = wg_df['word_group_id'].map(word_group_counter)\n",
    "\n",
    "    return wg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save data to disk - first the char matrix and the letter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_objects(char_matrix:np.ndarray, letter_dict:dict, word_df:pd.DataFrame,\n",
    "                 wg_df:pd.DataFrame, letter_count_df:pd.DataFrame,\n",
    "                 output_file_path:str, db_name:str) -> None:\n",
    "\n",
    "    # save the char matrix\n",
    "    output_name = 'char_matrix.npy'\n",
    "    opn = os.path.join(output_file_path, output_name)\n",
    "    np.save(file = opn, arr = char_matrix)\n",
    "    \n",
    "    # letter dictionary\n",
    "    output_name = 'letter_dict.pkl'\n",
    "    save_pickle(file_path = output_file_path, file_name = output_name, obj = letter_dict)\n",
    "    \n",
    "    # Now, the dataframes    \n",
    "    # save the word df to sqlite db\n",
    "    write_data_to_sqlite(df = word_df, table_name = 'words', db_path = output_file_path, db_name = db_name)\n",
    "    \n",
    "    write_data_to_sqlite(df = wg_df, table_name = 'word_groups', db_path = output_file_path, db_name = db_name)\n",
    "    \n",
    "    # now, the word / letter count\n",
    "    write_data_to_sqlite(df = letter_count_df, table_name = 'letter_count', db_path = output_file_path, db_name = db_name)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path and name of input data\n",
    "in_file_path = '/git/finding_anagrams/data/'\n",
    "in_file_name = 'words.txt'\n",
    "\n",
    "base_output_file_path = '/project/finding_anagrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Reading in list of words...\n",
      "...found 235,886 words to find anagrams for...\n",
      "...now writing: words\n",
      "...now writing: word_groups\n",
      "...now writing: letter_count\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# construct the input file path\n",
    "in_fpn = os.path.join(in_file_path, in_file_name)\n",
    "\n",
    "# paths to output directories\n",
    "output_file_path = os.path.join(base_output_file_path, 'data')\n",
    "db_name = 'words.db'\n",
    "\n",
    "# setup the data output path\n",
    "if os.path.exists(output_file_path):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "\n",
    "word_df = import_and_format_words(in_fpn = in_fpn)\n",
    "\n",
    "word_df, letter_dict = compute_word_group_id(word_df = word_df)\n",
    "\n",
    "word_df, letter_count_df, letter_count_rank_dict = count_letter_frequency(word_df = word_df)\n",
    "\n",
    "char_matrix = generate_character_matrix(word_df = word_df)\n",
    "\n",
    "wg_df = create_word_group_df(word_df = word_df)\n",
    "\n",
    "\n",
    "\n",
    "save_objects(char_matrix = char_matrix, letter_dict = letter_dict, word_df = word_df,\n",
    "                 wg_df = wg_df, letter_count_df = letter_count_df,\n",
    "                 output_file_path = output_file_path, db_name=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_it(in_file_path:str, in_file_name:str, \n",
    "           base_output_file_path:str, db_name:str):    \n",
    "    \n",
    "    # construct the input file path\n",
    "    in_fpn = os.path.join(in_file_path, in_file_name)\n",
    "    \n",
    "    # paths to output directories\n",
    "    output_file_path = os.path.join(base_output_file_path, 'data')    \n",
    "    \n",
    "    # setup the data output path\n",
    "    if os.path.exists(output_file_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(output_file_path)\n",
    "    \n",
    "    \n",
    "    word_df = import_and_format_words(in_fpn = in_fpn)\n",
    "    \n",
    "    word_df, letter_dict = compute_word_group_id(word_df = word_df)\n",
    "    \n",
    "    word_df, letter_count_df, letter_count_rank_dict = count_letter_frequency(word_df = word_df)\n",
    "    \n",
    "    char_matrix = generate_character_matrix(word_df = word_df)\n",
    "    \n",
    "    wg_df = create_word_group_df(word_df = word_df)    \n",
    "    \n",
    "    \n",
    "    save_objects(char_matrix = char_matrix, letter_dict = letter_dict, word_df = word_df,\n",
    "                     wg_df = wg_df, letter_count_df = letter_count_df,\n",
    "                     output_file_path = output_file_path, db_name=db_name)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Reading in list of words...\n",
      "...found 235,886 words to find anagrams for...\n",
      "...now writing: words\n",
      "...now writing: word_groups\n",
      "...now writing: letter_count\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # path and name of input data\n",
    "    in_file_path = '/git/finding_anagrams/data/'\n",
    "    in_file_name = 'words.txt'\n",
    "\n",
    "    base_output_file_path = '/project/finding_anagrams'\n",
    "\n",
    "    db_name = 'words.db'\n",
    "\n",
    "    run_it(in_file_path = in_file_path, in_file_name = in_file_name, \n",
    "           base_output_file_path = base_output_file_path, db_name = db_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
