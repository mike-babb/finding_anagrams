{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mike Babb\n",
    "# babbm@uw.edu\n",
    "# Find anagrams\n",
    "## Part 2: Generate and store the anagrams v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard libraries - installed by default\n",
    "import collections\n",
    "import datetime\n",
    "import pickle\n",
    "import sqlite3\n",
    "import string\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# external libraries - not installed by default\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from part_00_file_db_utils import *\n",
    "from part_00_process_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base file path\n",
    "base_file_path = '/project/finding_anagrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input path\n",
    "in_file_path = 'data'\n",
    "in_file_path = os.path.join(base_file_path, in_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output db path and name\n",
    "db_path = 'db'\n",
    "db_path = os.path.join(base_file_path, db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(db_path):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name = 'words.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process control flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use numpy to perform matrix opertions and determine from/to and exact anagram relationships\n",
    "# Option 1: Full matrix\n",
    "# Option 2: Word-length\n",
    "# Option 3: First letter\n",
    "# Option 4: Single-least common letter\n",
    "# Option 5: n least common letters\n",
    "# Option 6: word-length and n least common letters\n",
    "\n",
    "matrix_extraction_option = 6\n",
    "\n",
    "# max number of letters to slice to use for the generation of sub-matrices for\n",
    "# options 5 and 6. More letters means more sub-matrices\n",
    "# 3 seems to be the sweet spot\n",
    "n_subset_letters = 3\n",
    "\n",
    "# set write_data to True to store the generated list of anagrams\n",
    "write_data = False\n",
    "\n",
    "# set to None to include all letters\n",
    "# test with a subset of letters by setting the letter_subset_list to ['q', 'x'] or \n",
    "# a different set of letters\n",
    "letter_subset_list = ['x']\n",
    "# letter_subset_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start a timer to record the entire operation\n",
    "total_time_start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df, wg_df, letter_dict, char_matrix, word_group_id_list, word_id_list, wchar_matrix = load_input_data(db_path = db_path, db_name = db_name, in_file_path = in_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the char_matrix into N sub matrices\n",
    "# See split_matrix() for a more elaborate description. \n",
    "# This function does a lot of things. Effectively, it computes and stores values in the wg_df, and splits the matrix into various components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_matrix(\n",
    "    letter_dict: dict,\n",
    "    word_group_id_list: np.ndarray,    \n",
    "    wg_df: pd.DataFrame,\n",
    "    wchar_matrix: np.ndarray,\n",
    "    n_subset_letters:int\n",
    "):\n",
    "    s_time = datetime.datetime.now()\n",
    "    \n",
    "    # create the letter selector and determine the max number\n",
    "    # of sub-matrices to makes\n",
    "    \n",
    "    \n",
    "    wg_df['letter_selector'] = wg_df['letter_group_ranked'].str[:n_subset_letters]\n",
    "    wg_df['first_letter_id'] = wg_df['first_letter'].map(letter_dict)\n",
    "    wg_df['single_letter_id'] = wg_df['letter_selector'].str[0].map(letter_dict)\n",
    "\n",
    "    # build the letter selector id\n",
    "    letter_selector_list = wg_df['letter_selector'].unique()\n",
    "    letter_selector_list.sort()\n",
    "    letter_selector_id_dict = {ls:i_ls for i_ls, ls in enumerate(letter_selector_list)}\n",
    "    \n",
    "    wg_df['letter_selector_id'] = wg_df['letter_selector'].map(letter_selector_id_dict)\n",
    "    \n",
    "    \n",
    "    nc_ls_df = wg_df[['n_chars', 'letter_selector_id', 'letter_selector']].drop_duplicates()\n",
    "    nc_ls_df['nc_ls_id'] = range(0, nc_ls_df.shape[0])\n",
    "    \n",
    "    \n",
    "    wg_df = pd.merge(left = wg_df, right = nc_ls_df)    \n",
    "\n",
    "\n",
    "    n_sub_matrices = nc_ls_df.shape[0]\n",
    "    print('...creating', \"{:,}\".format(n_sub_matrices), 'sub-matrices...')\n",
    "\n",
    "    # word length dictionary\n",
    "    # used in matrix extraction option: 2\n",
    "    n_char_matrix_dict = {}\n",
    "    n_char_lu_dict = dict.fromkeys(wg_df['n_chars'].unique())\n",
    "\n",
    "    # single letter matrix dict\n",
    "    # used in matrix extraction option: 3 and 4\n",
    "    single_letter_matrix_dict = {}\n",
    "    single_letter_lu_dict = dict.fromkeys(wg_df['first_letter_id'].unique())\n",
    "    \n",
    "    # letter selector dictionary\n",
    "    # used in matrix extraction option: 5\n",
    "    letter_selector_matrix_dict = {}\n",
    "    letter_selector_lu_dict = dict.fromkeys(nc_ls_df['letter_selector_id'].unique())\n",
    "    \n",
    "    # word length and lettor selector dictionary\n",
    "    # used in matrix extraction option: 6\n",
    "    nc_ls_matrix_dict = {}\n",
    "    #nc_ls_lu_dict = dict.fromkeys(nc_ls_df['nc_ls_id'].to_numpy())\n",
    "    nc_ls_lu_dict = collections.Counter()\n",
    "\n",
    "    # enumerate these combinations only once\n",
    "    # reduce the number of times we have to compute sets of ids\n",
    "    loop_count = 0\n",
    "\n",
    "    for nc, ls, ls_id, nc_ls_id in zip(nc_ls_df['n_chars'].to_numpy(),\n",
    "                                       nc_ls_df['letter_selector'].to_numpy(),\n",
    "                                       nc_ls_df['letter_selector_id'].to_numpy(),\n",
    "                                       nc_ls_df['nc_ls_id'].to_numpy()):\n",
    "    # def dict_populate(row):\n",
    "    #     nc = row.n_chars\n",
    "    #     ls = row.letter_selector\n",
    "    #     ls_id = row.letter_selector_id\n",
    "    #     nc_ls_id = row.nc_ls_id\n",
    "\n",
    "        ####\n",
    "        # DICTIONARY BY NUMBER OF CHARACTERS\n",
    "        ####\n",
    "        if nc not in n_char_matrix_dict:\n",
    "            nc_wg_id_list = wg_df.loc[\n",
    "                (wg_df[\"n_chars\"] >= nc), \"word_group_id\"\n",
    "            ].to_numpy()\n",
    "            nc_wg_id_set = set(nc_wg_id_list)\n",
    "\n",
    "            # subset the wchar_matrix to get the sub-matrix\n",
    "            nc_sub_wchar_matrix = wchar_matrix[nc_wg_id_list,]\n",
    "\n",
    "            n_char_matrix_dict[nc] = (nc_wg_id_list, nc_sub_wchar_matrix, nc_wg_id_set)\n",
    "\n",
    "            # count the number of look ups\n",
    "            #n_char_lu_dict[nc] = nc_wg_id_list.shape[0]\n",
    "        else:\n",
    "            nc_wg_id_list, nc_sub_wchar_matrix, nc_wg_id_set = n_char_matrix_dict[nc]\n",
    "\n",
    "        ####\n",
    "        # DICTIONARY BY SINGLE-LETTER\n",
    "        ####\n",
    "        ll = ls[0]\n",
    "        ll_id = letter_dict[ll]\n",
    "\n",
    "        # check to see if the sub-matrix with the first letter has already been created\n",
    "        if ll_id not in single_letter_matrix_dict:\n",
    "            # the submatrix has not been created, let's do it.\n",
    "            column_selector = [ll_id]\n",
    "            outcome = wchar_matrix[:, column_selector] > 0\n",
    "            outcome_indices = np.all(outcome > 0, axis=1)\n",
    "\n",
    "            # these indices match with the word_id_list, extract the subset\n",
    "            single_letter_word_group_id_list = word_group_id_list[outcome_indices]\n",
    "            single_letter_word_group_id_set = set(single_letter_word_group_id_list)\n",
    "\n",
    "            # subset the wchar_matrix to get the sub-matrix\n",
    "            single_letter_wchar_matrix = wchar_matrix[single_letter_word_group_id_list,]\n",
    "\n",
    "            single_letter_matrix_dict[ll_id] = (\n",
    "                single_letter_word_group_id_list,\n",
    "                single_letter_wchar_matrix,\n",
    "                single_letter_word_group_id_set,\n",
    "            )\n",
    "\n",
    "            # count the number of look ups\n",
    "            #single_letter_lu_dict[ll_id] = single_letter_word_group_id_list.shape[0]\n",
    "        else:\n",
    "            # query the sub-matrices split by individual letter to then get the smaller partitions\n",
    "            (\n",
    "                single_letter_word_group_id_list,\n",
    "                single_letter_wchar_matrix,\n",
    "                single_letter_word_group_id_set,\n",
    "            ) = single_letter_matrix_dict[ll_id]\n",
    "\n",
    "        ####\n",
    "        # DICTIONARY BY LETTER SELECTOR\n",
    "        ####\n",
    "        if ls_id not in letter_selector_matrix_dict:\n",
    "            # build a column selector\n",
    "            column_selector = [letter_dict[curr_letter] for curr_letter in ls]\n",
    "\n",
    "            # get the indices of the single_letter_wchar_matrix that feature the n least common letters\n",
    "            outcome = single_letter_wchar_matrix[:, column_selector] > 0\n",
    "            outcome_indices = np.all(outcome > 0, axis=1)\n",
    "\n",
    "            # these are now the ids\n",
    "            ls_wg_id_list = single_letter_word_group_id_list[outcome_indices]\n",
    "            ls_wg_id_set = set(ls_wg_id_list)\n",
    "\n",
    "            # subset the wchar_matrix to get the sub-matrix - this contains the N least common letters for a group of words\n",
    "            ls_wchar_matrix = wchar_matrix[ls_wg_id_list,]\n",
    "            letter_selector_matrix_dict[ls_id] = (\n",
    "                ls_wg_id_list,\n",
    "                ls_wchar_matrix,\n",
    "                ls_wg_id_set,\n",
    "            )\n",
    "\n",
    "            # count the number of possible values for a given letter select\n",
    "            #letter_selector_lu_dict[ls_id] = ls_wg_id_list.shape[0]\n",
    "        else:\n",
    "            # this is the submatrix by letter selector\n",
    "            ls_wg_id_list, ls_wchar_matrix, ls_wg_id_set = letter_selector_matrix_dict[\n",
    "                ls_id\n",
    "            ]\n",
    "\n",
    "        # now, compute the intersection of the two\n",
    "        ####\n",
    "        # DICTIONARY BY NUMBER OF CHARACTERS AND LETTER SELECTOR\n",
    "        ####\n",
    "\n",
    "        # perform the intersection\n",
    "        # this is incredibly slow.\n",
    "        nc_ls_wg_id_set = nc_wg_id_set.intersection(ls_wg_id_set)\n",
    "        # nc_ls_wg_id_list = np.array(object = list(nc_ls_wg_id_set), dtype = int)\n",
    "        nc_ls_wg_id_list = np.fromiter(iter=nc_ls_wg_id_set, dtype=int)\n",
    "        # now, get the rows\n",
    "        nc_ls_wchar_matrix = wchar_matrix[nc_ls_wg_id_list,]\n",
    "        nc_ls_matrix_dict[nc_ls_id] = (\n",
    "            nc_ls_wg_id_list,\n",
    "            nc_ls_wchar_matrix,\n",
    "            nc_ls_wg_id_set,\n",
    "        )        \n",
    "\n",
    "        # count the number of lookups\n",
    "        nc_ls_lu_dict[nc_ls_id] = nc_ls_wg_id_list.shape[0]\n",
    "\n",
    "        # let's count things\n",
    "        # full matrix, n_char, single character, single letter selector, full letter selector, n_char & letter selector        \n",
    "        loop_count = len(nc_ls_lu_dict)\n",
    "        if loop_count % 1000 == 0:\n",
    "            print(\"...{:,}\".format(loop_count), 'sub-matrices created...')            \n",
    "        #return None\n",
    "\n",
    "    #output_catch = nc_ls_df.apply(func = dict_populate, axis = 1)\n",
    "    \n",
    "    print(\"...{:,}\".format(len(nc_ls_lu_dict)), 'sub-matrices created...')\n",
    "    e_time = datetime.datetime.now()\n",
    "    p_time = e_time - s_time\n",
    "    p_time = round(p_time.total_seconds(), 2)\n",
    "    print(\"Total extraction time:\", p_time, 'seconds.')\n",
    "\n",
    "    # count the look-ups!\n",
    "    wg_df['me_01_full_matrix_lookup'] = wchar_matrix.shape[0]\n",
    "    wg_df['me_02_n_char_lookup'] = wg_df['n_chars'].map(n_char_lu_dict)\n",
    "    wg_df['me_03_first_letter_lookup'] = wg_df['first_letter_id'].map(single_letter_lu_dict)\n",
    "    wg_df['me_04_single_letter_lookup'] = wg_df['single_letter_id'].map(single_letter_lu_dict)\n",
    "    wg_df['me_05_letter_selector_lookup'] = wg_df['letter_selector_id'].map(letter_selector_lu_dict)\n",
    "    wg_df['me_06_nc_ls_lookup'] = wg_df['nc_ls_id'].map(nc_ls_lu_dict)\n",
    "    \n",
    "    # # build a dataframe of the different counts by n_char and letter selector\n",
    "    # split_count_df = pd.DataFrame(\n",
    "    #     data=split_count_list,\n",
    "    #     columns=[\n",
    "    #         \"nc_ls_tuple\",\n",
    "    #         \"full_matrix_lookup\",\n",
    "    #         \"n_char_lookup\",\n",
    "    #         \"single_letter_lookup\",\n",
    "    #         \"letter_selector_lookup\",\n",
    "    #         \"nc_ls_lookup\",\n",
    "    #     ],\n",
    "    # )\n",
    "\n",
    "    # # let's get objects to track this:\n",
    "    # # only need the:\n",
    "    # # n_char_lookup\n",
    "    # n_char_lu_dict = {}\n",
    "    # # single_letter_lookup\n",
    "    # single_letter_lu_dict = {}\n",
    "    # # letter_selector_lookup\n",
    "    # letter_selector_lu_dict = {}\n",
    "    # # nc_ls_lookup\n",
    "    # nc_ls_lu_dict = {}\n",
    "\n",
    "    # # build a dataframe counting the number of rows by the different sub-matrix splits\n",
    "    \n",
    "    # split_count_df[[\"n_chars\", \"letter_selector\"]] = pd.DataFrame(\n",
    "    #     data=split_count_df[\"nc_ls_tuple\"].tolist(), index=split_count_df.index\n",
    "    # )\n",
    "\n",
    "    # # split out the letter selector\n",
    "    # single_letter_df = split_count_df.loc[\n",
    "    #     split_count_df[\"n_chars\"] == 1, [\"letter_selector\", \"single_letter_lookup\"]\n",
    "    # ]\n",
    "    # single_letter_df.columns = [\"first_letter\", \"first_letter_lookup\"]\n",
    "\n",
    "    # # merge the word group df and the single letter df to get the counts\n",
    "    # # of lookups by technique\n",
    "    # wg_df = pd.merge(left = wg_df, right = single_letter_df)\n",
    "\n",
    "    # # same thing with the split count df\n",
    "    # wg_df = pd.merge(left = wg_df, right = split_count_df)\n",
    "    \n",
    "    return (\n",
    "        wg_df,\n",
    "        n_char_matrix_dict,\n",
    "        single_letter_matrix_dict,\n",
    "        letter_selector_matrix_dict,\n",
    "        nc_ls_matrix_dict       \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df, n_char_matrix_dict, single_letter_matrix_dict, letter_selector_matrix_dict, nc_ls_matrix_dict= split_matrix(\n",
    "    letter_dict = letter_dict,\n",
    "    word_group_id_list = word_group_id_list,\n",
    "        wg_df = wg_df,\n",
    "    wchar_matrix = wchar_matrix, \n",
    "    n_subset_letters = n_subset_letters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blarcho = dict.fromkeys(['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wg_df['first_letter'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blarcho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n_char_matrix_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(single_letter_matrix_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(letter_selector_matrix_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testo = list(nc_ls_matrix_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate total number of from/to word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many anagrams are there?\n",
    "# let's estimate the number of anagrams by assuming that the number of\n",
    "# parent/from words is a function of word length. \n",
    "# estimate_total_pairs() estimates the total number of from/to word pairs\n",
    "# the reason for estimating the upper bound is that it is both just interesting \n",
    "# to know but it also means that we can use the estimated values to allocate an \n",
    "# object in memory as opposed to incrementally appending to a list - this is faster\n",
    "# the object in memory is a NumPy Array that will store integers: from word group id | to word group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_possible_anagrams = estimate_total_pairs(wg_df = wg_df, nc_ls_matrix_dict = nc_ls_matrix_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discover from/to word group id pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we want to santize the inputs? no.\n",
    "# that is beyond the scope of this. \n",
    "# little bobby tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_to_word_group_pairs_simple(\n",
    "    wg_df: pd.DataFrame,    \n",
    "    n_possible_anagrams: int,\n",
    "    matrix_extraction_option: int,\n",
    "    wchar_matrix:np.ndarray,\n",
    "    word_group_id_list:np.ndarray,\n",
    "    n_char_matrix_dict:dict,\n",
    "    single_letter_matrix_dict:dict,\n",
    "    letter_selector_matrix_dict:dict,\n",
    "    nc_ls_matrix_dict:dict,\n",
    "    letter_subset_list:str = None\n",
    "):    \n",
    "    \n",
    "    # use numpy to pre-allocate an array that will be updated while enumerating.\n",
    "    # this eliminates list.append() calls which are fine in small amounts, but\n",
    "    # hundreds of thousands of append calls add a lot of overhead.\n",
    "\n",
    "    output_list = np.full(shape=(n_possible_anagrams, 2), fill_value=-1, dtype=int)    \n",
    "\n",
    "    # this dictionary will store the calculations for each word\n",
    "    proc_time_dict = {}\n",
    "    \n",
    "    if letter_subset_list == 'SAMPLE':\n",
    "        # generate 100 samples within each n_chars and first_letter group combination\n",
    "        curr_wg_df = wg_df.groupby(['n_chars', 'first_letter']).sample(n = 100, replace = True, random_state = 123).drop_duplicates()\n",
    "    elif isinstance(letter_subset_list, str) or isinstance(letter_subset_list, list):        \n",
    "        # subset by a specific set of letters or a single letter\n",
    "        curr_wg_df = wg_df.loc[wg_df['first_letter'].isin(set(letter_subset_list)), :].copy()                  \n",
    "    else:    \n",
    "        curr_wg_df = wg_df.copy()\n",
    "\n",
    "    # display counts\n",
    "    curr_word_count = curr_wg_df.shape[0]\n",
    "\n",
    "    n_curr_words = \"{:,}\".format(curr_word_count)\n",
    "    print(\n",
    "        \"...finding parent anagrams for\",\n",
    "        n_curr_words,\n",
    "        \"words...\"\n",
    "    )\n",
    "\n",
    "    # establish counters for record keeping\n",
    "    row_count = 0\n",
    "    anagram_pair_count = 0\n",
    "    # enumerate by word id, working with integers is faster than words\n",
    "    for row in curr_wg_df.itertuples(index=False):\n",
    "        # start timing to record processing for each word\n",
    "        s_time = datetime.datetime.now()\n",
    "\n",
    "        # word group id\n",
    "        wg_id = row.word_group_id\n",
    "\n",
    "        # get the current word length, from the word id\n",
    "                    \n",
    "\n",
    "        # get the tuple associated with the word id\n",
    "        # much faster to look up stored values for the hash value than it is to\n",
    "        # only look up if the hash value has changed\n",
    "\n",
    "        # get the possible candidate word_group_ids and char matrix\n",
    "        \n",
    "        if matrix_extraction_option == 1:\n",
    "            # option 1: full matrix\n",
    "            outcome_word_id_list = get_values_full_matrix(\n",
    "                wg_id=wg_id,\n",
    "                wchar_matrix=wchar_matrix,\n",
    "                word_group_id_list=word_group_id_list,\n",
    "            )\n",
    "        elif matrix_extraction_option == 2:\n",
    "            # option 2: word length\n",
    "            outcome_word_id_list = get_values_n_char(\n",
    "                wg_id=wg_id,\n",
    "                n_char=row.n_chars,\n",
    "                n_char_matrix_dict=n_char_matrix_dict,\n",
    "            )\n",
    "        elif matrix_extraction_option == 3:\n",
    "            # option 3: first character\n",
    "            outcome_word_id_list = get_values_single_letter(\n",
    "                wg_id=wg_id,\n",
    "                single_letter=row.first_letter,\n",
    "                single_letter_matrix_dict=single_letter_matrix_dict,\n",
    "            )\n",
    "        elif matrix_extraction_option == 4:\n",
    "            # option 4: single least common letter\n",
    "            outcome_word_id_list = get_values_single_letter(\n",
    "                wg_id=wg_id,\n",
    "                single_letter=row.letter_selector[0],\n",
    "                single_letter_matrix_dict=single_letter_matrix_dict,\n",
    "            )\n",
    "        elif matrix_extraction_option == 5:\n",
    "            # option 5: letter selector / focal letter\n",
    "            outcome_word_id_list = get_values_letter_selector(\n",
    "                wg_id=wg_id,\n",
    "                letter_selector=row.letter_selector,\n",
    "                letter_selector_matrix_dict=letter_selector_matrix_dict,\n",
    "            )\n",
    "        else:\n",
    "            # option 6: word length and letter selector\n",
    "            outcome_word_id_list = get_values_n_char_letter_selector(\n",
    "                wg_id=wg_id,\n",
    "                nc_ls_id=row.nc_ls_id,\n",
    "                nc_ls_matrix_dict=nc_ls_matrix_dict,\n",
    "            )\n",
    "\n",
    "        # if the outcome is greater than or equal to zero, then the current word is an\n",
    "        # anagram of the other word\n",
    "        # a value  >= 0 means that the current word contains the exact same number of focal letters\n",
    "        # mite --> time or miter --> time\n",
    "        # a value >= 1 means that current word contains at least the same number of focal letters\n",
    "        # terminator --> time\n",
    "        # a value of <= -1 means that the current word does not have the\n",
    "        # correct number of letters and is therefore not an anagram.\n",
    "        # trait <> time\n",
    "\n",
    "        # number of parent words found\n",
    "        n_from_words = outcome_word_id_list.shape[0]\n",
    "\n",
    "        if n_from_words > 1:\n",
    "            # we have matches\n",
    "            # the focal word\n",
    "\n",
    "            # enumerate the from/parent words\n",
    "            new_anagram_pair_count = anagram_pair_count + n_from_words\n",
    "\n",
    "            output_list[anagram_pair_count:new_anagram_pair_count, :] = outcome_word_id_list\n",
    "\n",
    "            # set the anagram pair count\n",
    "            anagram_pair_count = new_anagram_pair_count\n",
    "\n",
    "        # delete the intermediate list\n",
    "        del outcome_word_id_list\n",
    "\n",
    "        # record the time for the word\n",
    "        e_time = datetime.datetime.now()\n",
    "        p_time = e_time - s_time\n",
    "        p_time = round(p_time.total_seconds(), 8)\n",
    "\n",
    "        proc_time_dict[wg_id] = (p_time, n_from_words)\n",
    "\n",
    "        row_count += 1\n",
    "        if row_count % 1e4 == 0:\n",
    "            print('...found parent anagrams for', \"{:,}\".format(row_count), 'words...')        \n",
    "\n",
    "    # last update\n",
    "    print('...found parent anagrams for', \"{:,}\".format(row_count), 'words...')        \n",
    "    # create a dataframe from the proc_time_dict\n",
    "    proc_time_df = pd.DataFrame.from_dict(data=proc_time_dict, orient=\"index\")\n",
    "    proc_time_df = proc_time_df.reset_index()\n",
    "    proc_time_df.columns = [\"word_group_id\", \"n_seconds\", \"n_from_word_groups\"]\n",
    "\n",
    "    # display processing time for the current letter\n",
    "    total_proc_time_s = round(proc_time_df[\"n_seconds\"].sum(), 4)\n",
    "    total_proc_time_m = round(proc_time_df[\"n_seconds\"].sum() / 60, 4)\n",
    "    print(\n",
    "        \"...finding parent anagrams for\",\n",
    "        n_curr_words,\n",
    "        \"words took\",\n",
    "        total_proc_time_s,\n",
    "        \"seconds |\",\n",
    "        total_proc_time_m,\n",
    "        \"minutes...\"        \n",
    "    )\n",
    "\n",
    "\n",
    "    # truncate the output array to only include rows with a from/to word pair\n",
    "    # this removes any row that has a value of -1\n",
    "    print('...truncating output list...')\n",
    "    output_indices = np.all(output_list >= 0, axis=1)\n",
    "    output_list = output_list[output_indices,]\n",
    "    del output_indices\n",
    "\n",
    "    # how many anagram pairs were found?\n",
    "    n_total_anagrams = output_list.shape[0]\n",
    "    n_total_anagrams_formatted = \"{:,}\".format(n_total_anagrams)\n",
    "    print(\"...total anagrams:\", n_total_anagrams_formatted)    \n",
    "\n",
    "    return proc_time_df, output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_extraction_option = 6\n",
    "letter_subset_list = 'SAMPLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_char_matrix_dict = None\n",
    "single_letter_matrix_dict = None\n",
    "letter_selector_matrix_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_time_df, output_list = \\\n",
    "    generate_from_to_word_group_pairs_simple(wg_df = wg_df,                                      \n",
    "                                      n_possible_anagrams = n_possible_anagrams,\n",
    "                                      matrix_extraction_option = matrix_extraction_option,\n",
    "                                                   wchar_matrix = wchar_matrix,\n",
    "                                                   word_group_id_list = word_group_id_list,\n",
    "                                                   n_char_matrix_dict = None,\n",
    "                                                   single_letter_matrix_dict = None,\n",
    "                                                   letter_selector_matrix_dict = None,\n",
    "                                                   nc_ls_matrix_dict = nc_ls_matrix_dict,\n",
    "                                             letter_subset_list = letter_subset_list,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write anagram pairs to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write the anagram pairs to the database\n",
    "if write_data:\n",
    "    store_anagram_pairs(output_list = output_list, db_path = db_path, db_name = db_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store number of from/to word pairs and time related to processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have three dataframes: wg_df, word_df, and proc_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_time_df, word_df = format_anagaram_processing(output_list = output_list, \n",
    "                                                   proc_time_df = proc_time_df,\n",
    "                                                   word_df = word_df,\n",
    "                                                   wg_df = wg_df,\n",
    "                                                   matrix_extraction_option = matrix_extraction_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_anagram_processing(proc_time_df = proc_time_df, word_df = word_df, matrix_extraction_option = matrix_extraction_option, db_path = db_path, db_name = db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_total_processing_time(proc_time_df = proc_time_df, total_time_start = total_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
