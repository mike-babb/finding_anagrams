{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mike Babb\n",
    "# babbm@uw.edu\n",
    "# Find anagrams\n",
    "## Part 2: Generate and store the anagrams v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard libraries - installed by default\n",
    "import collections\n",
    "import datetime\n",
    "import pickle\n",
    "import sqlite3\n",
    "import string\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# external libraries - not installed by default\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from part_00_file_db_utils import *\n",
    "from part_00_process_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### set input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base file path\n",
    "base_file_path = '/project/finding_anagrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input path\n",
    "in_file_path = 'data'\n",
    "in_file_path = os.path.join(base_file_path, in_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output db path and name\n",
    "db_path = 'db'\n",
    "db_path = os.path.join(base_file_path, db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(db_path):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name = 'words.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### process control flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use numpy to perform matrix opertions and determine from/to and exact anagram relationships\n",
    "# Option 1: Full matrix\n",
    "# Option 2: Word-length\n",
    "# Option 3: First letter\n",
    "# Option 4: Single-least common letter\n",
    "# Option 5: n least common letters\n",
    "# Option 6: word-length and n least common letters\n",
    "\n",
    "matrix_extraction_option = 6\n",
    "\n",
    "# max number of letters to slice to use for the generation of sub-matrices for\n",
    "# options 5 and 6. More letters means more sub-matrices\n",
    "# 3 seems to be the sweet spot\n",
    "n_subset_letters = 3\n",
    "\n",
    "# set write_data to True to store the generated list of anagrams\n",
    "write_data = False\n",
    "\n",
    "# set to None to include all letters\n",
    "# test with a subset of letters by setting the letter_subset_list to ['q', 'x'] or \n",
    "# a different set of letters\n",
    "letter_subset_list = ['x']\n",
    "# letter_subset_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start a timer to record the entire operation\n",
    "total_time_start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df, wg_df, letter_dict, char_matrix, word_group_id_list, word_id_list, wchar_matrix = load_input_data(db_path = db_path, db_name = db_name, in_file_path = in_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the char_matrix into N sub matrices\n",
    "# See split_matrix() for a more elaborate description. \n",
    "# This function does a lot of things. Effectively, it computes and stores values in the wg_df, and splits the matrix into various components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_matrix(\n",
    "    letter_dict: dict,\n",
    "    word_group_id_list: np.ndarray,\n",
    "    wg_df: pd.DataFrame,\n",
    "    wchar_matrix: np.ndarray,\n",
    "    n_subset_letters: int,    \n",
    "    matrix_extraction_option:int\n",
    "):\n",
    "\n",
    "    # the different matrix extraction options\n",
    "    # Option 0: Return all of the different types of matrix extraction options\n",
    "    # Option 1: Full matrix - no objects are returned\n",
    "    # Option 2: Word-length - returns matrices split by the number of characters\n",
    "    # Option 3: First letter - returns matrices split by each letter\n",
    "    # Option 4: Single-least common letter - return matrices split by each letter\n",
    "    # Option 5: n least common letters - return matrices split by least common letters\n",
    "    # Option 6: word-length and n least common letters - return matrices split by least common letters and word length.\n",
    "    \n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    # create the letter selector and determine the max number\n",
    "    # of sub-matrices to makes\n",
    "    wg_df[\"letter_selector\"] = wg_df[\"letter_group_ranked\"].str[:n_subset_letters]\n",
    "    wg_df[\"first_letter_id\"] = wg_df[\"first_letter\"].map(letter_dict)\n",
    "    wg_df[\"single_letter_id\"] = wg_df[\"letter_selector\"].str[0].map(letter_dict)\n",
    "\n",
    "    # build the letter selector id\n",
    "    letter_selector_list = wg_df[\"letter_selector\"].unique()\n",
    "    letter_selector_list.sort()\n",
    "    letter_selector_id_dict = {ls: i_ls for i_ls, ls in enumerate(letter_selector_list)}\n",
    "\n",
    "    wg_df[\"letter_selector_id\"] = wg_df[\"letter_selector\"].map(letter_selector_id_dict)    \n",
    "\n",
    "    nc_ls_df = wg_df[\n",
    "        [\"n_chars\", \"letter_selector_id\", \"letter_selector\"]\n",
    "    ].drop_duplicates()\n",
    "    nc_ls_df[\"nc_ls_id\"] = range(0, nc_ls_df.shape[0])\n",
    "\n",
    "    wg_df = pd.merge(left=wg_df, right=nc_ls_df)    \n",
    "\n",
    "    # only proceed if matrix_extraction_option != 1:\n",
    "    if matrix_extraction_option != 1:        \n",
    "    \n",
    "        # word length dictionary\n",
    "        # used in matrix extraction option: 2\n",
    "        n_char_matrix_dict = {}    \n",
    "    \n",
    "        # single letter matrix dict\n",
    "        # used in matrix extraction option: 3 and 4\n",
    "        single_letter_matrix_dict = {}    \n",
    "    \n",
    "        # letter selector dictionary\n",
    "        # used in matrix extraction option: 5\n",
    "        letter_selector_matrix_dict = {}    \n",
    "    \n",
    "        # word length and lettor selector dictionary\n",
    "        # used in matrix extraction option: 6\n",
    "        nc_ls_matrix_dict = {}\n",
    "    \n",
    "        # create dictionaries to hold sets - these will only exist in the context of this function\n",
    "        n_char_set_dict = {}\n",
    "        single_letter_set_dict = {}\n",
    "        letter_selector_set_dict = {}\n",
    "        nc_ls_set_dict ={}    \n",
    "    \n",
    "        # enumerate these combinations only once\n",
    "        # reduce the number of times we have to compute ids and sub-matrices\n",
    "        \n",
    "\n",
    "        # we have created some ids, but we don't need to enumerate for all of the \n",
    "        # matrix extraction options.\n",
    "        # but because of the way enumeration and creation of dictionaries is \n",
    "        # setup, we're over-enumerating for options 2 through 5. \n",
    "        # this is trade off between minimizing code and data enumeration   \n",
    "        n_records = nc_ls_df.shape[0]            \n",
    "        \n",
    "        print(\"...enumerating\", \"{:,}\".format(n_records), \"records...\")        \n",
    "\n",
    "        loop_count = 0 \n",
    "        for row in nc_ls_df.itertuples(index = False):\n",
    "            nc = row.n_chars\n",
    "            ls = row.letter_selector\n",
    "            ls_id = row.letter_selector_id\n",
    "            \n",
    "            if matrix_extraction_option in (0,6):\n",
    "                nc_ls_id = row.nc_ls_id\n",
    "            \n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION1: NO SUB-MATRICES ARE CREATED.\n",
    "            ####\n",
    "            \n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION 2: DICTIONARY BY NUMBER OF CHARACTERS\n",
    "            ####\n",
    "            if nc not in n_char_matrix_dict:\n",
    "                nc_wg_id_list = wg_df.loc[\n",
    "                    (wg_df[\"n_chars\"] >= nc), \"word_group_id\"\n",
    "                ].to_numpy()\n",
    "                nc_wg_id_set = set(nc_wg_id_list)\n",
    "                n_char_set_dict[nc] = nc_wg_id_set            \n",
    "    \n",
    "                # subset the wchar_matrix to get the sub-matrix\n",
    "                nc_sub_wchar_matrix = wchar_matrix[nc_wg_id_list,]\n",
    "    \n",
    "                n_char_matrix_dict[nc] = (nc_wg_id_list, nc_sub_wchar_matrix)\n",
    "                \n",
    "            else:\n",
    "                nc_wg_id_list, nc_sub_wchar_matrix = n_char_matrix_dict[nc]\n",
    "                nc_wg_id_set = n_char_set_dict[nc]\n",
    "    \n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION 3 AND 4: DICTIONARY BY SINGLE-LETTER\n",
    "            ####\n",
    "            ll = ls[0]\n",
    "            ll_id = letter_dict[ll]\n",
    "    \n",
    "            # check to see if the sub-matrix with the first letter has already been created\n",
    "            if ll_id not in single_letter_matrix_dict:\n",
    "                # the submatrix has not been created, let's do it.\n",
    "                column_selector = [ll_id]\n",
    "                outcome = wchar_matrix[:, column_selector] > 0\n",
    "                outcome_indices = np.all(outcome > 0, axis=1)\n",
    "    \n",
    "                # these indices match with the word_id_list, extract the subset\n",
    "                single_letter_word_group_id_list = word_group_id_list[outcome_indices]\n",
    "    \n",
    "                # the set of ids\n",
    "                single_letter_word_group_id_set = set(single_letter_word_group_id_list)\n",
    "                single_letter_set_dict[ll_id] = single_letter_word_group_id_set\n",
    "        \n",
    "                # subset the wchar_matrix to get the sub-matrix\n",
    "                single_letter_wchar_matrix = wchar_matrix[single_letter_word_group_id_list, ]\n",
    "    \n",
    "                single_letter_matrix_dict[ll_id] = (\n",
    "                    single_letter_word_group_id_list,\n",
    "                    single_letter_wchar_matrix                \n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                # query the sub-matrices split by individual letter to then get the smaller partitions\n",
    "                (\n",
    "                    single_letter_word_group_id_list,\n",
    "                    single_letter_wchar_matrix,                \n",
    "                ) = single_letter_matrix_dict[ll_id]\n",
    "    \n",
    "                single_letter_word_group_id_set = single_letter_set_dict[ll_id]\n",
    "    \n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION 5: DICTIONARY BY LETTER SELECTOR\n",
    "            ####\n",
    "            if ls_id not in letter_selector_matrix_dict:\n",
    "                # build a column selector\n",
    "                column_selector = [letter_dict[curr_letter] for curr_letter in ls]\n",
    "    \n",
    "                # get the indices of the single_letter_wchar_matrix that feature the n least common letters\n",
    "                outcome = single_letter_wchar_matrix[:, column_selector] > 0\n",
    "                outcome_indices = np.all(outcome > 0, axis=1)\n",
    "    \n",
    "                # these are now the ids\n",
    "                ls_wg_id_list = single_letter_word_group_id_list[outcome_indices]\n",
    "    \n",
    "                # the set of ids\n",
    "                ls_wg_id_set = set(ls_wg_id_list)\n",
    "                letter_selector_set_dict[ls_id] = ls_wg_id_set\n",
    "    \n",
    "                # subset the wchar_matrix to get the sub-matrix - this contains the N least common letters for a group of words\n",
    "                ls_wchar_matrix = wchar_matrix[ls_wg_id_list,]\n",
    "                letter_selector_matrix_dict[ls_id] = (\n",
    "                    ls_wg_id_list,\n",
    "                    ls_wchar_matrix                \n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                # this is the submatrix by letter selector\n",
    "                ls_wg_id_list, ls_wchar_matrix = letter_selector_matrix_dict[\n",
    "                    ls_id\n",
    "                ]\n",
    "    \n",
    "                ls_wg_id_set = letter_selector_set_dict[ls_id]\n",
    "    \n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION 6: DICTIONARY BY NUMBER OF CHARACTERS AND LETTER SELECTOR\n",
    "            ####\n",
    "    \n",
    "            ##\n",
    "            # We need to find the intersection of the word_group_id by number of characters\n",
    "            # and the word_group_id by letter selector. The fastest way to do that\n",
    "            # is to use the set().intersection() method. It blows other methods out of the water.\n",
    "            # But...\n",
    "    \n",
    "            # THERE IS A LOT OF OVERHEAD IN THIS PART - THE set() INTERSECTION AND THEN CONVERTING THE\n",
    "            # RESULTING SET TO A NUMPY ARRAY. THIS TAKES ABOUT 33% OF THE TOTAL\n",
    "            # RUNTIME OF THIS FUNCTION\n",
    "            # LEAVING THESE SNIPPETS OF ALTERNATIVES IN FOR REFERENCE AND LEARNING\n",
    "            ##\n",
    "    \n",
    "            # 2024 02 05: USE np.intersect1d(): This is very slow\n",
    "            # nc_ls_wg_id_list = np.intersect1d(ar1 = nc_wg_id_list, ar2=ls_wg_id_list, assume_unique=True)\n",
    "    \n",
    "            # 2024 02 05: use a pandas join: This is very slow\n",
    "            # df_ls = pd.DataFrame(data = ls_wg_id_list, columns = ['word_group_id'])\n",
    "            # df_nc = pd.DataFrame(data = nc_wg_id_list, columns = ['word_group_id'])\n",
    "            # df_out = pd.merge(left = df_ls, right = df_nc)\n",
    "            # nc_ls_wg_id_set = None\n",
    "            # nc_ls_wg_id_list = df_out['word_group_id'].to_numpy()\n",
    "    \n",
    "            # 2024 02 06: user a collections.Counter(). This is also sloooooooow!\n",
    "            # this_counter = collections.Counter(nc_wg_id_list)\n",
    "            # this_counter.update(ls_wg_id_list)\n",
    "            # this_array = np.array(list(this_counter.items()))\n",
    "            # outcome = this_array[:, 1] == 2\n",
    "            # nc_ls_wg_id_list = this_array[outcome, 0]\n",
    "            # nc_ls_wg_id_set = None\n",
    "    \n",
    "            # This is the fastest implementation\n",
    "            if matrix_extraction_option in (0, 6):\n",
    "                nc_ls_wg_id_set = nc_wg_id_set.intersection(ls_wg_id_set)\n",
    "                nc_ls_wg_id_list = np.fromiter(iter=nc_ls_wg_id_set, dtype=int)\n",
    "        \n",
    "                # now, get the rows\n",
    "                nc_ls_wchar_matrix = wchar_matrix[nc_ls_wg_id_list,]\n",
    "                nc_ls_matrix_dict[nc_ls_id] = (\n",
    "                    nc_ls_wg_id_list,\n",
    "                    nc_ls_wchar_matrix                \n",
    "                )\n",
    "                \n",
    "            # get the right loop count                    \n",
    "            loop_count += 1\n",
    "            if loop_count % 1000 == 0:\n",
    "                print(\"...{:,}\".format(loop_count), \"records enumerated...\")\n",
    "        \n",
    "        # display the final count\n",
    "        if matrix_extraction_option == 2:            \n",
    "            n_sub_matrices = len(n_char_matrix_dict)\n",
    "\n",
    "        if matrix_extraction_option in (3,4):\n",
    "            n_sub_matrices = len(single_letter_matrix_dict)\n",
    "\n",
    "        if matrix_extraction_option == 5:\n",
    "            n_sub_matrices = len(letter_selector_matrix_dict)\n",
    "\n",
    "        if matrix_extraction_option in (0,6):\n",
    "            n_sub_matrices = len(nc_ls_matrix_dict)\n",
    "        \n",
    "        print(\"...{:,}\".format(n_sub_matrices), \"sub-matrices created...\")\n",
    "        e_time = datetime.datetime.now()\n",
    "        p_time = e_time - s_time\n",
    "        p_time = round(p_time.total_seconds(), 2)\n",
    "        print(\"Total extraction time:\", p_time, \"seconds.\")\n",
    "\n",
    "    # set things to None so that we can free up memory and reduce overhead\n",
    "    # these objects are no longer needed\n",
    "    # only return objects specific to the particular matrix extraction option\n",
    "    if matrix_extraction_option not in (0, 2):\n",
    "        # option 2\n",
    "        n_char_matrix_dict = None\n",
    "\n",
    "    if matrix_extraction_option not in (0, 3, 4):\n",
    "        # option 3 and 4\n",
    "        single_letter_matrix_dict = None\n",
    "\n",
    "    if matrix_extraction_option not in (0,5):\n",
    "        # option 5\n",
    "        letter_selector_matrix_dict = None\n",
    "\n",
    "    if matrix_extraction_option not in (0,6):\n",
    "        # option 6\n",
    "        nc_ls_matrix_dict = None\n",
    "\n",
    "    return (\n",
    "        wg_df,\n",
    "        n_char_matrix_dict,\n",
    "        single_letter_matrix_dict,\n",
    "        letter_selector_matrix_dict,\n",
    "        nc_ls_matrix_dict,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meo = 0\n",
    "wg_df, n_char_matrix_dict, single_letter_matrix_dict, letter_selector_matrix_dict, nc_ls_matrix_dict= split_matrix(\n",
    "        letter_dict = letter_dict,\n",
    "        word_group_id_list = word_group_id_list,\n",
    "            wg_df = wg_df,\n",
    "        wchar_matrix = wchar_matrix, \n",
    "        n_subset_letters = n_subset_letters,\n",
    "        matrix_extraction_option=meo\n",
    "    )\n",
    "print('Matrix extraction option 2: n_char_matrix_dict:', type(n_char_matrix_dict))\n",
    "print('Matrix extraction options 3 and 4: single_letter_matrix_dict:', type(single_letter_matrix_dict))\n",
    "print('Matrix extraction option 5: letter_selector_matrix_dict:', type(letter_selector_matrix_dict))\n",
    "print('Matrix extraction option 6: nc_ls_matrix_dict:', type(nc_ls_matrix_dict))\n",
    "\n",
    "print(wg_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lookups(wg_df:pd.DataFrame,\n",
    "                    n_char_matrix_dict:dict,\n",
    "                    single_letter_matrix_dict:dict,\n",
    "                    letter_selector_matrix_dict:dict,\n",
    "                    nc_ls_matrix_dict:dict):\n",
    "    \n",
    "    # time to count some stuff!\n",
    "\n",
    "    # count the number of look ups for a letter\n",
    "    n_char_lu_dict = {nc:nc_items[0].shape[0] for nc, nc_items in n_char_matrix_dict.items()}    \n",
    "\n",
    "    # count the number of look ups\n",
    "    single_letter_lu_dict = {sl:sl_items[0].shape[0] for sl, sl_items in single_letter_matrix_dict.items()}\n",
    "\n",
    "    # count the number of possible values for a given letter select\n",
    "    letter_selector_lu_dict = {ls:ls_items[0].shape[0] for ls, ls_items in letter_selector_matrix_dict.items()}\n",
    "\n",
    "    # count the number of lookups\n",
    "    nc_ls_lu_dict = {nc_ls:nc_ls_items[0].shape[0] for nc_ls, nc_ls_items in nc_ls_matrix_dict.items()}          \n",
    "\n",
    "    col_names = ['word_id','word_group_id', 'n_chars','first_letter_id','single_letter_id','letter_selector_id','nc_ls_id']\n",
    "    \n",
    "    wg_lu_df = wg_df[col_names].copy()\n",
    "    \n",
    "    # count the look-ups!\n",
    "    wg_lu_df[\"me_01_full_matrix_lookup\"] = wchar_matrix.shape[0]\n",
    "    wg_lu_df[\"me_02_n_char_lookup\"] = wg_lu_df[\"n_chars\"].map(n_char_lu_dict)\n",
    "    wg_lu_df[\"me_03_first_letter_lookup\"] = wg_lu_df[\"first_letter_id\"].map(\n",
    "    single_letter_lu_dict\n",
    "    )\n",
    "    wg_lu_df[\"me_04_single_letter_lookup\"] = wg_lu_df[\"single_letter_id\"].map(\n",
    "    single_letter_lu_dict\n",
    "    )\n",
    "    wg_lu_df[\"me_05_letter_selector_lookup\"] = wg_lu_df[\"letter_selector_id\"].map(\n",
    "    letter_selector_lu_dict\n",
    "    )\n",
    "    wg_lu_df[\"me_06_nc_ls_lookup\"] = wg_lu_df[\"nc_ls_id\"].map(nc_ls_lu_dict)\n",
    "\n",
    "    return wg_lu_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meo in range(0, 7):\n",
    "    print('*** matrix extraction option:', meo, '***')\n",
    "\n",
    "    wg_df, n_char_matrix_dict, single_letter_matrix_dict, letter_selector_matrix_dict, nc_ls_matrix_dict= split_matrix(\n",
    "        letter_dict = letter_dict,\n",
    "        word_group_id_list = word_group_id_list,\n",
    "            wg_df = wg_df,\n",
    "        wchar_matrix = wchar_matrix, \n",
    "        n_subset_letters = n_subset_letters,\n",
    "        matrix_extraction_option=meo\n",
    "    )\n",
    "\n",
    "    print('n_char_matrix_dict:', type(n_char_matrix_dict))\n",
    "    print('single_letter_matrix_dict:', type(single_letter_matrix_dict))\n",
    "    print('letter_selector_matrix_dict:', type(letter_selector_matrix_dict))\n",
    "    print('nc_ls_matrix_dict:', type(nc_ls_matrix_dict))\n",
    "\n",
    "    print(wg_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lu_df = compute_lookups(wg_df=wg_df,\n",
    "                    n_char_matrix_dict = n_char_matrix_dict,\n",
    "                    single_letter_matrix_dict = single_letter_matrix_dict,\n",
    "                    letter_selector_matrix_dict = letter_selector_matrix_dict,\n",
    "                    nc_ls_matrix_dict = nc_ls_matrix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lu_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blarcho = dict.fromkeys(['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wg_df['first_letter'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blarcho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n_char_matrix_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(single_letter_matrix_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(letter_selector_matrix_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testo = list(nc_ls_matrix_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### estimate total number of from/to word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many anagrams are there?\n",
    "# let's estimate the number of anagrams by assuming that the number of\n",
    "# parent/from words is a function of word length. \n",
    "# estimate_total_pairs() estimates the total number of from/to word pairs\n",
    "# the reason for estimating the upper bound is that it is both just interesting \n",
    "# to know but it also means that we can use the estimated values to allocate an \n",
    "# object in memory as opposed to incrementally appending to a list - this is faster\n",
    "# the object in memory is a NumPy Array that will store integers: from word group id | to word group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_possible_anagrams = estimate_total_pairs(wg_df = wg_df, nc_ls_matrix_dict = nc_ls_matrix_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### discover from/to word group id pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we want to santize the inputs? no.\n",
    "# that is beyond the scope of this. \n",
    "# little bobby tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_to_word_group_pairs_simple(\n",
    "    wg_df: pd.DataFrame,    \n",
    "    n_possible_anagrams: int,\n",
    "    matrix_extraction_option: int,\n",
    "    wchar_matrix:np.ndarray,\n",
    "    word_group_id_list:np.ndarray,\n",
    "    n_char_matrix_dict:dict,\n",
    "    single_letter_matrix_dict:dict,\n",
    "    letter_selector_matrix_dict:dict,\n",
    "    nc_ls_matrix_dict:dict,\n",
    "    letter_subset_list:str = None\n",
    "):    \n",
    "    \n",
    "    # use numpy to pre-allocate an array that will be updated while enumerating.\n",
    "    # this eliminates list.append() calls which are fine in small amounts, but\n",
    "    # hundreds of thousands of append calls add a lot of overhead.\n",
    "\n",
    "    output_list = np.full(shape=(n_possible_anagrams, 2), fill_value=-1, dtype=int)    \n",
    "\n",
    "    # this dictionary will store the calculations for each word\n",
    "    proc_time_dict = {}\n",
    "    \n",
    "    if letter_subset_list == 'SAMPLE':\n",
    "        # generate 100 samples within each n_chars and first_letter group combination\n",
    "        curr_wg_df = wg_df.groupby(['n_chars', 'first_letter']).sample(n = 100, replace = True, random_state = 123).drop_duplicates()\n",
    "    elif isinstance(letter_subset_list, str) or isinstance(letter_subset_list, list):        \n",
    "        # subset by a specific set of letters or a single letter\n",
    "        curr_wg_df = wg_df.loc[wg_df['first_letter'].isin(set(letter_subset_list)), :].copy()                  \n",
    "    else:    \n",
    "        curr_wg_df = wg_df.copy()\n",
    "\n",
    "    # display counts\n",
    "    curr_word_count = curr_wg_df.shape[0]\n",
    "\n",
    "    n_curr_words = \"{:,}\".format(curr_word_count)\n",
    "    print(\n",
    "        \"...finding parent anagrams for\",\n",
    "        n_curr_words,\n",
    "        \"words...\"\n",
    "    )\n",
    "\n",
    "    # establish counters for record keeping\n",
    "    row_count = 0\n",
    "    anagram_pair_count = 0\n",
    "    # enumerate by word id, working with integers is faster than words\n",
    "    for row in curr_wg_df.itertuples(index=False):\n",
    "        # start timing to record processing for each word\n",
    "        s_time = datetime.datetime.now()\n",
    "\n",
    "        # word group id\n",
    "        wg_id = row.word_group_id\n",
    "\n",
    "        # get the current word length, from the word id\n",
    "                    \n",
    "\n",
    "        # get the tuple associated with the word id\n",
    "        # much faster to look up stored values for the hash value than it is to\n",
    "        # only look up if the hash value has changed\n",
    "\n",
    "        # get the possible candidate word_group_ids and char matrix\n",
    "        \n",
    "        if matrix_extraction_option == 1:\n",
    "            # option 1: full matrix\n",
    "            outcome_word_id_list = get_values_full_matrix(\n",
    "                wg_id=wg_id,\n",
    "                wchar_matrix=wchar_matrix,\n",
    "                word_group_id_list=word_group_id_list,\n",
    "            )\n",
    "        elif matrix_extraction_option == 2:\n",
    "            # option 2: word length\n",
    "            outcome_word_id_list = get_values_n_char(\n",
    "                wg_id=wg_id,\n",
    "                n_char=row.n_chars,\n",
    "                n_char_matrix_dict=n_char_matrix_dict,\n",
    "            )\n",
    "        elif matrix_extraction_option == 3:\n",
    "            # option 3: first character\n",
    "            outcome_word_id_list = get_values_single_letter(\n",
    "                wg_id=wg_id,\n",
    "                single_letter=row.first_letter,\n",
    "                single_letter_matrix_dict=single_letter_matrix_dict,\n",
    "            )\n",
    "        elif matrix_extraction_option == 4:\n",
    "            # option 4: single least common letter\n",
    "            outcome_word_id_list = get_values_single_letter(\n",
    "                wg_id=wg_id,\n",
    "                single_letter=row.letter_selector[0],\n",
    "                single_letter_matrix_dict=single_letter_matrix_dict,\n",
    "            )\n",
    "        elif matrix_extraction_option == 5:\n",
    "            # option 5: letter selector / focal letter\n",
    "            outcome_word_id_list = get_values_letter_selector(\n",
    "                wg_id=wg_id,\n",
    "                letter_selector=row.letter_selector,\n",
    "                letter_selector_matrix_dict=letter_selector_matrix_dict,\n",
    "            )\n",
    "        else:\n",
    "            # option 6: word length and letter selector\n",
    "            outcome_word_id_list = get_values_n_char_letter_selector(\n",
    "                wg_id=wg_id,\n",
    "                nc_ls_id=row.nc_ls_id,\n",
    "                nc_ls_matrix_dict=nc_ls_matrix_dict,\n",
    "            )\n",
    "\n",
    "        # if the outcome is greater than or equal to zero, then the current word is an\n",
    "        # anagram of the other word\n",
    "        # a value  >= 0 means that the current word contains the exact same number of focal letters\n",
    "        # mite --> time or miter --> time\n",
    "        # a value >= 1 means that current word contains at least the same number of focal letters\n",
    "        # terminator --> time\n",
    "        # a value of <= -1 means that the current word does not have the\n",
    "        # correct number of letters and is therefore not an anagram.\n",
    "        # trait <> time\n",
    "\n",
    "        # number of parent words found\n",
    "        n_from_words = outcome_word_id_list.shape[0]\n",
    "\n",
    "        if n_from_words > 1:\n",
    "            # we have matches\n",
    "            # the focal word\n",
    "\n",
    "            # enumerate the from/parent words\n",
    "            new_anagram_pair_count = anagram_pair_count + n_from_words\n",
    "\n",
    "            output_list[anagram_pair_count:new_anagram_pair_count, :] = outcome_word_id_list\n",
    "\n",
    "            # set the anagram pair count\n",
    "            anagram_pair_count = new_anagram_pair_count\n",
    "\n",
    "        # delete the intermediate list\n",
    "        del outcome_word_id_list\n",
    "\n",
    "        # record the time for the word\n",
    "        e_time = datetime.datetime.now()\n",
    "        p_time = e_time - s_time\n",
    "        p_time = round(p_time.total_seconds(), 8)\n",
    "\n",
    "        proc_time_dict[wg_id] = (p_time, n_from_words)\n",
    "\n",
    "        row_count += 1\n",
    "        if row_count % 1e4 == 0:\n",
    "            print('...found parent anagrams for', \"{:,}\".format(row_count), 'words...')        \n",
    "\n",
    "    # last update\n",
    "    print('...found parent anagrams for', \"{:,}\".format(row_count), 'words...')        \n",
    "    # create a dataframe from the proc_time_dict\n",
    "    proc_time_df = pd.DataFrame.from_dict(data=proc_time_dict, orient=\"index\")\n",
    "    proc_time_df = proc_time_df.reset_index()\n",
    "    proc_time_df.columns = [\"word_group_id\", \"n_seconds\", \"n_from_word_groups\"]\n",
    "\n",
    "    # display processing time for the current letter\n",
    "    total_proc_time_s = round(proc_time_df[\"n_seconds\"].sum(), 4)\n",
    "    total_proc_time_m = round(proc_time_df[\"n_seconds\"].sum() / 60, 4)\n",
    "    print(\n",
    "        \"...finding parent anagrams for\",\n",
    "        n_curr_words,\n",
    "        \"words took\",\n",
    "        total_proc_time_s,\n",
    "        \"seconds |\",\n",
    "        total_proc_time_m,\n",
    "        \"minutes...\"        \n",
    "    )\n",
    "\n",
    "\n",
    "    # truncate the output array to only include rows with a from/to word pair\n",
    "    # this removes any row that has a value of -1\n",
    "    print('...truncating output list...')\n",
    "    output_indices = np.all(output_list >= 0, axis=1)\n",
    "    output_list = output_list[output_indices,]\n",
    "    del output_indices\n",
    "\n",
    "    # how many anagram pairs were found?\n",
    "    n_total_anagrams = output_list.shape[0]\n",
    "    n_total_anagrams_formatted = \"{:,}\".format(n_total_anagrams)\n",
    "    print(\"...total anagrams:\", n_total_anagrams_formatted)    \n",
    "\n",
    "    return proc_time_df, output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_extraction_option = 6\n",
    "letter_subset_list = 'SAMPLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_char_matrix_dict = None\n",
    "single_letter_matrix_dict = None\n",
    "letter_selector_matrix_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_time_df, output_list = \\\n",
    "    generate_from_to_word_group_pairs_simple(wg_df = wg_df,                                      \n",
    "                                      n_possible_anagrams = n_possible_anagrams,\n",
    "                                      matrix_extraction_option = matrix_extraction_option,\n",
    "                                                   wchar_matrix = wchar_matrix,\n",
    "                                                   word_group_id_list = word_group_id_list,\n",
    "                                                   n_char_matrix_dict = None,\n",
    "                                                   single_letter_matrix_dict = None,\n",
    "                                                   letter_selector_matrix_dict = None,\n",
    "                                                   nc_ls_matrix_dict = nc_ls_matrix_dict,\n",
    "                                             letter_subset_list = letter_subset_list,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### write anagram pairs to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write the anagram pairs to the database\n",
    "if write_data:\n",
    "    store_anagram_pairs(output_list = output_list, db_path = db_path, db_name = db_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### store number of from/to word pairs and time related to processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have three dataframes: wg_df, word_df, and proc_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_time_df, word_df = format_anagaram_processing(output_list = output_list, \n",
    "                                                   proc_time_df = proc_time_df,\n",
    "                                                   word_df = word_df,\n",
    "                                                   wg_df = wg_df,\n",
    "                                                   matrix_extraction_option = matrix_extraction_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_anagram_processing(proc_time_df = proc_time_df, word_df = word_df, matrix_extraction_option = matrix_extraction_option, db_path = db_path, db_name = db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_total_processing_time(proc_time_df = proc_time_df, total_time_start = total_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
