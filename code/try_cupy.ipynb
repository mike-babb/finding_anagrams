{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa58d5-75a5-41ea-8943-ebae8f18d273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "from time import perf_counter_ns\n",
    "import time\n",
    "\n",
    "# external libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "# custom libraries\n",
    "from _run_constants import *\n",
    "from part_00_file_db_utils import *\n",
    "from part_00_process_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fca8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupyx.profiler import benchmark\n",
    "\n",
    "def my_func(a):\n",
    "    return cp.sqrt(cp.sum(a**2, axis=-1))\n",
    "\n",
    "a = cp.random.random((256, 1024))\n",
    "print(benchmark(my_func, (a,), n_repeat=20))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_gpu = cp.cuda.Event()\n",
    "end_gpu = cp.cuda.Event()\n",
    "\n",
    "start_gpu.record()\n",
    "start_cpu = time.perf_counter()\n",
    "out = my_func(a)\n",
    "end_cpu = time.perf_counter()\n",
    "end_gpu.record()\n",
    "end_gpu.synchronize()\n",
    "t_gpu = cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "t_cpu = end_cpu - start_cpu\n",
    "print(t_gpu, t_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247235e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df, wg_df, letter_dict, char_matrix, \\\n",
    "    word_group_id_list, word_id_list, wchar_matrix = load_input_data(\n",
    "        db_path=rc.DB_PATH, db_name=rc.DB_NAME,\n",
    "        in_file_path=rc.IN_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's process 1000 rows using a single lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b103cb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b36966",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_possible_anagrams = int(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25610500",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{n_possible_anagrams :,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f806b2",
   "metadata": {},
   "source": [
    "# Using the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb82534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1000 rows\n",
    "n_samples = 1000\n",
    "sample_wg_id = wg_df['word_group_id'].sample(n = n_samples).to_numpy()\n",
    "\n",
    "# establish counters for record keeping\n",
    "output_list = np.full(shape=(n_possible_anagrams, 2),\n",
    "                          fill_value=-1, dtype=int)\n",
    "\n",
    "row_count = 0\n",
    "anagram_pair_count = 0\n",
    "intermediate_to_word_count = collections.Counter()\n",
    "\n",
    "for wg_id in sample_wg_id:\n",
    "    # identify parent words\n",
    "    outcome = wchar_matrix - wchar_matrix[wg_id, ]\n",
    "    \n",
    "    # compute the score by finding where rows, across all columns, are GTE 0\n",
    "    outcome_indices = np.all(outcome >= 0, axis=1)\n",
    "    outcome = None\n",
    "\n",
    "    n_from_words = outcome_indices.sum()\n",
    "\n",
    "    if n_from_words >= 1:\n",
    "        # extract anagrams based on index values    \n",
    "        outcome_word_id_list = word_group_id_list[outcome_indices]    \n",
    "\n",
    "        # we have matches\n",
    "        # the focal word\n",
    "        curr_output_list = cp.zeros(shape=(n_from_words, 2), dtype=int)\n",
    "\n",
    "        # update the output list with the word_id_list - these are from/parent words\n",
    "        curr_output_list[:, 0] = outcome_word_id_list\n",
    "\n",
    "        # update with the word_id - this is the to/child word\n",
    "        curr_output_list[:, 1] = wg_id\n",
    "\n",
    "        # enumerate the from/parent wordsds\n",
    "        new_anagram_pair_count = anagram_pair_count + n_from_words\n",
    "\n",
    "        output_list[anagram_pair_count:new_anagram_pair_count,\n",
    "                    :] = curr_output_list\n",
    "\n",
    "        # n_to_word_counter = collections.Counter(output_list[:, 0])\n",
    "        intermediate_to_word_count.update(outcome_word_id_list.tolist())\n",
    "\n",
    "        # set the anagram pair count\n",
    "        anagram_pair_count = new_anagram_pair_count\n",
    "    \n",
    "    row_count += 1\n",
    "\n",
    "    if row_count % 100 == 0:\n",
    "        print(row_count)\n",
    "print('truncating list')\n",
    "output_indices = np.all(output_list >= 0, axis=1)\n",
    "output_list = output_list[output_indices,]\n",
    "del output_indices\n",
    "output_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df50a3",
   "metadata": {},
   "source": [
    "# using the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561dc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to cupy objects\n",
    "wchar_matrix_cp = cp.asarray(a = wchar_matrix)\n",
    "word_group_id_list_cp = cp.asarray(a = word_group_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780194d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_possible_anagrams = load_possible_anagrams(db_path=rc.DB_PATH, db_name=rc.DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_possible_anagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823231d3-51f8-4a6e-a1bd-e4db346a680d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# establish counters for record keeping\n",
    "# sample 1000 rows\n",
    "n_samples = 5000\n",
    "sample_wg_id = wg_df['word_group_id'].sample(n = n_samples).to_numpy()\n",
    "# sample_wg_id = wg_df['word_group_id'].to_numpy()\n",
    "sample_wg_id_cp = cp.asarray(a = sample_wg_id)\n",
    "\n",
    "output_list_cp = cp.full(shape=(n_possible_anagrams, 2),\n",
    "                          fill_value=-1, dtype=int)\n",
    "\n",
    "row_count = 0\n",
    "anagram_pair_count = 0\n",
    "intermediate_to_word_count = collections.Counter()\n",
    "\n",
    "for wg_id in sample_wg_id_cp:\n",
    "    outcome = wchar_matrix_cp - wchar_matrix_cp[wg_id, ]\n",
    "    \n",
    "    # compute the score by finding where rows, across all columns, are GTE 0\n",
    "    outcome_indices_cp = cp.all(outcome >= 0, axis=1)\n",
    "    outcome = None\n",
    "\n",
    "    n_from_words = outcome_indices_cp.sum()   \n",
    "            \n",
    "    \n",
    "    if n_from_words >= 1:\n",
    "        # extract anagrams based on index values    \n",
    "        outcome_word_id_list_cp = word_group_id_list_cp[outcome_indices_cp]    \n",
    "\n",
    "        # we have matches\n",
    "        curr_output_list_cp = cp.zeros(shape=(outcome_word_id_list_cp.shape[0], 2), dtype=int)\n",
    "\n",
    "        # update the output list with the word_id_list - these are from/parent words\n",
    "        curr_output_list_cp[:, 0] = outcome_word_id_list_cp\n",
    "\n",
    "        # update with the word_id - this is the to/child word\n",
    "        curr_output_list_cp[:, 1] = wg_id\n",
    "\n",
    "        # enumerate the from/parent wordsds\n",
    "        new_anagram_pair_count = anagram_pair_count + n_from_words\n",
    "\n",
    "        # update the total output list\n",
    "        output_list_cp[anagram_pair_count:new_anagram_pair_count,\n",
    "                    :] = curr_output_list_cp\n",
    "\n",
    "        # n_to_word_counter = collections.Counter(output_list[:, 0])\n",
    "        intermediate_to_word_count.update(outcome_word_id_list_cp.tolist())\n",
    "\n",
    "        # set the anagram pair count\n",
    "        anagram_pair_count = new_anagram_pair_count\n",
    "    \n",
    "    row_count += 1\n",
    "\n",
    "    if row_count % 10000 == 0:\n",
    "        print(row_count)\n",
    "\n",
    "print('truncating list')\n",
    "output_indices_cp = cp.all(output_list_cp >= 0, axis=1)\n",
    "output_list_cp = output_list_cp[output_indices_cp,]\n",
    "del output_indices_cp\n",
    "print(output_list_cp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's build "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311441cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_matrix_cp(\n",
    "    letter_dict: dict,\n",
    "    word_group_id_list: cp.ndarray,\n",
    "    wg_df: pd.DataFrame,\n",
    "    wchar_matrix: cp.ndarray,\n",
    "    n_subset_letters: int,\n",
    "    matrix_extraction_option: int = 0\n",
    "):\n",
    "\n",
    "    # the different matrix extraction options\n",
    "    # Option 0: Return all of the different types of matrix extraction options\n",
    "    # Option 1: Full matrix - no objects are returned\n",
    "    # Option 2: Word-length - returns matrices split by the number of characters\n",
    "    # Option 3: First letter - returns matrices split by each letter\n",
    "    # Option 4: Single-least common letter - return matrices split by each letter\n",
    "    # Option 5: n least common letters - return matrices split by least common letters\n",
    "    # Option 6: word-length and n least common letters - return matrices split by least common letters and word length.\n",
    "\n",
    "    s_time = perf_counter_ns()\n",
    "    \n",
    "\n",
    "    # create the letter selector and determine the max number\n",
    "    # of sub-matrices to makes\n",
    "    wg_df[\"letter_selector\"] = wg_df[\"letter_group_ranked\"].str[:n_subset_letters]\n",
    "    wg_df[\"first_letter_id\"] = wg_df[\"first_letter\"].map(letter_dict)\n",
    "    wg_df[\"single_letter_id\"] = wg_df[\"letter_selector\"].str[0].map(\n",
    "        letter_dict)\n",
    "\n",
    "    # build the letter selector id list and dict\n",
    "    letter_selector_list = wg_df[\"letter_selector\"].unique()\n",
    "    letter_selector_list.sort()\n",
    "    letter_selector_id_dict = {ls: i_ls for i_ls,\n",
    "                               ls in enumerate(letter_selector_list)}\n",
    "\n",
    "    wg_df[\"letter_selector_id\"] = wg_df[\"letter_selector\"].map(\n",
    "        letter_selector_id_dict)\n",
    "\n",
    "    nc_ls_df = wg_df[\n",
    "        [\"n_chars\", \"letter_selector_id\", \"letter_selector\"]\n",
    "    ].drop_duplicates()\n",
    "    nc_ls_df[\"nc_ls_id\"] = range(0, nc_ls_df.shape[0])\n",
    "\n",
    "    wg_df = pd.merge(left=wg_df, right=nc_ls_df)\n",
    "\n",
    "    # only proceed if matrix_extraction_option != 1:\n",
    "    if matrix_extraction_option != 1:\n",
    "\n",
    "        # word length dictionary\n",
    "        # used in matrix extraction option: 2\n",
    "        n_char_matrix_dict = {}\n",
    "\n",
    "        # single letter matrix dict\n",
    "        # used in matrix extraction option: 3 and 4\n",
    "        single_letter_matrix_dict = {}\n",
    "\n",
    "        # letter selector dictionary\n",
    "        # used in matrix extraction option: 5\n",
    "        letter_selector_matrix_dict = {}\n",
    "\n",
    "        # word length and lettor selector dictionary\n",
    "        # used in matrix extraction option: 6\n",
    "        nc_ls_matrix_dict = {}\n",
    "\n",
    "        # create dictionaries to hold sets - these will only exist in the context of this function\n",
    "        n_char_set_dict = {}\n",
    "        single_letter_set_dict = {}\n",
    "        letter_selector_set_dict = {}\n",
    "\n",
    "        # enumerate these combinations only once\n",
    "        # reduce the number of times we have to compute ids and sub-matrices\n",
    "\n",
    "        # we have created some ids, but we don't need to enumerate for all of the\n",
    "        # matrix extraction options.\n",
    "        # but because of the way enumeration and creation of dictionaries is\n",
    "        # setup, we're over-enumerating for options 2 through 5.\n",
    "        # this is trade off between minimizing code, code reuse, and data enumeration\n",
    "        n_records = nc_ls_df.shape[0]\n",
    "\n",
    "        print(\"...enumerating\", \"{:,}\".format(n_records), \"records...\")\n",
    "\n",
    "        loop_count = 0\n",
    "        for row in nc_ls_df.itertuples(index=False):\n",
    "            nc = row.n_chars\n",
    "            ls = row.letter_selector\n",
    "            ls_id = row.letter_selector_id\n",
    "\n",
    "            if matrix_extraction_option in (0, 6):\n",
    "                nc_ls_id = row.nc_ls_id\n",
    "\n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION 1: NO SUB-MATRICES ARE CREATED.\n",
    "            ####\n",
    "            # (Block left here for convenience)\n",
    "\n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION 2: DICTIONARY BY NUMBER OF CHARACTERS\n",
    "            ####\n",
    "            if nc not in n_char_matrix_dict:\n",
    "                nc_wg_id_list = wg_df.loc[\n",
    "                    (wg_df[\"n_chars\"] >= nc), \"word_group_id\"\n",
    "                ].to_numpy()\n",
    "                nc_wg_id_set = set(nc_wg_id_list)\n",
    "                n_char_set_dict[nc] = nc_wg_id_set\n",
    "\n",
    "                # subset the wchar_matrix to get the sub-matrix\n",
    "                nc_sub_wchar_matrix = wchar_matrix[nc_wg_id_list,]\n",
    "\n",
    "                n_char_matrix_dict[nc] = (nc_wg_id_list, nc_sub_wchar_matrix)\n",
    "\n",
    "            else:\n",
    "                nc_wg_id_list, nc_sub_wchar_matrix = n_char_matrix_dict[nc]\n",
    "                nc_wg_id_set = n_char_set_dict[nc]\n",
    "\n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION 3 AND 4: DICTIONARY BY SINGLE-LETTER\n",
    "            ####\n",
    "            ll = ls[0]\n",
    "            ll_id = letter_dict[ll]\n",
    "\n",
    "            # check to see if the sub-matrix with the first letter has already been created\n",
    "            if ll_id not in single_letter_matrix_dict:\n",
    "                # the submatrix has not been created, let's do it.\n",
    "                column_selector = [ll_id]\n",
    "                outcome = wchar_matrix[:, column_selector] > 0\n",
    "                \n",
    "                outcome_indices = cp.all(outcome > 0, axis=1)\n",
    "                \n",
    "                # these indices match with the word_id_list, extract the subset\n",
    "                single_letter_word_group_id_list = word_group_id_list[outcome_indices]\n",
    "\n",
    "                # the set of ids\n",
    "                single_letter_word_group_id_set = set(\n",
    "                    single_letter_word_group_id_list.tolist())\n",
    "                \n",
    "                single_letter_set_dict[ll_id] = single_letter_word_group_id_set\n",
    "\n",
    "                # subset the wchar_matrix to get the sub-matrix\n",
    "                single_letter_wchar_matrix = wchar_matrix[single_letter_word_group_id_list, ]\n",
    "\n",
    "                single_letter_matrix_dict[ll_id] = (\n",
    "                    single_letter_word_group_id_list,\n",
    "                    single_letter_wchar_matrix\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # query the sub-matrices split by individual letter to then get the smaller matrices\n",
    "                (\n",
    "                    single_letter_word_group_id_list,\n",
    "                    single_letter_wchar_matrix,\n",
    "                ) = single_letter_matrix_dict[ll_id]\n",
    "\n",
    "                single_letter_word_group_id_set = single_letter_set_dict[ll_id]\n",
    "\n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION 5: DICTIONARY BY LETTER SELECTOR\n",
    "            ####\n",
    "            if ls_id not in letter_selector_matrix_dict:\n",
    "                # build a column selector\n",
    "                column_selector = [letter_dict[curr_letter]\n",
    "                                   for curr_letter in ls]\n",
    "\n",
    "                # get the indices of the single_letter_wchar_matrix that feature the n least common letters\n",
    "                outcome = single_letter_wchar_matrix[:, column_selector] > 0\n",
    "                outcome_indices = cp.all(outcome > 0, axis=1)\n",
    "                \n",
    "                # these are now the ids\n",
    "                ls_wg_id_list = single_letter_word_group_id_list[outcome_indices]\n",
    "\n",
    "                # the set of ids\n",
    "                ls_wg_id_set = set(ls_wg_id_list.tolist())\n",
    "                letter_selector_set_dict[ls_id] = ls_wg_id_set\n",
    "\n",
    "                # subset the wchar_matrix to get the sub-matrix - this contains the N least common letters for a group of words\n",
    "                ls_wchar_matrix = wchar_matrix[ls_wg_id_list,]\n",
    "                letter_selector_matrix_dict[ls_id] = (\n",
    "                    ls_wg_id_list,\n",
    "                    ls_wchar_matrix\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # this is the submatrix by letter selector\n",
    "                ls_wg_id_list, ls_wchar_matrix = letter_selector_matrix_dict[\n",
    "                    ls_id\n",
    "                ]\n",
    "\n",
    "                ls_wg_id_set = letter_selector_set_dict[ls_id]\n",
    "\n",
    "            ####\n",
    "            # MATRIX EXTRACTION OPTION 6: DICTIONARY BY NUMBER OF CHARACTERS AND LETTER SELECTOR\n",
    "            ####\n",
    "\n",
    "            ##\n",
    "            # We need to find the intersection of the word_group_id by number of characters\n",
    "            # and the word_group_id by letter selector. The fastest way to do that\n",
    "            # is to use the set().intersection() method. It blows other methods out of the water.\n",
    "            # But...\n",
    "\n",
    "            # THERE IS A LOT OF OVERHEAD IN THIS PART - THE set() INTERSECTION\n",
    "            # AND THEN CONVERTING THE RESULTING SET TO A NUMPY ARRAY. THIS TAKES\n",
    "            # ABOUT 33% OF THE TOTAL RUNTIME OF THIS FUNCTION\n",
    "            # LEAVING THESE SNIPPETS OF ALTERNATIVES IN FOR REFERENCE AND LEARNING\n",
    "            ##\n",
    "\n",
    "            # 2024 02 05: USE np.intersect1d(): This is very slow\n",
    "            # nc_ls_wg_id_list = np.intersect1d(ar1 = nc_wg_id_list, ar2=ls_wg_id_list, assume_unique=True)\n",
    "\n",
    "            # 2024 02 05: use a pandas join: This is very slow\n",
    "            # df_ls = pd.DataFrame(data = ls_wg_id_list, columns = ['word_group_id'])\n",
    "            # df_nc = pd.DataFrame(data = nc_wg_id_list, columns = ['word_group_id'])\n",
    "            # df_out = pd.merge(left = df_ls, right = df_nc)\n",
    "            # nc_ls_wg_id_set = None\n",
    "            # nc_ls_wg_id_list = df_out['word_group_id'].to_numpy()\n",
    "\n",
    "            # 2024 02 06: use a collections.Counter(). This is also sloooooooow!\n",
    "            # this_counter = collections.Counter(nc_wg_id_list)\n",
    "            # this_counter.update(ls_wg_id_list)\n",
    "            # this_array = np.array(list(this_counter.items()))\n",
    "            # outcome = this_array[:, 1] == 2\n",
    "            # nc_ls_wg_id_list = this_array[outcome, 0]\n",
    "            # nc_ls_wg_id_set = None\n",
    "\n",
    "            # This is the fastest implementation\n",
    "            if matrix_extraction_option in (0, 6):\n",
    "                nc_ls_wg_id_set = nc_wg_id_set.intersection(ls_wg_id_set)\n",
    "                nc_ls_wg_id_list = cp.fromiter(iter=nc_ls_wg_id_set, dtype=int)\n",
    "                \n",
    "                # now, get the rows\n",
    "                nc_ls_wchar_matrix = wchar_matrix[nc_ls_wg_id_list,]\n",
    "                nc_ls_matrix_dict[nc_ls_id] = (\n",
    "                    nc_ls_wg_id_list,\n",
    "                    nc_ls_wchar_matrix\n",
    "                )\n",
    "\n",
    "            # get the right loop count\n",
    "            loop_count += 1\n",
    "            if loop_count % 1000 == 0:\n",
    "                print(\"...{:,}\".format(loop_count), \"records enumerated...\")\n",
    "\n",
    "        # display the final count\n",
    "        if matrix_extraction_option == 2:\n",
    "            n_sub_matrices = len(n_char_matrix_dict)\n",
    "\n",
    "        if matrix_extraction_option in (3, 4):\n",
    "            n_sub_matrices = len(single_letter_matrix_dict)\n",
    "\n",
    "        if matrix_extraction_option == 5:\n",
    "            n_sub_matrices = len(letter_selector_matrix_dict)\n",
    "\n",
    "        if matrix_extraction_option in (0, 6):\n",
    "            n_sub_matrices = len(nc_ls_matrix_dict)\n",
    "    else:\n",
    "        n_sub_matrices = 0\n",
    "\n",
    "    print(\"...{:,}\".format(n_sub_matrices), \"sub-matrices created...\")\n",
    "    p_time = calc_time(time_start=s_time)\n",
    "    print(\"Total extraction time:\", p_time, \"seconds.\")\n",
    "\n",
    "    # set things to None so that we can free up memory and reduce overhead\n",
    "    # these objects are no longer needed\n",
    "    # only return objects specific to the particular matrix extraction option\n",
    "    if matrix_extraction_option not in (0, 2):\n",
    "        # option 2\n",
    "        n_char_matrix_dict = None\n",
    "\n",
    "    if matrix_extraction_option not in (0, 3, 4):\n",
    "        # option 3 and 4\n",
    "        single_letter_matrix_dict = None\n",
    "\n",
    "    if matrix_extraction_option not in (0, 5):\n",
    "        # option 5\n",
    "        letter_selector_matrix_dict = None\n",
    "\n",
    "    if matrix_extraction_option not in (0, 6):\n",
    "        # option 6\n",
    "        nc_ls_matrix_dict = None\n",
    "\n",
    "    return (\n",
    "        wg_df,\n",
    "        n_char_matrix_dict,\n",
    "        single_letter_matrix_dict,\n",
    "        letter_selector_matrix_dict,\n",
    "        nc_ls_matrix_dict,\n",
    "        p_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545edc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_group_id_list = cp.asarray(word_group_id_list)\n",
    "wchar_matrix = cp.asarray(a = wchar_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96823550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the matrix\n",
    "n_subset_letters = 3\n",
    "matrix_extraction_option = 5\n",
    "wg_df, n_char_matrix_dict, single_letter_matrix_dict, letter_selector_matrix_dict, nc_ls_matrix_dict, p_time = split_matrix_cp(\n",
    "    letter_dict=letter_dict,\n",
    "    word_group_id_list=word_group_id_list,\n",
    "    wg_df=wg_df,\n",
    "    wchar_matrix=wchar_matrix_cp,\n",
    "    n_subset_letters=n_subset_letters,\n",
    "    matrix_extraction_option=matrix_extraction_option\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wchar_matrix_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output_list_cp(outcome_word_id_list: cp.ndarray, wg_id: int) -> cp.ndarray:\n",
    "        \n",
    "    output_list = cp.zeros(shape=(outcome_word_id_list.shape[0], 2), dtype=int)\n",
    "\n",
    "    # update the output list with the word_id_list - these are from/parent words\n",
    "    output_list[:, 0] = outcome_word_id_list\n",
    "\n",
    "    # update with the word_id - this is the to/child word\n",
    "    output_list[:, 1] = wg_id\n",
    "\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def get_values_full_matrix_cp(\n",
    "    wg_id: int, wchar_matrix: cp.ndarray, word_group_id_list: cp.ndarray\n",
    "):\n",
    "        \n",
    "\n",
    "    # matrix extraction option 1\n",
    "    outcome = wchar_matrix - wchar_matrix[wg_id,]\n",
    "\n",
    "    # compute the score by finding where rows, across all columns, are GTE 0\n",
    "    outcome_indices = cp.all(outcome >= 0, axis=1)\n",
    "    outcome = None\n",
    "\n",
    "    # extract anagrams based on index values\n",
    "    outcome_word_id_list = word_group_id_list[outcome_indices]\n",
    "\n",
    "    output_list = format_output_list_cp(\n",
    "        outcome_word_id_list=outcome_word_id_list, wg_id=wg_id\n",
    "    )\n",
    "\n",
    "    return output_list\n",
    "\n",
    "\n",
    "\n",
    "def get_values_n_char_cp(wg_id: int, n_char: int, n_char_matrix_dict: dict):\n",
    "\n",
    "\n",
    "\n",
    "    # matrix extraction option 2\n",
    "    nc_wg_id_list, nc_sub_wchar_matrix = n_char_matrix_dict[n_char]\n",
    "    new_word_id = nc_wg_id_list == wg_id\n",
    "\n",
    "    # perform the comparison\n",
    "    outcome = nc_sub_wchar_matrix - nc_sub_wchar_matrix[new_word_id,]\n",
    "\n",
    "    # compute the score by finding where rows, across all columns, are GTE 0\n",
    "    outcome_indices = cp.all(outcome >= 0, axis=1)\n",
    "    outcome = None\n",
    "\n",
    "    # extract anagrams based on index values\n",
    "    outcome_word_id_list = nc_wg_id_list[outcome_indices]\n",
    "\n",
    "    output_list = format_output_list_cp(\n",
    "        outcome_word_id_list=outcome_word_id_list, wg_id=wg_id\n",
    "    )\n",
    "\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def get_values_single_letter_cp(\n",
    "    wg_id: int, single_letter_id: str, single_letter_matrix_dict: dict\n",
    "):\n",
    "    \n",
    "\n",
    "    # matrix extraction option 3 and 4\n",
    "    (\n",
    "        single_letter_word_group_id_list,\n",
    "        single_letter_wchar_matrix\n",
    "    ) = single_letter_matrix_dict[single_letter_id]\n",
    "\n",
    "    new_word_id = single_letter_word_group_id_list == wg_id\n",
    "\n",
    "    # now, peform the comparison\n",
    "    outcome = single_letter_wchar_matrix - \\\n",
    "        single_letter_wchar_matrix[new_word_id,]\n",
    "\n",
    "    # compute the score by finding where rows, across all columns, are GTE 0\n",
    "    outcome_indices = cp.all(outcome >= 0, axis=1)\n",
    "    outcome = None\n",
    "\n",
    "    # extract anagrams based on index values\n",
    "    outcome_word_id_list = single_letter_word_group_id_list[outcome_indices]\n",
    "\n",
    "    output_list = format_output_list_cp(\n",
    "        outcome_word_id_list=outcome_word_id_list, wg_id=wg_id\n",
    "    )\n",
    "\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def get_values_letter_selector_cp(\n",
    "    wg_id: int, letter_selector_id: str, letter_selector_matrix_dict: dict\n",
    "):\n",
    "    \n",
    "    # matrix extraction option 5\n",
    "    ls_wg_id_list, ls_wchar_matrix = letter_selector_matrix_dict[\n",
    "        letter_selector_id\n",
    "    ]\n",
    "\n",
    "    new_word_id = ls_wg_id_list == wg_id\n",
    "    print(type(new_word_id))\n",
    "\n",
    "    # now, perform the comparison\n",
    "    outcome = ls_wchar_matrix - ls_wchar_matrix[new_word_id,]\n",
    "    print(type(outcome))\n",
    "\n",
    "    # compute the score by finding where rows, across all columns, are GTE 0\n",
    "    outcome_indices = cp.all(outcome >= 0, axis=1)\n",
    "    print(type(outcome_indices))\n",
    "    outcome = None\n",
    "\n",
    "    # extract anagrams based on index values\n",
    "    outcome_word_id_list = ls_wg_id_list[outcome_indices]\n",
    "    print(type(outcome_word_id_list))\n",
    "\n",
    "    output_list = format_output_list_cp(\n",
    "        outcome_word_id_list=outcome_word_id_list, wg_id=wg_id\n",
    "    )\n",
    "    print(type(output_list))\n",
    "\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def get_values_n_char_letter_selector_cp(\n",
    "    wg_id: int, nc_ls_id: tuple, nc_ls_matrix_dict: dict\n",
    "):   \n",
    "\n",
    "    # matrix extraction option 6\n",
    "    nc_ls_wg_id_list, nc_ls_wchar_matrix = nc_ls_matrix_dict[nc_ls_id]\n",
    "\n",
    "    new_word_id = nc_ls_wg_id_list == wg_id\n",
    "\n",
    "    # now, perform the comparison\n",
    "    outcome = nc_ls_wchar_matrix - nc_ls_wchar_matrix[new_word_id,]\n",
    "\n",
    "    # compute the score by finding where rows, across all columns, are GTE 0\n",
    "    outcome_indices = cp.all(outcome >= 0, axis=1)\n",
    "    outcome = None\n",
    "\n",
    "    # extract anagrams based on index values\n",
    "    outcome_word_id_list = nc_ls_wg_id_list[outcome_indices]\n",
    "\n",
    "    output_list = format_output_list_cp(\n",
    "        outcome_word_id_list=outcome_word_id_list, wg_id=wg_id\n",
    "    )\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b78248",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# generate_from_to_word_group_pairs placeholder\n",
    "####\n",
    "def generate_from_to_word_group_pairs_simple_cp(\n",
    "    wg_df: pd.DataFrame,\n",
    "    n_possible_anagrams: int,\n",
    "    matrix_extraction_option: int,\n",
    "    wchar_matrix: cp.ndarray,\n",
    "    word_group_id_list: cp.ndarray,\n",
    "    n_char_matrix_dict: dict,\n",
    "    single_letter_matrix_dict: dict,\n",
    "    letter_selector_matrix_dict: dict,\n",
    "    nc_ls_matrix_dict: dict,\n",
    "    letter_subset_list: str = None\n",
    "):\n",
    "\n",
    "    # use numpy to pre-allocate an array that will be updated while enumerating.\n",
    "    # this eliminates list.append() calls which are fine in small amounts, but\n",
    "    # hundreds of thousands of append calls are very slow.      \n",
    "\n",
    "    output_list = cp.full(shape=(n_possible_anagrams, 2),\n",
    "                          fill_value=-1, dtype=int)\n",
    "\n",
    "    # this dictionary will store the calculations for each word\n",
    "    proc_time_dict = {}\n",
    "\n",
    "    if letter_subset_list == 'SAMPLE':\n",
    "        # generate 100 samples within each n_chars and first_letter group combination\n",
    "        curr_wg_df = wg_df.groupby(['n_chars', 'first_letter']).sample(\n",
    "            n=100, replace=True, random_state=123).drop_duplicates()\n",
    "    elif isinstance(letter_subset_list, str) or isinstance(letter_subset_list, list):\n",
    "        # subset by a specific set of letters or a single letter\n",
    "        curr_wg_df = wg_df.loc[wg_df['first_letter'].isin(\n",
    "            set(letter_subset_list)), :].copy()\n",
    "    else:\n",
    "        curr_wg_df = wg_df.copy()\n",
    "\n",
    "    # display counts\n",
    "    curr_word_count = curr_wg_df.shape[0]\n",
    "\n",
    "    n_curr_words = \"{:,}\".format(curr_word_count)\n",
    "    print(\n",
    "        \"...finding parent anagrams for\",\n",
    "        n_curr_words,\n",
    "        \"words...\"\n",
    "    )\n",
    "\n",
    "    # establish counters for record keeping\n",
    "    row_count = 0\n",
    "    anagram_pair_count = 0\n",
    "    intmerdiate_to_word_count = collections.Counter()\n",
    "    # enumerate by word id, working with integers is faster than words\n",
    "    temp_curr_wg_df = curr_wg_df.iloc[0:10]\n",
    "    for row in temp_curr_wg_df.itertuples(index=False):\n",
    "        # start timing to record processing for each word\n",
    "        s_time = perf_counter_ns()\n",
    "\n",
    "        # word group id\n",
    "        wg_id = row.word_group_id\n",
    "\n",
    "        if matrix_extraction_option == 1:\n",
    "            # option 1: full matrix\n",
    "            outcome_word_id_list = get_values_full_matrix_cp(\n",
    "                wg_id=wg_id,\n",
    "                wchar_matrix=wchar_matrix,\n",
    "                word_group_id_list=word_group_id_list,\n",
    "            )\n",
    "        elif matrix_extraction_option == 2:\n",
    "            # option 2: word length\n",
    "            outcome_word_id_list = get_values_n_char_cp(\n",
    "                wg_id=wg_id,\n",
    "                n_char=row.n_chars,\n",
    "                n_char_matrix_dict=n_char_matrix_dict,\n",
    "            )\n",
    "        elif matrix_extraction_option == 3:\n",
    "            # option 3: first character\n",
    "            outcome_word_id_list = get_values_single_letter_cp(\n",
    "                wg_id=wg_id,\n",
    "                single_letter_id=row.first_letter_id,\n",
    "                single_letter_matrix_dict=single_letter_matrix_dict,\n",
    "            )\n",
    "        elif matrix_extraction_option == 4:\n",
    "            # option 4: single least common letter\n",
    "            outcome_word_id_list = get_values_single_letter_cp(\n",
    "                wg_id=wg_id,\n",
    "                single_letter_id=row.single_letter_id,\n",
    "                single_letter_matrix_dict=single_letter_matrix_dict,\n",
    "            )\n",
    "        elif matrix_extraction_option == 5:\n",
    "            # option 5: letter selector / focal letter\n",
    "            outcome_word_id_list = get_values_letter_selector_cp(\n",
    "                wg_id=wg_id,\n",
    "                letter_selector_id=row.letter_selector_id,\n",
    "                letter_selector_matrix_dict=letter_selector_matrix_dict,\n",
    "            )\n",
    "        else:\n",
    "            # option 6: word length and letter selector\n",
    "            outcome_word_id_list = get_values_n_char_letter_selector_cp(\n",
    "                wg_id=wg_id,\n",
    "                nc_ls_id=row.nc_ls_id,\n",
    "                nc_ls_matrix_dict=nc_ls_matrix_dict,\n",
    "            )\n",
    "\n",
    "        # if the outcome is greater than or equal to zero, then the current word is an\n",
    "        # anagram of the other word\n",
    "        # a value  >= 0 means that the current word contains the exact same number of focal letters\n",
    "        # mite --> time or miter --> time\n",
    "        # a value >= 1 means that current word contains at least the same number of focal letters\n",
    "        # terminator --> time\n",
    "        # a value of <= -1 means that the current word does not have the\n",
    "        # correct number of letters and is therefore not an anagram.\n",
    "        # trait <> time\n",
    "\n",
    "        # number of parent words found\n",
    "        n_from_words = outcome_word_id_list.shape[0]\n",
    "\n",
    "        if n_from_words >= 1:\n",
    "            # we have matches\n",
    "            # the focal word\n",
    "\n",
    "            # enumerate the from/parent words\n",
    "            new_anagram_pair_count = anagram_pair_count + n_from_words\n",
    "\n",
    "            output_list[anagram_pair_count:new_anagram_pair_count,\n",
    "                        :] = outcome_word_id_list\n",
    "\n",
    "            # n_to_word_counter = collections.Counter(output_list[:, 0])            \n",
    "            intmerdiate_to_word_count.update(outcome_word_id_list[:, 0].tolist())\n",
    "\n",
    "            # set the anagram pair count\n",
    "            anagram_pair_count = new_anagram_pair_count\n",
    "\n",
    "        # delete the intermediate list\n",
    "        del outcome_word_id_list\n",
    "\n",
    "        # record the time for the word\n",
    "        p_time = calc_time(time_start=s_time, round_digits=-1)\n",
    "\n",
    "        proc_time_dict[wg_id] = (p_time, n_from_words)\n",
    "\n",
    "        row_count += 1\n",
    "        if row_count % 1e4 == 0:\n",
    "            print('...found parent anagrams for',\n",
    "                  \"{:,}\".format(row_count), 'words...')\n",
    "\n",
    "    # last update\n",
    "    print('...found parent anagrams for', \"{:,}\".format(row_count), 'words...')\n",
    "    # create a dataframe from the proc_time_dict\n",
    "    proc_time_df = pd.DataFrame.from_dict(data=proc_time_dict, orient=\"index\")\n",
    "    proc_time_df = proc_time_df.reset_index()\n",
    "    proc_time_df.columns = [\"word_group_id\", \"n_seconds\", \"n_from_word_groups\"]\n",
    "\n",
    "    # display processing time for the current letter\n",
    "    total_proc_time_s = round(proc_time_df[\"n_seconds\"].sum(), 2)\n",
    "    total_proc_time_m = round(proc_time_df[\"n_seconds\"].sum() / 60, 2)\n",
    "    print(\n",
    "        \"...finding parent anagrams for\",\n",
    "        n_curr_words,\n",
    "        \"words took\",\n",
    "        total_proc_time_s,\n",
    "        \"seconds |\",\n",
    "        total_proc_time_m,\n",
    "        \"minutes...\"\n",
    "    )\n",
    "\n",
    "    # truncate the output array to only include rows with a from/to word pair\n",
    "    # this removes any row that has a value of -1\n",
    "    print('...truncating output list...')\n",
    "    output_indices = cp.all(output_list >= 0, axis=1)\n",
    "    output_list = output_list[output_indices,]\n",
    "    del output_indices\n",
    "\n",
    "    # initialize Counters to hold the count of found pairs for a given word\n",
    "    # for the count of to/child words, we need to count the number of times\n",
    "    # each word_group_id\n",
    "    # exists in the from/parent column\n",
    "    # count the number of to words\n",
    "    # seems little counter-intuitive... but we're counting the number of\n",
    "    # to-words from each from-word. So, this is the number of child words\n",
    "    # from each parent word.\n",
    "    # https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "\n",
    "    # we do not need the count of from-word, but leaving in for convenience\n",
    "    # print(\"...populating the count of from-words...\")\n",
    "    # n_from_word_counter = collections.Counter(output_list[:, 1])\n",
    "\n",
    "    print(\"...populating the count of to-words...\")\n",
    "    # big_count_start_time = perf_counter_ns()\n",
    "    # n_to_word_counter = collections.Counter(output_list[:, 0])\n",
    "    # print(calc_time(time_start = big_count_start_time))\n",
    "    # outcome_test = intmerdiate_to_word_count == n_to_word_counter\n",
    "    # print(outcome_test)\n",
    "\n",
    "    # now, use the map function to get the number of from/to words and the number of\n",
    "    # candidate words for each word\n",
    "    proc_time_df[\"n_to_word_groups\"] = proc_time_df[\"word_group_id\"].map(\n",
    "        intmerdiate_to_word_count\n",
    "    )\n",
    "\n",
    "    # record the matrix extraction option\n",
    "    proc_time_df['matrix_extraction_option'] = matrix_extraction_option\n",
    "\n",
    "    # how many anagram pairs were found?\n",
    "    n_total_anagrams = output_list.shape[0]\n",
    "    n_total_anagrams_formatted = \"{:,}\".format(n_total_anagrams)\n",
    "    print(\"...total anagram pairs:\", n_total_anagrams_formatted)\n",
    "\n",
    "    return proc_time_df, output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddfc116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover from/to word group id pairs\n",
    "letter_subset_list = None\n",
    "matrix_extraction_option = 5\n",
    "proc_time_df, output_list = \\\n",
    "    generate_from_to_word_group_pairs_simple_cp(wg_df=wg_df,\n",
    "                                                n_possible_anagrams=n_possible_anagrams,\n",
    "                                                matrix_extraction_option=matrix_extraction_option,\n",
    "                                                wchar_matrix=wchar_matrix,\n",
    "                                                word_group_id_list=word_group_id_list,\n",
    "                                                n_char_matrix_dict=n_char_matrix_dict,\n",
    "                                                single_letter_matrix_dict=single_letter_matrix_dict,\n",
    "                                                letter_selector_matrix_dict=letter_selector_matrix_dict,\n",
    "                                                nc_ls_matrix_dict=nc_ls_matrix_dict,\n",
    "                                                letter_subset_list=letter_subset_list,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the matrix\n",
    "n_subset_letters = 3\n",
    "matrix_extraction_option = 5\n",
    "wg_df, n_char_matrix_dict, single_letter_matrix_dict, letter_selector_matrix_dict, nc_ls_matrix_dict, p_time = split_matrix(\n",
    "    letter_dict=letter_dict,\n",
    "    word_group_id_list=cp.asnumpy(word_group_id_list),\n",
    "    wg_df=wg_df,\n",
    "    wchar_matrix=cp.asnumpy(wchar_matrix),\n",
    "    n_subset_letters=n_subset_letters,\n",
    "    matrix_extraction_option=matrix_extraction_option\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover from/to word group id pairs\n",
    "letter_subset_list = None\n",
    "matrix_extraction_option = 5\n",
    "proc_time_df, output_list = \\\n",
    "    generate_from_to_word_group_pairs_simple(wg_df=wg_df,\n",
    "                                                n_possible_anagrams=n_possible_anagrams,\n",
    "                                                matrix_extraction_option=matrix_extraction_option,\n",
    "                                                wchar_matrix=cp.asnumpy(wchar_matrix),\n",
    "                                                word_group_id_list=cp.asnumpy(word_group_id_list),\n",
    "                                                n_char_matrix_dict=n_char_matrix_dict,\n",
    "                                                single_letter_matrix_dict=single_letter_matrix_dict,\n",
    "                                                letter_selector_matrix_dict=letter_selector_matrix_dict,\n",
    "                                                nc_ls_matrix_dict=nc_ls_matrix_dict,\n",
    "                                                letter_subset_list=letter_subset_list,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac17b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example array\n",
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# Subtract each row from every other row\n",
    "result = arr[:, np.newaxis, :] - arr[np.newaxis, :, :]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d020cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wchar_matrix[:, cp.newaxis, :] - wchar_matrix[cp.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e592fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
