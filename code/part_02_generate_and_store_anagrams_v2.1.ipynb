{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mike Babb\n",
    "# babbm@uw.edu\n",
    "# Find anagrams\n",
    "## Part 2: Generate and store the anagrams v2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard libraries - installed by default\n",
    "import collections\n",
    "import datetime\n",
    "import pickle\n",
    "import sqlite3\n",
    "import string\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# external libraries - not installed by default\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from part_00_process_functions import load_pickle, build_db_conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base file path\n",
    "base_file_path = '/project/finding_anagrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input path\n",
    "in_file_path = 'data'\n",
    "in_file_path = os.path.join(base_file_path, in_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output db path and name\n",
    "db_path = 'db'\n",
    "db_path = os.path.join(base_file_path, db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(db_path):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name = 'words.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process control flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use numpy to perform matrix opertions and determine from/to and exact anagram relationships\n",
    "# option 1 - work with the full char_matrix\n",
    "# option 2 - create submatrices by word length\n",
    "# option 3 - create submatrices by word length and letter\n",
    "# option 4 - create submatrices by word length and least common two letters\n",
    "\n",
    "# max number of letters to slice to use for the generation of sub-matrices for\n",
    "# option 4. More letters means more sub-matrices\n",
    "n_common_letters = 4\n",
    "\n",
    "# set write_data to true to store the generated list of anagrams\n",
    "write_data = False\n",
    "\n",
    "# set to None to include all letters\n",
    "# test with a subset of letters by setting the letter_subset_list to ['q', 'x'] or \n",
    "# a different set of letters\n",
    "#letter_subset_list = ['q', 'x', 's']\n",
    "letter_subset_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start a timer to record the entire operation\n",
    "total_time_start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the word_df, the words from Part 1\n",
    "input_file_name = 'word_df.csv'\n",
    "# build the file path\n",
    "ipn = os.path.join(in_file_path, input_file_name)\n",
    "\n",
    "# specify the datatypes of the columns using a dictionary\n",
    "# because NA and NULL are reserved python words, but also words in our list of words,\n",
    "# we need to specify the data types of the columns\n",
    "dtype_dict = {'word': str,\n",
    "              'lcase': str,\n",
    "              'word_sum': int,\n",
    "              'first_letter': str,\n",
    "              'word_id': int,\n",
    "              'word_group_id': int,\n",
    "              'letter_group': str,\n",
    "              'letter_group_ranked': str}\n",
    "\n",
    "# read in the file and be careful of the NA and NULL values\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "word_df = pd.read_csv(filepath_or_buffer = ipn, sep = '\\t', header = 0,\n",
    "                          dtype=dtype_dict, na_values = '!!', keep_default_na=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>lcase</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>first_letter</th>\n",
       "      <th>word_id</th>\n",
       "      <th>word_group_id</th>\n",
       "      <th>letter_group</th>\n",
       "      <th>letter_group_ranked</th>\n",
       "      <th>letters_sorted</th>\n",
       "      <th>letters_ranked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234365</th>\n",
       "      <td>zythem</td>\n",
       "      <td>zythem</td>\n",
       "      <td>6</td>\n",
       "      <td>z</td>\n",
       "      <td>234365</td>\n",
       "      <td>215837</td>\n",
       "      <td>ehmtyz</td>\n",
       "      <td>zyhmte</td>\n",
       "      <td>ehmtyz</td>\n",
       "      <td>zyhmte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234366</th>\n",
       "      <td>Zythia</td>\n",
       "      <td>zythia</td>\n",
       "      <td>6</td>\n",
       "      <td>z</td>\n",
       "      <td>234366</td>\n",
       "      <td>215838</td>\n",
       "      <td>ahityz</td>\n",
       "      <td>zyhtai</td>\n",
       "      <td>ahityz</td>\n",
       "      <td>zyhtai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234367</th>\n",
       "      <td>zythum</td>\n",
       "      <td>zythum</td>\n",
       "      <td>6</td>\n",
       "      <td>z</td>\n",
       "      <td>234367</td>\n",
       "      <td>215839</td>\n",
       "      <td>hmtuyz</td>\n",
       "      <td>zyhmut</td>\n",
       "      <td>hmtuyz</td>\n",
       "      <td>zyhmut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234368</th>\n",
       "      <td>Zyzomys</td>\n",
       "      <td>zyzomys</td>\n",
       "      <td>7</td>\n",
       "      <td>z</td>\n",
       "      <td>234368</td>\n",
       "      <td>215840</td>\n",
       "      <td>mosyz</td>\n",
       "      <td>zymso</td>\n",
       "      <td>mosyyzz</td>\n",
       "      <td>zzyymso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234369</th>\n",
       "      <td>Zyzzogeton</td>\n",
       "      <td>zyzzogeton</td>\n",
       "      <td>10</td>\n",
       "      <td>z</td>\n",
       "      <td>234369</td>\n",
       "      <td>215841</td>\n",
       "      <td>egnotyz</td>\n",
       "      <td>zgytnoe</td>\n",
       "      <td>egnootyzzz</td>\n",
       "      <td>zzzgytnooe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word       lcase  n_chars first_letter  word_id  word_group_id  \\\n",
       "234365      zythem      zythem        6            z   234365         215837   \n",
       "234366      Zythia      zythia        6            z   234366         215838   \n",
       "234367      zythum      zythum        6            z   234367         215839   \n",
       "234368     Zyzomys     zyzomys        7            z   234368         215840   \n",
       "234369  Zyzzogeton  zyzzogeton       10            z   234369         215841   \n",
       "\n",
       "       letter_group letter_group_ranked letters_sorted letters_ranked  \n",
       "234365       ehmtyz              zyhmte         ehmtyz         zyhmte  \n",
       "234366       ahityz              zyhtai         ahityz         zyhtai  \n",
       "234367       hmtuyz              zyhmut         hmtuyz         zyhmut  \n",
       "234368        mosyz               zymso        mosyyzz        zzyymso  \n",
       "234369      egnotyz             zgytnoe     egnootyzzz     zzzgytnooe  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the column of word ids as a numpy array\n",
    "word_id_list = word_df['word_id'].to_numpy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the letters sorted by the frequency of words that\n",
    "# start with a particular letter\n",
    "agg_word_df = word_df['first_letter'].groupby(word_df['first_letter']).agg(np.size).to_frame()\n",
    "\n",
    "# set column names\n",
    "agg_word_df.columns = ['word_count']\n",
    "\n",
    "# reset the index to rename columns\n",
    "agg_word_df = agg_word_df.reset_index()\n",
    "\n",
    "# sort the dataframe by frequency\n",
    "agg_word_df = agg_word_df.sort_values(by='word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_letter</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>x</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>y</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>z</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>q</td>\n",
       "      <td>1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>j</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>k</td>\n",
       "      <td>2239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>v</td>\n",
       "      <td>3418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>w</td>\n",
       "      <td>3910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>l</td>\n",
       "      <td>6228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n</td>\n",
       "      <td>6742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g</td>\n",
       "      <td>6771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f</td>\n",
       "      <td>6836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>o</td>\n",
       "      <td>7830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>8703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i</td>\n",
       "      <td>8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h</td>\n",
       "      <td>8992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>r</td>\n",
       "      <td>9613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>10849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>10963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>m</td>\n",
       "      <td>12513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>t</td>\n",
       "      <td>12853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>u</td>\n",
       "      <td>16361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>16974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>19783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>p</td>\n",
       "      <td>24341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>s</td>\n",
       "      <td>24929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first_letter  word_count\n",
       "23            x         380\n",
       "24            y         663\n",
       "25            z         942\n",
       "16            q        1148\n",
       "9             j        1603\n",
       "10            k        2239\n",
       "21            v        3418\n",
       "22            w        3910\n",
       "11            l        6228\n",
       "13            n        6742\n",
       "6             g        6771\n",
       "5             f        6836\n",
       "14            o        7830\n",
       "4             e        8703\n",
       "8             i        8786\n",
       "7             h        8992\n",
       "17            r        9613\n",
       "3             d       10849\n",
       "1             b       10963\n",
       "12            m       12513\n",
       "19            t       12853\n",
       "20            u       16361\n",
       "0             a       16974\n",
       "2             c       19783\n",
       "15            p       24341\n",
       "18            s       24929"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_word_df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the letters sorted by word frequency\n",
    "sorted_first_letters = agg_word_df['first_letter'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the letter dictionary from part 1\n",
    "in_file_name = 'letter_dict.pkl'\n",
    "letter_dict = load_pickle(in_file_path = in_file_path, in_file_name=in_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the word dictionary from part 1\n",
    "in_file_name = 'word_dict.pkl'\n",
    "word_dict = load_pickle(in_file_path = in_file_path, in_file_name=in_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the char matrix from part 1\n",
    "in_file_name = 'char_matrix.npy'\n",
    "ipn = os.path.join(in_file_path, in_file_name)\n",
    "char_matrix = np.load(file = ipn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_df['word_sum'] = word_df['lcase'].map(lambda x: sum([letter_dict[y] + 1 for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>lcase</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>first_letter</th>\n",
       "      <th>word_id</th>\n",
       "      <th>word_group_id</th>\n",
       "      <th>letter_group</th>\n",
       "      <th>letter_group_ranked</th>\n",
       "      <th>letters_sorted</th>\n",
       "      <th>letters_ranked</th>\n",
       "      <th>word_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aal</td>\n",
       "      <td>aal</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>al</td>\n",
       "      <td>la</td>\n",
       "      <td>aal</td>\n",
       "      <td>laa</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalii</td>\n",
       "      <td>aalii</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ail</td>\n",
       "      <td>lai</td>\n",
       "      <td>aaiil</td>\n",
       "      <td>laaii</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aam</td>\n",
       "      <td>aam</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>am</td>\n",
       "      <td>ma</td>\n",
       "      <td>aam</td>\n",
       "      <td>maa</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  lcase  n_chars first_letter  word_id  word_group_id letter_group  \\\n",
       "0      A      a        1            a        0              0            a   \n",
       "1     aa     aa        2            a        1              1            a   \n",
       "2    aal    aal        3            a        2              2           al   \n",
       "3  aalii  aalii        5            a        3              3          ail   \n",
       "4    aam    aam        3            a        4              4           am   \n",
       "\n",
       "  letter_group_ranked letters_sorted letters_ranked  word_sum  \n",
       "0                   a              a              a         1  \n",
       "1                   a             aa             aa         2  \n",
       "2                  la            aal            laa        14  \n",
       "3                 lai          aaiil          laaii        32  \n",
       "4                  ma            aam            maa        15  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what is the distribution of the word_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    234370.000000\n",
       "mean        112.669766\n",
       "std          40.784130\n",
       "min           1.000000\n",
       "25%          83.000000\n",
       "50%         110.000000\n",
       "75%         139.000000\n",
       "max         328.000000\n",
       "Name: word_sum, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df['word_sum'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(10, 101, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deciles = np.percentile(a = word_df['word_sum'].values, q = range(10, 101, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 62.,  77.,  89.,  99., 110., 121., 132., 147., 167., 328.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    234370.000000\n",
       "mean        112.669766\n",
       "std          40.784130\n",
       "min           1.000000\n",
       "25%          83.000000\n",
       "50%         110.000000\n",
       "75%         139.000000\n",
       "max         328.000000\n",
       "Name: word_sum, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df['word_sum'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deciles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_df['word_sum_group'] = np.digitize(x = word_df['word_sum'], bins = deciles, right = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    24828\n",
       "7    24288\n",
       "0    24189\n",
       "2    23998\n",
       "5    23990\n",
       "1    23610\n",
       "9    22987\n",
       "8    22529\n",
       "3    21982\n",
       "6    21969\n",
       "Name: word_sum_group, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df['word_sum_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract sub-matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop duplicates based on the word group. \n",
    "# by default, this will only keep the first record and it will drop all others\n",
    "wg_df = word_df.drop_duplicates(subset = ['word_group_id']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wg_df = wg_df.sort_values(by = 'word_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234370"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215842"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique word groups\n",
    "len(wg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>lcase</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>first_letter</th>\n",
       "      <th>word_id</th>\n",
       "      <th>word_group_id</th>\n",
       "      <th>letter_group</th>\n",
       "      <th>letter_group_ranked</th>\n",
       "      <th>letters_sorted</th>\n",
       "      <th>letters_ranked</th>\n",
       "      <th>word_sum</th>\n",
       "      <th>word_sum_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aal</td>\n",
       "      <td>aal</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>al</td>\n",
       "      <td>la</td>\n",
       "      <td>aal</td>\n",
       "      <td>laa</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalii</td>\n",
       "      <td>aalii</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ail</td>\n",
       "      <td>lai</td>\n",
       "      <td>aaiil</td>\n",
       "      <td>laaii</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aam</td>\n",
       "      <td>aam</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>am</td>\n",
       "      <td>ma</td>\n",
       "      <td>aam</td>\n",
       "      <td>maa</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  lcase  n_chars first_letter  word_id  word_group_id letter_group  \\\n",
       "0      A      a        1            a        0              0            a   \n",
       "1     aa     aa        2            a        1              1            a   \n",
       "2    aal    aal        3            a        2              2           al   \n",
       "3  aalii  aalii        5            a        3              3          ail   \n",
       "4    aam    aam        3            a        4              4           am   \n",
       "\n",
       "  letter_group_ranked letters_sorted letters_ranked  word_sum  word_sum_group  \n",
       "0                   a              a              a         1               0  \n",
       "1                   a             aa             aa         2               0  \n",
       "2                  la            aal            laa        14               0  \n",
       "3                 lai          aaiil          laaii        32               0  \n",
       "4                  ma            aam            maa        15               0  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9209455135042881"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_df.shape[0] / word_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    215842.000000\n",
       "mean     107920.500000\n",
       "std       62308.362739\n",
       "min           0.000000\n",
       "25%       53960.250000\n",
       "50%      107920.500000\n",
       "75%      161880.750000\n",
       "max      215841.000000\n",
       "Name: word_group_id, dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_df['word_group_id'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the word group ids\n",
    "word_group_id_list = wg_df['word_group_id'].to_numpy()\n",
    "# and the associated word_id\n",
    "word_id_list = wg_df['word_id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trim the char matrix by word id\n",
    "# and not the word_group id\n",
    "wchar_matrix = char_matrix[word_id_list, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i don't use these objects\n",
    "# build a word_id to word_group_id dictionary\n",
    "word_id_wg_id_dict = dict()\n",
    "# and a word_group_id to word_id dictionary\n",
    "wg_id_word_id_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word_id, wg_id in zip(wg_df['word_id'], wg_df['word_group_id']):\n",
    "    word_id_wg_id_dict[word_id] = wg_id\n",
    "    wg_id_word_id_dict[wg_id] = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the dictionary holding the sub-matrices\n",
    "ws_matrix_dict = {}\n",
    "\n",
    "# by word length\n",
    "word_length_list = sorted(wg_df['word_sum'].unique().tolist())\n",
    "\n",
    "# python dictionaries work by storing the hash values of objects\n",
    "# Anything that can be hashed can be a dictionary key. \n",
    "# Computing the hash value of an object ahead of time can reduce dictionary access time.\n",
    "# we'll compute the associated hash value of the tuple used to identify the sub-matrices.\n",
    "\n",
    "#wg_id_ws_matrix_dict = {}\n",
    "wg_id_ws_matrix_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...creating 37411 sub matrices\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'word_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\analysis\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\analysis\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\analysis\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'word_sum'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m ls_word_id_list_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     31\u001b[0m working_char_matrix_rows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape \u001b[38;5;241m=\u001b[39m (n_sub_matrices, \u001b[38;5;241m0\u001b[39m), dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ws, letter_selector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(letter_selector_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_sum\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     34\u001b[0m                                     letter_selector_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mletter_selector\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# word id set by word sum\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# words of at least a certain word sum\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ws \u001b[38;5;129;01min\u001b[39;00m ws_word_id_list_dict:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;66;03m# get the set if it already exists\u001b[39;00m\n\u001b[0;32m     40\u001b[0m         curr_ws_word_id_set \u001b[38;5;241m=\u001b[39m ws_word_id_list_dict[ws]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\analysis\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\analysis\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'word_sum'"
     ]
    }
   ],
   "source": [
    "####\n",
    "# populate the dictionaries\n",
    "####\n",
    "\n",
    "loop_count = 0\n",
    "s_time = datetime.datetime.now()\n",
    "\n",
    "# by word length and n least common letters\n",
    "wg_df['letter_selector'] = wg_df['letter_group_ranked'].str[:n_common_letters]\n",
    "\n",
    "# store the tuple in the wg_df\n",
    "# we have to use tuples because tuples are immutable - once created, they cannot be changed\n",
    "#https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences    \n",
    "wg_df['wg_id_ws_matrix_key'] = tuple(zip(wg_df['word_sum_group'], wg_df['letter_selector']))\n",
    "\n",
    "# This is a combinatorial problem.\n",
    "# Limit the number of selections we need to make    \n",
    "letter_selector_df = wg_df[['letter_selector', 'word_sum_group']].drop_duplicates()\n",
    "n_sub_matrices = len(letter_selector_df)\n",
    "print('...creating', n_sub_matrices, 'sub matrices')\n",
    "# this means n_sub_matrices are queried.\n",
    "# we can expedite this by only selecting certain word ids once, converting to a set,\n",
    "# and then storing that set based on the selection criteria.\n",
    "# many words are going to have the same least common characters, let's identify the\n",
    "# corresponding rows accordingly\n",
    "\n",
    "letter_selector_list = letter_selector_df['letter_selector'].unique().tolist()\n",
    "ws_word_id_list_dict = {}\n",
    "ls_word_id_list_dict = {}\n",
    "\n",
    "working_char_matrix_rows = np.zeros(shape = (n_sub_matrices, 0), dtype = int)\n",
    "\n",
    "for ws, letter_selector in zip(letter_selector_df['word_sum'],\n",
    "                                    letter_selector_df['letter_selector']):\n",
    "\n",
    "    # word id set by word sum\n",
    "    # words of at least a certain word sum\n",
    "    if ws in ws_word_id_list_dict:\n",
    "        # get the set if it already exists\n",
    "        curr_ws_word_id_set = ws_word_id_list_dict[ws]\n",
    "    else:\n",
    "        # create the set if it does not exist\n",
    "        curr_ws_word_id_set = wg_df.loc[(wg_df['word_sum']>=ws) , 'word_group_id'].tolist()\n",
    "        curr_ws_word_id_set = set(curr_ws_word_id_set)\n",
    "        ws_word_id_list_dict[ws] = curr_ws_word_id_set\n",
    "\n",
    "    # word id by letter selector\n",
    "    # words with at least these common letters\n",
    "    if letter_selector in ls_word_id_list_dict:\n",
    "        # get the set if it already exists\n",
    "        curr_letter_select_word_id_set = ls_word_id_list_dict[letter_selector]\n",
    "    else:\n",
    "        # the set needs to be computed\n",
    "        # build the column selector using list comprehension\n",
    "        column_selector = [letter_dict[curr_letter] for curr_letter in letter_selector]\n",
    "\n",
    "        # create a true-false matrix where only certain columns, corresponding to\n",
    "        # letter indices, have a value of 1 or more\n",
    "        outcome = wchar_matrix[:, column_selector] > 0    \n",
    "\n",
    "        # which rows in the above matrix evaluate to all True\n",
    "        outcome_indices = np.all(a = outcome, axis = 1)\n",
    "\n",
    "        # these indices match with the word_id_list, extract the subset        \n",
    "        curr_letter_select_word_id_set = word_group_id_list[outcome_indices]\n",
    "        curr_letter_select_word_id_set = set(curr_letter_select_word_id_set)\n",
    "        ls_word_id_list_dict[letter_selector] = curr_letter_select_word_id_set\n",
    "\n",
    "    # the set intersection of the curr_ws_word_id_set and the\n",
    "    # curr_letter_select_word_id_set are indices that feature a word of at\n",
    "    # least a certain word sum and the characters of interest\n",
    "    outcome_wg_id_set = curr_ws_word_id_set.intersection(curr_letter_select_word_id_set)\n",
    "    # convert the set to an array\n",
    "    outcome_word_group_id_list = np.array(list(outcome_wg_id_set))\n",
    "\n",
    "    # subset the wchar_matrix to get the sub matrix\n",
    "    curr_ws_char_matrix = wchar_matrix[outcome_word_group_id_list, ]\n",
    "    #working_char_matrix_rows[loop_count] = curr_wchar_matrix.shape[0]    \n",
    "   \n",
    "    # now, store that in the sub matrix dictionary\n",
    "    key_value = (ws, letter_selector)\n",
    "    key_value_hash = hash(key_value)\n",
    "    ws_matrix_dict[key_value_hash] = (outcome_word_group_id_list, curr_ws_char_matrix)\n",
    "\n",
    "    # simple progress display\n",
    "    loop_count += 1\n",
    "    if loop_count % 1000 == 0:\n",
    "        print(loop_count)\n",
    "        \n",
    "wg_df['wg_id_ws_matrix_key_hash'] = wg_df['wg_id_ws_matrix_key'].map(hash)\n",
    "for curr_word_id, curr_key_hash in zip(wg_df['word_group_id'], wg_df['wg_id_ws_matrix_key_hash']):\n",
    "    wg_id_ws_matrix_dict[curr_word_id] = curr_key_hash\n",
    "\n",
    "e_time = datetime.datetime.now()\n",
    "p_time = e_time - s_time\n",
    "# how long did this pre-processing take?\n",
    "p_time = round(p_time.total_seconds(), 2)\n",
    "print('...sub-matrix extraction took', p_time, 'seconds...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "letter_selector_df['search_records'] = working_char_matrix_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215842"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wg_id_ws_matrix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's examine what we've created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this will find all words in this group: emit, item, mite, time\n",
    "temp_focal_word = 'emit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58098\n",
      "-2528622905513044880\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_char_matrix_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(temp_focal_word_hash_id)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# the candidate word ids and the sub matrix\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m temp_word_id_list, temp_sub_matrix \u001b[38;5;241m=\u001b[39m n_char_matrix_dict[temp_focal_word_hash_id]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(temp_word_id_list\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(temp_sub_matrix\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_char_matrix_dict' is not defined"
     ]
    }
   ],
   "source": [
    "temp_focal_word_id = wg_df.loc[wg_df['lcase']==temp_focal_word, 'word_group_id'].iloc[0]\n",
    "# the ID of the focal word\n",
    "print(temp_focal_word_id)\n",
    "# the hash corresponding to the tuple of the candidate word ids and the sub-matrix\n",
    "temp_focal_word_hash_id = wg_id_ws_matrix_dict[temp_focal_word_id]\n",
    "print(temp_focal_word_hash_id)\n",
    "# the candidate word ids and the sub matrix\n",
    "temp_word_id_list, temp_sub_matrix = n_char_matrix_dict[temp_focal_word_hash_id]\n",
    "print(temp_word_id_list.shape)\n",
    "print(temp_sub_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a function to query the matrix, examine the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_values(wg_id, word_group_id_list,\n",
    "               n_char_matrix_dict,\n",
    "               wg_id_ws_matrix_dict, wchar_matrix):\n",
    "    \"\"\" FIND ANAGRAMS FOR A SPECIFIC word USING word_group_id AND MATRIX COMPARISONS    \n",
    "    \"\"\" \n",
    "    \n",
    "    # A USEFUL WAY TO PROTOTYPE, TIME, AND DETERMINE THE\n",
    "    # CORRECTNESS OF PROGRAM OPERATION AND OUTPUT\n",
    "\n",
    "    # get information data based on word id\n",
    "        \n",
    "    key_hash = wg_id_ws_matrix_dict[wg_id]\n",
    "        \n",
    "    cw_id_list, curr_char_matrix = n_char_matrix_dict[key_hash]\n",
    "    # subtract the curr_test_vector from every row in the matrix\n",
    "    # this produces a new matrix.        \n",
    "    new_word_id = cw_id_list==wg_id        \n",
    "    outcome = curr_char_matrix - curr_char_matrix[new_word_id, ]\n",
    "\n",
    "    # compute the score by finding where rows, across all columns, are GTE 0\n",
    "    outcome_indices = np.all(outcome >= 0, axis = 1)\n",
    "    outcome = None        \n",
    "\n",
    "    # extract anagrams based on index values\n",
    "    outcome_word_id_list = cw_id_list[outcome_indices]    \n",
    "    \n",
    "    output_list = np.zeros(shape = (len(outcome_word_id_list), 2),  dtype=np.int32)\n",
    "    \n",
    "    # update the output list with the word_id_list - these are from/parent words    \n",
    "    output_list[:, 0] = outcome_word_id_list\n",
    "    \n",
    "    # update with the word_id - this is the to/child word\n",
    "    output_list[:, 1] = wg_id\n",
    "        \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# demontrate the look up with the word 'quiet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curr_word_group_id = word_df.loc[word_df['lcase'] == 'quiet', 'word_group_id'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curr_word_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = get_values(wg_id = curr_word_group_id, word_group_id_list = word_group_id_list,                     \n",
    "                    n_char_matrix_dict = n_char_matrix_dict,\n",
    "                    wg_id_ws_matrix_dict = wg_id_ws_matrix_dict,\n",
    "                    wchar_matrix = wchar_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many parent/from words were found for the word 'quiet'?\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is an array of from words to the word 'quiet'\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and those words are...\n",
    "word_list = word_df.loc[word_df['word_group_id'].isin(output[:, 0]), 'lcase'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we've tested with one word, let's time many evaluations to get a sense of how quickly \n",
    "# the current matrix_extraction_option executes\n",
    "# use the timeit() function to evaluate how long, on average, a single matrix operation\n",
    "# takes to complete\n",
    "code_snippet = \"\"\"get_values(wg_id = curr_word_group_id, word_group_id_list = word_group_id_list,                     \n",
    "                    n_char_matrix_dict = n_char_matrix_dict,\n",
    "                    wg_id_ws_matrix_dict = wg_id_ws_matrix_dict,\n",
    "                    wchar_matrix = wchar_matrix)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "total_time = timeit.timeit(code_snippet,\n",
    "              number=n_trials, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# average number of seconds per trial\n",
    "total_time / n_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate total number of from/to word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many anagrams are there?\n",
    "# let's estimate the number of anagrams by assuming that the number of\n",
    "# parent/from words is a function of word length. \n",
    "# let's sample 10 words of each word length, compute the number of from/parent anagrams\n",
    "# for each word in the sample, compute the min, mean, and max, and apply those values\n",
    "# to the numbers of words by length and multiply accordingly\n",
    "# this will give us very generous upper bound of anagram pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of the number of characters per word\n",
    "ws_list = sorted(word_df['word_sum'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enumerate and sample\n",
    "output_list = []\n",
    "for i_ws, ws in enumerate(ws_list):\n",
    "    curr_id_list = wg_df.loc[wg_df['word_sum']==ws, 'word_group_id'].to_numpy()\n",
    "    # sample with replacement\n",
    "    sample_id_list = np.random.choice(a = curr_id_list, size = 10, replace = True)\n",
    "    for sid in sample_id_list:\n",
    "        output = get_values(wg_id = sid, word_group_id_list = word_group_id_list,                     \n",
    "                    n_char_matrix_dict = n_char_matrix_dict,\n",
    "                    wg_id_ws_matrix_dict = wg_id_ws_matrix_dict,\n",
    "                    wchar_matrix = wchar_matrix)\n",
    "        curr_from_words = len(output)\n",
    "        curr_output = [ws, curr_from_words]\n",
    "        output_list.append(curr_output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a dataframe from the possibilities\n",
    "pos_df = pd.DataFrame(data = output_list, columns = ['word_sum', 'n_from_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# minimum, max, and mean number of from words\n",
    "agg_pos_df = pos_df.groupby('word_sum').agg([np.min, np.max, np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agg_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agg_pos_df.columns = ['min_n_from_words', 'max_n_from_words', 'mean_n_from_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's aggregate by number of letters per word, and then join\n",
    "n_word_length_df = word_df['word_sum'].groupby(word_df['word_sum']).agg(np.size).to_frame()\n",
    "n_word_length_df.columns = ['n_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pos_df = pd.merge(left = n_word_length_df, right = agg_pos_df, left_index = True,\n",
    "                   right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pos_df['n_tot_max_anagrams'] = n_pos_df['n_words'] * n_pos_df['max_n_from_words']\n",
    "n_pos_df['n_tot_mean_anagrams'] = n_pos_df['n_words'] * n_pos_df['mean_n_from_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the upper bound of anagrams as the midway point\n",
    "# between the mean and the max of the estimated number of anagrams\n",
    "n_possible_anagrams = (n_pos_df['n_tot_mean_anagrams'].sum() + n_pos_df['n_tot_max_anagrams'].sum()) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# round and convert to integer\n",
    "n_possible_anagrams = int(np.round(n_possible_anagrams, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this number will be used to create an array that will hold the from/to pairs\n",
    "n_possible_anagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wg_df['n_records'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls_agg_df = wg_df[['letter_selector', 'n_records']].groupby(['letter_selector']).agg(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls_agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls_agg_df['n_records'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls_agg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wg_df['wg_id_ws_matrix_key_hash'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discover from/to word group id pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize counters to count the number of to (child words) from a focal word.\n",
    "# we could do this in post-processing, but the data are already in memory and it's a simple \n",
    "# calculation to make.\n",
    "# we want to minimize the number of trips through our data.\n",
    "\n",
    "# the number of candidate words examined for each focal word\n",
    "\n",
    "# a list to hold the dataframes generated for each letter\n",
    "proc_time_df_list = []\n",
    "\n",
    "# subset the list of leters\n",
    "if letter_subset_list:\n",
    "    letters = letter_subset_list[:]\n",
    "else:\n",
    "    letters = sorted_first_letters\n",
    "\n",
    "anagram_pair_count = 0 \n",
    "# use numpy to pre-allocate an array that will be updated while enumerating. \n",
    "# this eliminates list.append() calls\n",
    "\n",
    "output_list = np.full(shape = (n_possible_anagrams, 2), fill_value = -1,  dtype=int)\n",
    "\n",
    "wg_count = 0\n",
    "\n",
    "for i_cl, curr_letter in enumerate(letters):\n",
    "    # enumerate by each letter\n",
    "    # this isn't absolutely necessary, we could just enumerate by word id, \n",
    "    # but for testing and development, letters are a handy way to chunk up the data. \n",
    "\n",
    "    # this dictionary will store the calculations for each letter\n",
    "    proc_time_dict = {}    \n",
    "    \n",
    "    # the list of words that start with the focal letter     \n",
    "    curr_wg_df = wg_df.loc[wg_df['first_letter'] == curr_letter, :]\n",
    "    \n",
    "    # sort the dataframe by ws and letter_selector, if it exists.\n",
    "    # this will cut down on dictionary lookups for matrix_extraction_types 3 and 4.        \n",
    "    curr_wg_df = curr_wg_df.sort_values(by = ['word_sum', 'letter_selector'])        \n",
    "    curr_word_group_id_list = curr_wg_df['word_group_id'].tolist()\n",
    "    \n",
    "    wg_count += len(curr_word_group_id_list)\n",
    "    \n",
    "    n_curr_words = '{:,}'.format(len(curr_wg_df))    \n",
    "    print('...finding parent anagrams for', n_curr_words, 'words that start with', curr_letter)               \n",
    "    \n",
    "    # enumerate by word id, working with integers is faster than words    \n",
    "    for i_wi, word_group_id in enumerate(curr_word_group_id_list):            \n",
    "        # start timing to record processing for each word            \n",
    "        s_time = datetime.datetime.now()\n",
    "        \n",
    "        # get the current word length, from the word id\n",
    "        #to_word, to_word_length, curr_first_letter, clg, clgr = word_dict[word_group_id]   \n",
    "        to_word_length = word_dict[word_group_id][1]               \n",
    "\n",
    "        # get the tuple associated with the word id\n",
    "        # much faster to look up stored values for the hash value than it is to \n",
    "        # only look up if the hash value has changed            \n",
    "        key_hash = wg_id_ws_matrix_dict[word_group_id]                \n",
    "        # get the possible candidate word_group_ids and char matrix\n",
    "        curr_word_id_index_list, curr_char_matrix = n_char_matrix_dict[key_hash]                                \n",
    "\n",
    "        # how many candidates?\n",
    "        n_possible_words = len(curr_word_id_index_list)\n",
    "\n",
    "        # subtract the curr_test_vector from every row in the matrix\n",
    "        # this produces a new matrix.        \n",
    "        new_word_id = curr_word_id_index_list == word_group_id            \n",
    "        outcome = curr_char_matrix - curr_char_matrix[new_word_id, ]\n",
    "        del new_word_id\n",
    "                        \n",
    "        # compute the score by finding where rows, across all columns, are GTE 0\n",
    "        outcome_indices = np.all(outcome >= 0, axis = 1)\n",
    "        outcome = None        \n",
    "        \n",
    "        # extract anagrams based on same index values\n",
    "        outcome_word_id_list = curr_word_id_index_list[outcome_indices].tolist()\n",
    "            \n",
    "        outcome_indices = None               \n",
    "        \n",
    "        # if the outcome is greater than or equal to zero, then the current word is an\n",
    "        # anagram of the other word    \n",
    "        # a value  >= 0 means that the current word contains the exact same number of focal letters\n",
    "        # mite --> time or miter --> time\n",
    "        # a value >= 1 means that current word contains at least the same number of focal letters\n",
    "        # terminator --> time\n",
    "        # a value of <=-1 means that the current word does not have the \n",
    "        # correct number of letters and is therefore not an anagram.\n",
    "        # trait <> time        \n",
    "\n",
    "        # number of parent words found\n",
    "        n_from_words = len(outcome_word_id_list)\n",
    "\n",
    "        if n_from_words > 1:\n",
    "            \n",
    "            # we have matches\n",
    "            # the focal word   \n",
    "                                    \n",
    "            # enumerate the from/parent words            \n",
    "            new_anagram_pair_count = anagram_pair_count + len(outcome_word_id_list)\n",
    "            # the from words\n",
    "            #print(anagram_pair_count)    \n",
    "            #print(new_anagram_pair_count)\n",
    "            #print(len(outcome_word_id_list))\n",
    "            #print(output_list.shape)\n",
    "            output_list[anagram_pair_count:new_anagram_pair_count, 0] = outcome_word_id_list      \n",
    "            \n",
    "            # the to word\n",
    "            output_list[anagram_pair_count:new_anagram_pair_count, 1] = word_group_id                                            \n",
    "            \n",
    "            # set the anagram pair count\n",
    "            anagram_pair_count = new_anagram_pair_count\n",
    "                    \n",
    "                \n",
    "        del outcome_word_id_list\n",
    "            \n",
    "        # record the time for the word\n",
    "        e_time = datetime.datetime.now()\n",
    "        p_time = e_time - s_time    \n",
    "        p_time = p_time.total_seconds()\n",
    "\n",
    "        proc_time_dict[word_group_id] = (p_time, n_from_words, n_possible_words)       \n",
    "    \n",
    "    # create a dataframe from the proc_time_dict\n",
    "    proc_time_df = pd.DataFrame.from_dict(data=proc_time_dict, orient='index')\n",
    "    proc_time_df = proc_time_df.reset_index()\n",
    "    proc_time_df.columns = ['word_group_id', 'n_seconds', 'n_from_word_groups', 'n_candidates']                \n",
    "    \n",
    "    # display processing time for the current letter\n",
    "    total_proc_time = round(proc_time_df['n_seconds'].sum(), 2)\n",
    "    print('...finding parent anagrams for', curr_letter, 'words took', total_proc_time, 'seconds...')\n",
    "    \n",
    "    proc_time_df_list.append(proc_time_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape and store output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# truncate the output array to only include indices with a from/to word pair\n",
    "output_indices = np.all(output_list >= 0, axis = 1)\n",
    "output_list = output_list[output_indices, ]\n",
    "del output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many anagram pairs were found?\n",
    "n_total_anagrams = len(output_list)\n",
    "n_total_anagrams_formatted = '{:,}'.format(n_total_anagrams)\n",
    "print('...total anagrams', n_total_anagrams_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## count the number of to words\n",
    "# https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "# number of to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the count of to words\n",
    "to_word_counter = collections.Counter(output_list[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create database connection objects\n",
    "db_conn = build_db_conn(db_path = db_path, db_name = db_name)\n",
    "db_cursor = db_conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write anagram pairs to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write the anagram pairs to the database\n",
    "if write_data:\n",
    "\n",
    "    # let's write to the SQLite database in chunks of 1M records\n",
    "    cut_size = 1000000\n",
    "    break_point_list = list(range(0, len(output_list), cut_size))\n",
    "    # add the last bit of records\n",
    "    if break_point_list[-1] < len(output_list):\n",
    "        break_point_list.append(len(output_list))\n",
    "    \n",
    "    # drop the anagrams table if it previously exists\n",
    "    sql = 'drop table if exists anagram_groups;'\n",
    "    \n",
    "    print('...dropping previous table...')\n",
    "    # send the sql statement to the database and commit the changes\n",
    "    db_cursor.execute(sql)\n",
    "    db_conn.commit()\n",
    "\n",
    "    # create the anagrams table\n",
    "    sql = 'create table anagram_groups ( from_word_group_id integer, to_word_group_id integer);'\n",
    "\n",
    "    # execute the statement and commit changes    \n",
    "    db_cursor.execute(sql)\n",
    "    db_conn.commit()\n",
    "        \n",
    "    # objects to record write time\n",
    "    db_write_time_list = []\n",
    "    db_write_time_start = datetime.datetime.now()\n",
    "    \n",
    "    # create a sql statement that we'll use to insert values.\n",
    "    print('...beginning to add anagram word group pairs...')\n",
    "    base_sql = 'insert into anagram_groups values (?,?)'    \n",
    "    \n",
    "    insert_count = 0    \n",
    "    curr_db_write_time_start = datetime.datetime.now()\n",
    "    for i_bp, bp in enumerate(break_point_list[:-1]):\n",
    "        # slice the output list of word id pairs, convert to a python list\n",
    "        # the numpy.int data type is not compatable with sqlite.\n",
    "        # the cursor.executemany() is a quick way to write a lot of data.\n",
    "        next_bp = break_point_list[i_bp + 1]\n",
    "        \n",
    "        # converting the entire output_list to a python list adds too much overheard.\n",
    "        curr_output_list = output_list[bp:next_bp, ].tolist()\n",
    "        \n",
    "        # use the executemany() function to write records\n",
    "        #https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.executemany\n",
    "        db_cursor.executemany(base_sql, curr_output_list)\n",
    "        \n",
    "        # commit changes every 10M records        \n",
    "        if next_bp % 1000000 == 0:\n",
    "            print('...commiting changes:', '{:,}'.format(next_bp), 'records')\n",
    "            db_conn.commit()\n",
    "            # calculate the current time to write 10M records\n",
    "            curr_db_write_time_end = datetime.datetime.now()\n",
    "            curr_db_write_time_proc = curr_db_write_time_end - curr_db_write_time_start            \n",
    "            curr_db_write_time_proc = curr_db_write_time_proc.total_seconds()\n",
    "            \n",
    "            # save this value\n",
    "            db_write_time_list.append(curr_db_write_time_proc)                                      \n",
    "\n",
    "            # compute average write time, display after 1M writes\n",
    "            mean_write_time = np.mean(db_write_time_list)\n",
    "\n",
    "            # compute ETA            \n",
    "            n_seconds = (n_total_anagrams / 10000000) * mean_write_time\n",
    "            add_seconds = datetime.timedelta(seconds = n_seconds)\n",
    "            eta_write_complete = db_write_time_start + add_seconds            \n",
    "            eta_write_complete = eta_write_complete.strftime(format = \"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "            mean_write_time = round(mean_write_time, 3)\n",
    "            print('...average write time per 10M records:', mean_write_time, 'seconds...')\n",
    "            print('...estimated write complete time:', eta_write_complete)\n",
    "            \n",
    "            # restart the current write time\n",
    "            curr_db_write_time_start = datetime.datetime.now()\n",
    "        \n",
    "    # commit the last round of changes\n",
    "    print('...commiting changes:', '{:,}'.format(len(curr_output_list)), 'records')\n",
    "    db_conn.commit()\n",
    "    \n",
    "    # compute total write times\n",
    "    db_write_time_end = datetime.datetime.now()\n",
    "    db_write_time_proc = db_write_time_end - db_write_time_start\n",
    "    db_write_time_proc = db_write_time_proc.total_seconds() / 60\n",
    "    db_write_time_proc = round(db_write_time_proc, 2)\n",
    "    print('...writing to db took', db_write_time_proc, 'minutes')\n",
    "    \n",
    "    del curr_output_list\n",
    "    \n",
    "# remove the list of from/to word pairs\n",
    "del output_list    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store number of from/to word pairs and time related to processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the processing times\n",
    "proc_time_df = pd.concat(proc_time_df_list)\n",
    "\n",
    "# drop columns related to data processing\n",
    "drop_col_names = ['letter_selector', 'word_id_n_char_matrix_key',\n",
    "                  'word_id_n_char_matrix_key_hash']\n",
    "curr_col_names = word_df.columns.tolist()\n",
    "for dcn in drop_col_names:    \n",
    "    if dcn in curr_col_names:\n",
    "        word_df = word_df.drop(dcn, axis = 1)\n",
    "\n",
    "# merge the word_df and the proc_time_df dataframes to get the processing time per word\n",
    "word_df = pd.merge(left=word_df, right = proc_time_df)\n",
    "\n",
    "# now, use the map function to get the number of from/to words and the number of\n",
    "# candidate words for each word\n",
    "word_df['n_to_word_groups'] = word_df['word_group_id'].map(to_word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rearrange columns\n",
    "col_names = ['word','lcase','word_sum','first_letter','word_id',\n",
    "             'word_group_id','letter_group','letter_group_ranked','n_seconds',\n",
    "             'n_from_word_groups','n_to_word_groups','n_candidates']\n",
    "word_df = word_df[col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's include a field to indicate which word was actually used from the candidate groups\n",
    "word_df['word_processed'] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_df.loc[word_df['word_id'].isin(wg_df['word_id']), 'word_processed'] = int(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add a matrix extraction option\n",
    "word_df['matrix_extraction_option'] = int(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output table name\n",
    "table_name = 'words_me_06'\n",
    "# write the processing option table\n",
    "word_df.to_sql(name=table_name, con=db_conn, if_exists='replace', index = False)    \n",
    "# write the words table\n",
    "word_df.to_sql(name='words_v2', con=db_conn, if_exists='replace', index = False)    \n",
    "    \n",
    "# close the connection\n",
    "db_cursor.close()\n",
    "db_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anagram_discovery_time = word_df.loc[word_df['word_processed'] == 1, 'n_seconds'].sum()\n",
    "anagram_discovery_time = anagram_discovery_time / 60\n",
    "anagram_discovery_time = round(anagram_discovery_time, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('...anagram discovery time:', anagram_discovery_time, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# record the total time\n",
    "total_time_end = datetime.datetime.now()\n",
    "total_time_proc = total_time_end - total_time_start\n",
    "total_time_proc = total_time_proc.total_seconds()\n",
    "total_time_proc = total_time_proc / 60\n",
    "total_time_proc = round(total_time_proc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('...total processing time:', total_time_proc, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_df['n_candidates'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
